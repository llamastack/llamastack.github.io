


<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Ollama Distribution &mdash; llama-stack 0.2.14 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/my_theme.css?v=f1163765" />
      <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
      <link rel="stylesheet" type="text/css" href="../../_static/dark_mode_css/general.css?v=c0a7eb24" />
      <link rel="stylesheet" type="text/css" href="../../_static/dark_mode_css/dark.css?v=70edf1c7" />

  
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=71c180b9"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../_static/copybutton.js?v=09bf800d"></script>
      <script src="../../_static/js/detect_theme.js?v=76226c80"></script>
      <script src="../../_static/js/keyboard_shortcuts.js"></script>
      <script src="../../_static/design-tabs.js?v=f930bc37"></script>
      <script src="../../_static/dark_mode_js/default_light.js?v=c2e647ce"></script>
      <script src="../../_static/dark_mode_js/theme_switcher.js?v=358d3910"></script>
    <script src="../../_static/js/theme.js"></script>
    <script src="../../_static/js/versions.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Together Distribution" href="together.html" />
    <link rel="prev" title="NVIDIA Distribution" href="nvidia.html" />
 

<script src="../../_static/version-loader.js"></script>

</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #2980B9" >

          
          
          <a href="../../index.html" class="icon icon-home">
            llama-stack
          </a>
              <div class="switch-menus">
                <div class="version-switch"></div>
                <div class="language-switch"></div>
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../index.html">Llama Stack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/index.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/detailed_tutorial.html">Detailed Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../introduction/index.html">Why Llama Stack?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../concepts/index.html">Core Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../openai/index.html">OpenAI API Compatibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../providers/index.html">Providers Overview</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Distributions Overview</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../importing_as_library.html">Using Llama Stack as a Library</a></li>
<li class="toctree-l2"><a class="reference internal" href="../configuration.html">Configuring a “Stack”</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../list_of_distributions.html">Available List of Distributions</a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="../list_of_distributions.html#selection-of-a-distribution-template">Selection of a Distribution / Template</a><ul class="current">
<li class="toctree-l4 current"><a class="reference internal" href="../list_of_distributions.html#distribution-details">Distribution Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../list_of_distributions.html#on-device-distributions">On-Device Distributions</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../kubernetes_deployment.html">Kubernetes Deployment Guide</a></li>
<li class="toctree-l2"><a class="reference internal" href="../building_distro.html">Build your own Distribution</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../building_applications/index.html">Building AI Applications (Examples)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../playground/index.html">Llama Stack Playground</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../contributing/index.html">Contributing to Llama-Stack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../references/index.html">References</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: #2980B9" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">llama-stack</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html">Distributions Overview</a></li>
          <li class="breadcrumb-item"><a href="../list_of_distributions.html">Available List of Distributions</a></li>
      <li class="breadcrumb-item active">Ollama Distribution</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/distributions/self_hosted_distro/ollama.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <!-- This file was auto-generated by distro_codegen.py, please edit source -->
<section class="tex2jax_ignore mathjax_ignore" id="ollama-distribution">
<h1>Ollama Distribution<a class="headerlink" href="#ollama-distribution" title="Link to this heading"></a></h1>
<div class="toctree-wrapper compound">
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">llamastack/distribution-ollama</span></code> distribution consists of the following provider configurations.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>API</p></th>
<th class="head"><p>Provider(s)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>agents</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">inline::meta-reference</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>datasetio</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">remote::huggingface</span></code>, <code class="docutils literal notranslate"><span class="pre">inline::localfs</span></code></p></td>
</tr>
<tr class="row-even"><td><p>eval</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">inline::meta-reference</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>files</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">inline::localfs</span></code></p></td>
</tr>
<tr class="row-even"><td><p>inference</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">remote::ollama</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>post_training</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">inline::huggingface</span></code></p></td>
</tr>
<tr class="row-even"><td><p>safety</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">inline::llama-guard</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>scoring</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">inline::basic</span></code>, <code class="docutils literal notranslate"><span class="pre">inline::llm-as-judge</span></code>, <code class="docutils literal notranslate"><span class="pre">inline::braintrust</span></code></p></td>
</tr>
<tr class="row-even"><td><p>telemetry</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">inline::meta-reference</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>tool_runtime</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">remote::brave-search</span></code>, <code class="docutils literal notranslate"><span class="pre">remote::tavily-search</span></code>, <code class="docutils literal notranslate"><span class="pre">inline::rag-runtime</span></code>, <code class="docutils literal notranslate"><span class="pre">remote::model-context-protocol</span></code>, <code class="docutils literal notranslate"><span class="pre">remote::wolfram-alpha</span></code></p></td>
</tr>
<tr class="row-even"><td><p>vector_io</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">inline::faiss</span></code>, <code class="docutils literal notranslate"><span class="pre">remote::chromadb</span></code>, <code class="docutils literal notranslate"><span class="pre">remote::pgvector</span></code></p></td>
</tr>
</tbody>
</table>
<p>You should use this distribution if you have a regular desktop machine without very powerful GPUs. Of course, if you have powerful GPUs, you can still continue using this distribution since Ollama supports GPU acceleration.</p>
<section id="environment-variables">
<h2>Environment Variables<a class="headerlink" href="#environment-variables" title="Link to this heading"></a></h2>
<p>The following environment variables can be configured:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">LLAMA_STACK_PORT</span></code>: Port for the Llama Stack distribution server (default: <code class="docutils literal notranslate"><span class="pre">8321</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">OLLAMA_URL</span></code>: URL of the Ollama server (default: <code class="docutils literal notranslate"><span class="pre">http://127.0.0.1:11434</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">INFERENCE_MODEL</span></code>: Inference model loaded into the Ollama server (default: <code class="docutils literal notranslate"><span class="pre">meta-llama/Llama-3.2-3B-Instruct</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">SAFETY_MODEL</span></code>: Safety model loaded into the Ollama server (default: <code class="docutils literal notranslate"><span class="pre">meta-llama/Llama-Guard-3-1B</span></code>)</p></li>
</ul>
</section>
<section id="setting-up-ollama-server">
<h2>Setting up Ollama server<a class="headerlink" href="#setting-up-ollama-server" title="Link to this heading"></a></h2>
<p>Please check the <a class="reference external" href="https://github.com/ollama/ollama">Ollama Documentation</a> on how to install and run Ollama. After installing Ollama, you need to run <code class="docutils literal notranslate"><span class="pre">ollama</span> <span class="pre">serve</span></code> to start the server.</p>
<p>In order to load models, you can run:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">INFERENCE_MODEL</span><span class="o">=</span><span class="s2">&quot;meta-llama/Llama-3.2-3B-Instruct&quot;</span>

<span class="c1"># ollama names this model differently, and we must use the ollama name when loading the model</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">OLLAMA_INFERENCE_MODEL</span><span class="o">=</span><span class="s2">&quot;llama3.2:3b-instruct-fp16&quot;</span>
ollama<span class="w"> </span>run<span class="w"> </span><span class="nv">$OLLAMA_INFERENCE_MODEL</span><span class="w"> </span>--keepalive<span class="w"> </span>60m
</pre></div>
</div>
<p>If you are using Llama Stack Safety / Shield APIs, you will also need to pull and run the safety model.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">SAFETY_MODEL</span><span class="o">=</span><span class="s2">&quot;meta-llama/Llama-Guard-3-1B&quot;</span>

<span class="c1"># ollama names this model differently, and we must use the ollama name when loading the model</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">OLLAMA_SAFETY_MODEL</span><span class="o">=</span><span class="s2">&quot;llama-guard3:1b&quot;</span>
ollama<span class="w"> </span>run<span class="w"> </span><span class="nv">$OLLAMA_SAFETY_MODEL</span><span class="w"> </span>--keepalive<span class="w"> </span>60m
</pre></div>
</div>
</section>
<section id="running-llama-stack">
<h2>Running Llama Stack<a class="headerlink" href="#running-llama-stack" title="Link to this heading"></a></h2>
<p>Now you are ready to run Llama Stack with Ollama as the inference provider. You can do this via Conda (build code) or Docker which has a pre-built image.</p>
<section id="via-docker">
<h3>Via Docker<a class="headerlink" href="#via-docker" title="Link to this heading"></a></h3>
<p>This method allows you to get started quickly without having to build the distribution code.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">LLAMA_STACK_PORT</span><span class="o">=</span><span class="m">8321</span>
docker<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-it<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--pull<span class="w"> </span>always<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-p<span class="w"> </span><span class="nv">$LLAMA_STACK_PORT</span>:<span class="nv">$LLAMA_STACK_PORT</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-v<span class="w"> </span>~/.llama:/root/.llama<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>llamastack/distribution-ollama<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--port<span class="w"> </span><span class="nv">$LLAMA_STACK_PORT</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">INFERENCE_MODEL</span><span class="o">=</span><span class="nv">$INFERENCE_MODEL</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">OLLAMA_URL</span><span class="o">=</span>http://host.docker.internal:11434
</pre></div>
</div>
<p>If you are using Llama Stack Safety / Shield APIs, use:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># You need a local checkout of llama-stack to run this, get it using</span>
<span class="c1"># git clone https://github.com/meta-llama/llama-stack.git</span>
<span class="nb">cd</span><span class="w"> </span>/path/to/llama-stack

docker<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-it<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--pull<span class="w"> </span>always<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-p<span class="w"> </span><span class="nv">$LLAMA_STACK_PORT</span>:<span class="nv">$LLAMA_STACK_PORT</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-v<span class="w"> </span>~/.llama:/root/.llama<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-v<span class="w"> </span>./llama_stack/templates/ollama/run-with-safety.yaml:/root/my-run.yaml<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>llamastack/distribution-ollama<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--config<span class="w"> </span>/root/my-run.yaml<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--port<span class="w"> </span><span class="nv">$LLAMA_STACK_PORT</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">INFERENCE_MODEL</span><span class="o">=</span><span class="nv">$INFERENCE_MODEL</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">SAFETY_MODEL</span><span class="o">=</span><span class="nv">$SAFETY_MODEL</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">OLLAMA_URL</span><span class="o">=</span>http://host.docker.internal:11434
</pre></div>
</div>
</section>
<section id="via-conda">
<h3>Via Conda<a class="headerlink" href="#via-conda" title="Link to this heading"></a></h3>
<p>Make sure you have done <code class="docutils literal notranslate"><span class="pre">uv</span> <span class="pre">pip</span> <span class="pre">install</span> <span class="pre">llama-stack</span></code> and have the Llama Stack CLI available.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">LLAMA_STACK_PORT</span><span class="o">=</span><span class="m">8321</span>

llama<span class="w"> </span>stack<span class="w"> </span>build<span class="w"> </span>--template<span class="w"> </span>ollama<span class="w"> </span>--image-type<span class="w"> </span>conda
llama<span class="w"> </span>stack<span class="w"> </span>run<span class="w"> </span>./run.yaml<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--port<span class="w"> </span><span class="nv">$LLAMA_STACK_PORT</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">INFERENCE_MODEL</span><span class="o">=</span><span class="nv">$INFERENCE_MODEL</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">OLLAMA_URL</span><span class="o">=</span>http://localhost:11434
</pre></div>
</div>
<p>If you are using Llama Stack Safety / Shield APIs, use:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>llama<span class="w"> </span>stack<span class="w"> </span>run<span class="w"> </span>./run-with-safety.yaml<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--port<span class="w"> </span><span class="nv">$LLAMA_STACK_PORT</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">INFERENCE_MODEL</span><span class="o">=</span><span class="nv">$INFERENCE_MODEL</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">SAFETY_MODEL</span><span class="o">=</span><span class="nv">$SAFETY_MODEL</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">OLLAMA_URL</span><span class="o">=</span>http://localhost:11434
</pre></div>
</div>
</section>
<section id="optional-update-model-serving-configuration">
<h3>(Optional) Update Model Serving Configuration<a class="headerlink" href="#optional-update-model-serving-configuration" title="Link to this heading"></a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Please check the <a class="reference external" href="https://github.com/meta-llama/llama-stack/blob/main/llama_stack/providers/remote/inference/ollama/models.py">model_entries</a> for the supported Ollama models.</p>
</div>
<p>To serve a new model with <code class="docutils literal notranslate"><span class="pre">ollama</span></code></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ollama<span class="w"> </span>run<span class="w"> </span>&lt;model_name&gt;
</pre></div>
</div>
<p>To make sure that the model is being served correctly, run <code class="docutils literal notranslate"><span class="pre">ollama</span> <span class="pre">ps</span></code> to get a list of models being served by ollama.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ ollama ps
NAME                         ID              SIZE      PROCESSOR    UNTIL
llama3.2:3b-instruct-fp16    195a8c01d91e    8.6 GB    100% GPU     9 minutes from now
</pre></div>
</div>
<p>To verify that the model served by ollama is correctly connected to Llama Stack server</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>llama-stack-client<span class="w"> </span>models<span class="w"> </span>list

Available<span class="w"> </span>Models

┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━┓
┃<span class="w"> </span>model_type<span class="w">   </span>┃<span class="w"> </span>identifier<span class="w">                           </span>┃<span class="w"> </span>provider_resource_id<span class="w">         </span>┃<span class="w"> </span>metadata<span class="w">  </span>┃<span class="w"> </span>provider_id<span class="w"> </span>┃
┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━┩
│<span class="w"> </span>llm<span class="w">          </span>│<span class="w"> </span>meta-llama/Llama-3.2-3B-Instruct<span class="w">     </span>│<span class="w"> </span>llama3.2:3b-instruct-fp16<span class="w">    </span>│<span class="w">           </span>│<span class="w"> </span>ollama<span class="w">      </span>│
└──────────────┴──────────────────────────────────────┴──────────────────────────────┴───────────┴─────────────┘

Total<span class="w"> </span>models:<span class="w"> </span><span class="m">1</span>
</pre></div>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="nvidia.html" class="btn btn-neutral float-left" title="NVIDIA Distribution" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="together.html" class="btn btn-neutral float-right" title="Together Distribution" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Meta.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  
<div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    <span class="fa fa-book"> Read the Docs</span>
    v: v0.2.14
    <span class="fa fa-caret-down"></span>
  </span>
  <div class="rst-other-versions">
    <dl>
      <dt>Versions</dt>
      <dd>
        <a href="/v0.2.19/">latest</a>
      </dd>
      <dd>
        <a href="/v0.2.19/">v0.2.19</a>
      </dd>
      <dd>
        <a href="/v0.2.18/">v0.2.18</a>
      </dd>
      <dd>
        <a href="/v0.2.17/">v0.2.17</a>
      </dd>
      <dd>
        <a href="/v0.2.16/">v0.2.16</a>
      </dd>
      <dd>
        <a href="/v0.2.15/">v0.2.15</a>
      </dd>
      <dd class="rtd-current-item">
        <a href="/v0.2.14/">v0.2.14</a>
      </dd>
      <dd>
        <a href="/v0.2.13/">v0.2.13</a>
      </dd>
      <dd>
        <a href="/v0.2.12/">v0.2.12</a>
      </dd>
      <dd>
        <a href="/v0.2.11/">v0.2.11</a>
      </dd>
    </dl>
  </div>
</div><script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>