


<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>iOS SDK &mdash; llama-stack 0.2.15 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/my_theme.css?v=f1163765" />
      <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
      <link rel="stylesheet" type="text/css" href="../../_static/dark_mode_css/general.css?v=c0a7eb24" />
      <link rel="stylesheet" type="text/css" href="../../_static/dark_mode_css/dark.css?v=70edf1c7" />

  
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=d5b904a0"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../_static/copybutton.js?v=09bf800d"></script>
      <script src="../../_static/js/detect_theme.js?v=76226c80"></script>
      <script src="../../_static/js/keyboard_shortcuts.js"></script>
      <script src="../../_static/design-tabs.js?v=f930bc37"></script>
      <script src="../../_static/dark_mode_js/default_light.js?v=c2e647ce"></script>
      <script src="../../_static/dark_mode_js/theme_switcher.js?v=358d3910"></script>
    <script src="../../_static/js/theme.js"></script>
    <script src="../../_static/js/versions.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Llama Stack Client Kotlin API Library" href="android_sdk.html" />
    <link rel="prev" title="Remote-Hosted Distributions" href="../remote_hosted_distro/index.html" />
 

<script src="../../_static/version-loader.js"></script>

</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #2980B9" >

          
          
          <a href="../../index.html" class="icon icon-home">
            llama-stack
          </a>
              <div class="switch-menus">
                <div class="version-switch"></div>
                <div class="language-switch"></div>
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../index.html">Llama Stack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/index.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../concepts/index.html">Core Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../providers/index.html">API Providers Overview</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Distributions Overview</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../list_of_distributions.html">Available Distributions</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../list_of_distributions.html#quick-reference">Quick Reference</a></li>
<li class="toctree-l3"><a class="reference internal" href="../list_of_distributions.html#choose-your-distribution">Choose Your Distribution</a></li>
<li class="toctree-l3 current"><a class="reference internal" href="../list_of_distributions.html#detailed-documentation">Detailed Documentation</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="../list_of_distributions.html#self-hosted-distributions">Self-Hosted Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../list_of_distributions.html#remote-hosted-solutions">Remote-Hosted Solutions</a></li>
<li class="toctree-l4 current"><a class="reference internal" href="../list_of_distributions.html#mobile-sdks">Mobile SDKs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../list_of_distributions.html#decision-flow">Decision Flow</a></li>
<li class="toctree-l3"><a class="reference internal" href="../list_of_distributions.html#next-steps">Next Steps</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../building_distro.html">Build your own Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../customizing_run_yaml.html">Customizing run.yaml Files</a></li>
<li class="toctree-l2"><a class="reference internal" href="../importing_as_library.html">Using Llama Stack as a Library</a></li>
<li class="toctree-l2"><a class="reference internal" href="../configuration.html">Configuring a “Stack”</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced_apis/index.html">Advanced APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../building_applications/index.html">AI Application Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../deploying/index.html">Deployment Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../contributing/index.html">Contributing to Llama-Stack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../references/index.html">References</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: #2980B9" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">llama-stack</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html">Distributions Overview</a></li>
          <li class="breadcrumb-item"><a href="../list_of_distributions.html">Available Distributions</a></li>
      <li class="breadcrumb-item active">iOS SDK</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/distributions/ondevice_distro/ios_sdk.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="ios-sdk">
<h1>iOS SDK<a class="headerlink" href="#ios-sdk" title="Link to this heading"></a></h1>
<p>We offer both remote and on-device use of Llama Stack in Swift via a single SDK <a class="reference external" href="https://github.com/meta-llama/llama-stack-client-swift/">llama-stack-client-swift</a> that contains two components:</p>
<ol class="arabic simple">
<li><p>LlamaStackClient for remote</p></li>
<li><p>Local Inference for on-device</p></li>
</ol>
<a class="reference internal image-reference" href="../../_images/remote_or_local.gif"><img alt="Seamlessly switching between local, on-device inference and remote hosted inference" class="align-center" src="../../_images/remote_or_local.gif" style="width: 412px;" />
</a>
<section id="remote-only">
<h2>Remote Only<a class="headerlink" href="#remote-only" title="Link to this heading"></a></h2>
<p>If you don’t want to run inference on-device, then you can connect to any hosted Llama Stack distribution with #1.</p>
<ol class="arabic simple">
<li><p>Add <code class="docutils literal notranslate"><span class="pre">https://github.com/meta-llama/llama-stack-client-swift/</span></code> as a Package Dependency in Xcode</p></li>
<li><p>Add <code class="docutils literal notranslate"><span class="pre">LlamaStackClient</span></code> as a framework to your app target</p></li>
<li><p>Call an API:</p></li>
</ol>
<div class="highlight-swift notranslate"><div class="highlight"><pre><span></span><span class="kd">import</span><span class="w"> </span><span class="nc">LlamaStackClient</span>

<span class="kd">let</span><span class="w"> </span><span class="nv">agents</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">RemoteAgents</span><span class="p">(</span><span class="n">url</span><span class="p">:</span><span class="w"> </span><span class="n">URL</span><span class="p">(</span><span class="n">string</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;http://localhost:8321&quot;</span><span class="p">)</span><span class="o">!</span><span class="p">)</span>
<span class="kd">let</span><span class="w"> </span><span class="nv">request</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">Components</span><span class="p">.</span><span class="n">Schemas</span><span class="p">.</span><span class="n">CreateAgentTurnRequest</span><span class="p">(</span>
<span class="w">        </span><span class="n">agent_id</span><span class="p">:</span><span class="w"> </span><span class="n">agentId</span><span class="p">,</span>
<span class="w">        </span><span class="n">messages</span><span class="p">:</span><span class="w"> </span><span class="p">[</span>
<span class="w">          </span><span class="p">.</span><span class="n">UserMessage</span><span class="p">(</span><span class="n">Components</span><span class="p">.</span><span class="n">Schemas</span><span class="p">.</span><span class="n">UserMessage</span><span class="p">(</span>
<span class="w">            </span><span class="n">content</span><span class="p">:</span><span class="w"> </span><span class="p">.</span><span class="n">case1</span><span class="p">(</span><span class="s">&quot;Hello Llama!&quot;</span><span class="p">),</span>
<span class="w">            </span><span class="n">role</span><span class="p">:</span><span class="w"> </span><span class="p">.</span><span class="n">user</span>
<span class="w">          </span><span class="p">))</span>
<span class="w">        </span><span class="p">],</span>
<span class="w">        </span><span class="n">session_id</span><span class="p">:</span><span class="w"> </span><span class="kc">self</span><span class="p">.</span><span class="n">agenticSystemSessionId</span><span class="p">,</span>
<span class="w">        </span><span class="n">stream</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span>
<span class="w">      </span><span class="p">)</span>

<span class="w">      </span><span class="k">for</span><span class="w"> </span><span class="k">try</span><span class="w"> </span><span class="k">await</span><span class="w"> </span><span class="n">chunk</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="k">try</span><span class="w"> </span><span class="k">await</span><span class="w"> </span><span class="n">agents</span><span class="p">.</span><span class="n">createTurn</span><span class="p">(</span><span class="n">request</span><span class="p">:</span><span class="w"> </span><span class="n">request</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="kd">let</span><span class="w"> </span><span class="nv">payload</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">chunk</span><span class="p">.</span><span class="n">event</span><span class="p">.</span><span class="n">payload</span>
<span class="w">      </span><span class="c1">// ...</span>
</pre></div>
</div>
<p>Check out <a class="reference external" href="https://github.com/meta-llama/llama-stack-client-swift/tree/main/examples/ios_calendar_assistant">iOSCalendarAssistant</a> for a complete app demo.</p>
</section>
<section id="localinference">
<h2>LocalInference<a class="headerlink" href="#localinference" title="Link to this heading"></a></h2>
<p>LocalInference provides a local inference implementation powered by <a class="reference external" href="https://github.com/pytorch/executorch/">executorch</a>.</p>
<p>Llama Stack currently supports on-device inference for iOS with Android coming soon. You can run on-device inference on Android today using <a class="reference external" href="https://github.com/pytorch/executorch/tree/main/examples/demo-apps/android/LlamaDemo">executorch</a>, PyTorch’s on-device inference library.</p>
<p>The APIs <em>work the same as remote</em> – the only difference is you’ll instead use the <code class="docutils literal notranslate"><span class="pre">LocalAgents</span></code> / <code class="docutils literal notranslate"><span class="pre">LocalInference</span></code> classes and pass in a <code class="docutils literal notranslate"><span class="pre">DispatchQueue</span></code>:</p>
<div class="highlight-swift notranslate"><div class="highlight"><pre><span></span><span class="kd">private</span><span class="w"> </span><span class="kd">let</span><span class="w"> </span><span class="nv">runnerQueue</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">DispatchQueue</span><span class="p">(</span><span class="n">label</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;org.llamastack.stacksummary&quot;</span><span class="p">)</span>
<span class="kd">let</span><span class="w"> </span><span class="nv">inference</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">LocalInference</span><span class="p">(</span><span class="n">queue</span><span class="p">:</span><span class="w"> </span><span class="n">runnerQueue</span><span class="p">)</span>
<span class="kd">let</span><span class="w"> </span><span class="nv">agents</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">LocalAgents</span><span class="p">(</span><span class="n">inference</span><span class="p">:</span><span class="w"> </span><span class="kc">self</span><span class="p">.</span><span class="n">inference</span><span class="p">)</span>
</pre></div>
</div>
<p>Check out <a class="reference external" href="https://github.com/meta-llama/llama-stack-client-swift/tree/main/examples/ios_calendar_assistant">iOSCalendarAssistantWithLocalInf</a> for a complete app demo.</p>
<section id="installation">
<h3>Installation<a class="headerlink" href="#installation" title="Link to this heading"></a></h3>
<p>We’re working on making LocalInference easier to set up. For now, you’ll need to import it via <code class="docutils literal notranslate"><span class="pre">.xcframework</span></code>:</p>
<ol class="arabic simple">
<li><p>Clone the executorch submodule in this repo and its dependencies: <code class="docutils literal notranslate"><span class="pre">git</span> <span class="pre">submodule</span> <span class="pre">update</span> <span class="pre">--init</span> <span class="pre">--recursive</span></code></p></li>
<li><p>Install <a class="reference external" href="https://cmake.org/">Cmake</a> for the executorch build`</p></li>
<li><p>Drag <code class="docutils literal notranslate"><span class="pre">LocalInference.xcodeproj</span></code> into your project</p></li>
<li><p>Add <code class="docutils literal notranslate"><span class="pre">LocalInference</span></code> as a framework in your app target</p></li>
</ol>
</section>
<section id="preparing-a-model">
<h3>Preparing a model<a class="headerlink" href="#preparing-a-model" title="Link to this heading"></a></h3>
<ol class="arabic simple">
<li><p>Prepare a <code class="docutils literal notranslate"><span class="pre">.pte</span></code> file <a class="reference external" href="https://github.com/pytorch/executorch/blob/main/examples/models/llama/README.md#step-2-prepare-model">following the executorch docs</a></p></li>
<li><p>Bundle the <code class="docutils literal notranslate"><span class="pre">.pte</span></code> and <code class="docutils literal notranslate"><span class="pre">tokenizer.model</span></code> file into your app</p></li>
</ol>
<p>We now support models quantized using SpinQuant and QAT-LoRA which offer a significant performance boost (demo app on iPhone 13 Pro):</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Llama 3.2 1B</p></th>
<th class="head text-left"><p>Tokens / Second (total)</p></th>
<th class="head text-left"><p></p></th>
<th class="head text-left"><p>Time-to-First-Token (sec)</p></th>
<th class="head text-left"><p></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p></p></td>
<td class="text-left"><p>Haiku</p></td>
<td class="text-left"><p>Paragraph</p></td>
<td class="text-left"><p>Haiku</p></td>
<td class="text-left"><p>Paragraph</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>BF16</p></td>
<td class="text-left"><p>2.2</p></td>
<td class="text-left"><p>2.5</p></td>
<td class="text-left"><p>2.3</p></td>
<td class="text-left"><p>1.9</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>QAT+LoRA</p></td>
<td class="text-left"><p>7.1</p></td>
<td class="text-left"><p>3.3</p></td>
<td class="text-left"><p>0.37</p></td>
<td class="text-left"><p>0.24</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>SpinQuant</p></td>
<td class="text-left"><p>10.1</p></td>
<td class="text-left"><p>5.2</p></td>
<td class="text-left"><p>0.2</p></td>
<td class="text-left"><p>0.2</p></td>
</tr>
</tbody>
</table>
</section>
<section id="using-localinference">
<h3>Using LocalInference<a class="headerlink" href="#using-localinference" title="Link to this heading"></a></h3>
<ol class="arabic simple">
<li><p>Instantiate LocalInference with a DispatchQueue. Optionally, pass it into your agents service:</p></li>
</ol>
<div class="highlight-swift notranslate"><div class="highlight"><pre><span></span><span class="w">  </span><span class="kd">init</span><span class="w"> </span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">runnerQueue</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">DispatchQueue</span><span class="p">(</span><span class="n">label</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;org.meta.llamastack&quot;</span><span class="p">)</span>
<span class="w">    </span><span class="n">inferenceService</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">LocalInferenceService</span><span class="p">(</span><span class="n">queue</span><span class="p">:</span><span class="w"> </span><span class="n">runnerQueue</span><span class="p">)</span>
<span class="w">    </span><span class="n">agentsService</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">LocalAgentsService</span><span class="p">(</span><span class="n">inference</span><span class="p">:</span><span class="w"> </span><span class="n">inferenceService</span><span class="p">)</span>
<span class="w">  </span><span class="p">}</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>Before making any inference calls, load your model from your bundle:</p></li>
</ol>
<div class="highlight-swift notranslate"><div class="highlight"><pre><span></span><span class="kd">let</span><span class="w"> </span><span class="nv">mainBundle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="n">Bundle</span><span class="p">.</span><span class="n">main</span>
<span class="n">inferenceService</span><span class="p">.</span><span class="n">loadModel</span><span class="p">(</span>
<span class="w">    </span><span class="n">modelPath</span><span class="p">:</span><span class="w"> </span><span class="n">mainBundle</span><span class="p">.</span><span class="n">url</span><span class="p">(</span><span class="n">forResource</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;llama32_1b_spinquant&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">withExtension</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;pte&quot;</span><span class="p">),</span>
<span class="w">    </span><span class="n">tokenizerPath</span><span class="p">:</span><span class="w"> </span><span class="n">mainBundle</span><span class="p">.</span><span class="n">url</span><span class="p">(</span><span class="n">forResource</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;tokenizer&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">withExtension</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;model&quot;</span><span class="p">),</span>
<span class="w">    </span><span class="n">completion</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="kc">_</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="p">}</span><span class="w"> </span><span class="c1">// use to handle load failures</span>
<span class="p">)</span>
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p>Make inference calls (or agents calls) as you normally would with LlamaStack:</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="k">await</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="k">try</span> <span class="k">await</span> <span class="n">agentsService</span><span class="o">.</span><span class="n">initAndCreateTurn</span><span class="p">(</span>
    <span class="n">messages</span><span class="p">:</span> <span class="p">[</span>
    <span class="o">.</span><span class="n">UserMessage</span><span class="p">(</span><span class="n">Components</span><span class="o">.</span><span class="n">Schemas</span><span class="o">.</span><span class="n">UserMessage</span><span class="p">(</span>
        <span class="n">content</span><span class="p">:</span> <span class="o">.</span><span class="n">case1</span><span class="p">(</span><span class="s2">&quot;Call functions as needed to handle any actions in the following text:</span><span class="se">\n\n</span><span class="s2">&quot;</span> <span class="o">+</span> <span class="n">text</span><span class="p">),</span>
        <span class="n">role</span><span class="p">:</span> <span class="o">.</span><span class="n">user</span><span class="p">))</span>
    <span class="p">]</span>
<span class="p">)</span> <span class="p">{</span>
</pre></div>
</div>
</section>
<section id="troubleshooting">
<h3>Troubleshooting<a class="headerlink" href="#troubleshooting" title="Link to this heading"></a></h3>
<p>If you receive errors like “missing package product” or “invalid checksum”, try cleaning the build folder and resetting the Swift package cache:</p>
<p>(Opt+Click) Product &gt; Clean Build Folder Immediately</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">rm</span> <span class="o">-</span><span class="n">rf</span> \
  <span class="o">~/</span><span class="n">Library</span><span class="o">/</span><span class="n">org</span><span class="o">.</span><span class="n">swift</span><span class="o">.</span><span class="n">swiftpm</span> \
  <span class="o">~/</span><span class="n">Library</span><span class="o">/</span><span class="n">Caches</span><span class="o">/</span><span class="n">org</span><span class="o">.</span><span class="n">swift</span><span class="o">.</span><span class="n">swiftpm</span> \
  <span class="o">~/</span><span class="n">Library</span><span class="o">/</span><span class="n">Caches</span><span class="o">/</span><span class="n">com</span><span class="o">.</span><span class="n">apple</span><span class="o">.</span><span class="n">dt</span><span class="o">.</span><span class="n">Xcode</span> \
  <span class="o">~/</span><span class="n">Library</span><span class="o">/</span><span class="n">Developer</span><span class="o">/</span><span class="n">Xcode</span><span class="o">/</span><span class="n">DerivedData</span>
</pre></div>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../remote_hosted_distro/index.html" class="btn btn-neutral float-left" title="Remote-Hosted Distributions" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="android_sdk.html" class="btn btn-neutral float-right" title="Llama Stack Client Kotlin API Library" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Meta.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  
<div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    <span class="fa fa-book"> Read the Docs</span>
    v: v0.2.15
    <span class="fa fa-caret-down"></span>
  </span>
  <div class="rst-other-versions">
    <dl>
      <dt>Versions</dt>
      <dd>
        <a href="/v0.2.19/">latest</a>
      </dd>
      <dd>
        <a href="/v0.2.19/">v0.2.19</a>
      </dd>
      <dd>
        <a href="/v0.2.18/">v0.2.18</a>
      </dd>
      <dd>
        <a href="/v0.2.17/">v0.2.17</a>
      </dd>
      <dd>
        <a href="/v0.2.16/">v0.2.16</a>
      </dd>
      <dd class="rtd-current-item">
        <a href="/v0.2.15/">v0.2.15</a>
      </dd>
      <dd>
        <a href="/v0.2.14/">v0.2.14</a>
      </dd>
      <dd>
        <a href="/v0.2.13/">v0.2.13</a>
      </dd>
      <dd>
        <a href="/v0.2.12/">v0.2.12</a>
      </dd>
      <dd>
        <a href="/v0.2.11/">v0.2.11</a>
      </dd>
    </dl>
  </div>
</div><script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>