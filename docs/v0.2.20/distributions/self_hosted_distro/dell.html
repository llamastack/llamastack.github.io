


<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Dell Distribution of Llama Stack &mdash; llama-stack 0.2.20 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/my_theme.css?v=f1163765" />
      <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
      <link rel="stylesheet" type="text/css" href="../../_static/dark_mode_css/general.css?v=c0a7eb24" />
      <link rel="stylesheet" type="text/css" href="../../_static/dark_mode_css/dark.css?v=70edf1c7" />

  
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=20e0ead2"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../_static/copybutton.js?v=09bf800d"></script>
      <script src="../../_static/js/detect_theme.js?v=76226c80"></script>
      <script src="../../_static/js/keyboard_shortcuts.js?v=62563c3b"></script>
      <script src="../../_static/design-tabs.js?v=f930bc37"></script>
      <script src="../../_static/dark_mode_js/default_light.js?v=c2e647ce"></script>
      <script src="../../_static/dark_mode_js/theme_switcher.js?v=358d3910"></script>
    <script src="../../_static/js/theme.js"></script>
    <script src="../../_static/js/versions.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
 

<script src="../../_static/version-loader.js"></script>

</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #2980B9" >

          
          
          <a href="../../index.html" class="icon icon-home">
            llama-stack
          </a>
              <div class="switch-menus">
                <div class="version-switch"></div>
                <div class="language-switch"></div>
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../index.html">Llama Stack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/index.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../concepts/index.html">Core Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../providers/index.html">API Providers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../index.html">Distributions Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced_apis/index.html">Advanced APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../building_applications/index.html">AI Application Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../deploying/index.html">Deployment Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../contributing/index.html">Contributing to Llama Stack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../contributing/index.html#llama-stack-benchmark-suite-on-kubernetes">Llama Stack Benchmark Suite on Kubernetes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../references/index.html">References</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: #2980B9" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">llama-stack</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Dell Distribution of Llama Stack</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/distributions/self_hosted_distro/dell.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <!-- This file was auto-generated by distro_codegen.py, please edit source -->
<section class="tex2jax_ignore mathjax_ignore" id="dell-distribution-of-llama-stack">
<h1>Dell Distribution of Llama Stack<a class="headerlink" href="#dell-distribution-of-llama-stack" title="Link to this heading"></a></h1>
<div class="toctree-wrapper compound">
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">llamastack/distribution-dell</span></code> distribution consists of the following provider configurations.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>API</p></th>
<th class="head"><p>Provider(s)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>agents</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">inline::meta-reference</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>datasetio</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">remote::huggingface</span></code>, <code class="docutils literal notranslate"><span class="pre">inline::localfs</span></code></p></td>
</tr>
<tr class="row-even"><td><p>eval</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">inline::meta-reference</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>inference</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">remote::tgi</span></code>, <code class="docutils literal notranslate"><span class="pre">inline::sentence-transformers</span></code></p></td>
</tr>
<tr class="row-even"><td><p>safety</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">inline::llama-guard</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>scoring</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">inline::basic</span></code>, <code class="docutils literal notranslate"><span class="pre">inline::llm-as-judge</span></code>, <code class="docutils literal notranslate"><span class="pre">inline::braintrust</span></code></p></td>
</tr>
<tr class="row-even"><td><p>telemetry</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">inline::meta-reference</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>tool_runtime</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">remote::brave-search</span></code>, <code class="docutils literal notranslate"><span class="pre">remote::tavily-search</span></code>, <code class="docutils literal notranslate"><span class="pre">inline::rag-runtime</span></code></p></td>
</tr>
<tr class="row-even"><td><p>vector_io</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">inline::faiss</span></code>, <code class="docutils literal notranslate"><span class="pre">remote::chromadb</span></code>, <code class="docutils literal notranslate"><span class="pre">remote::pgvector</span></code></p></td>
</tr>
</tbody>
</table>
<p>You can use this distribution if you have GPUs and want to run an independent TGI or Dell Enterprise Hub container for running inference.</p>
<section id="environment-variables">
<h2>Environment Variables<a class="headerlink" href="#environment-variables" title="Link to this heading"></a></h2>
<p>The following environment variables can be configured:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">DEH_URL</span></code>: URL for the Dell inference server (default: <code class="docutils literal notranslate"><span class="pre">http://0.0.0.0:8181</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">DEH_SAFETY_URL</span></code>: URL for the Dell safety inference server (default: <code class="docutils literal notranslate"><span class="pre">http://0.0.0.0:8282</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">CHROMA_URL</span></code>: URL for the Chroma server (default: <code class="docutils literal notranslate"><span class="pre">http://localhost:6601</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">INFERENCE_MODEL</span></code>: Inference model loaded into the TGI server (default: <code class="docutils literal notranslate"><span class="pre">meta-llama/Llama-3.2-3B-Instruct</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">SAFETY_MODEL</span></code>: Name of the safety (Llama-Guard) model to use (default: <code class="docutils literal notranslate"><span class="pre">meta-llama/Llama-Guard-3-1B</span></code>)</p></li>
</ul>
</section>
<section id="setting-up-inference-server-using-dell-enterprise-hubs-custom-tgi-container">
<h2>Setting up Inference server using Dell Enterprise Hub’s custom TGI container.<a class="headerlink" href="#setting-up-inference-server-using-dell-enterprise-hubs-custom-tgi-container" title="Link to this heading"></a></h2>
<p>NOTE: This is a placeholder to run inference with TGI. This will be updated to use <a class="reference external" href="https://dell.huggingface.co/authenticated/models">Dell Enterprise Hub’s containers</a> once verified.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">INFERENCE_PORT</span><span class="o">=</span><span class="m">8181</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">DEH_URL</span><span class="o">=</span>http://0.0.0.0:<span class="nv">$INFERENCE_PORT</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">INFERENCE_MODEL</span><span class="o">=</span>meta-llama/Llama-3.1-8B-Instruct
<span class="nb">export</span><span class="w"> </span><span class="nv">CHROMADB_HOST</span><span class="o">=</span>localhost
<span class="nb">export</span><span class="w"> </span><span class="nv">CHROMADB_PORT</span><span class="o">=</span><span class="m">6601</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">CHROMA_URL</span><span class="o">=</span>http://<span class="nv">$CHROMADB_HOST</span>:<span class="nv">$CHROMADB_PORT</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">LLAMA_STACK_PORT</span><span class="o">=</span><span class="m">8321</span>

docker<span class="w"> </span>run<span class="w"> </span>--rm<span class="w"> </span>-it<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--pull<span class="w"> </span>always<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--network<span class="w"> </span>host<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-v<span class="w"> </span><span class="nv">$HOME</span>/.cache/huggingface:/data<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-e<span class="w"> </span><span class="nv">HF_TOKEN</span><span class="o">=</span><span class="nv">$HF_TOKEN</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-p<span class="w"> </span><span class="nv">$INFERENCE_PORT</span>:<span class="nv">$INFERENCE_PORT</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--gpus<span class="w"> </span><span class="nv">$CUDA_VISIBLE_DEVICES</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>ghcr.io/huggingface/text-generation-inference<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--dtype<span class="w"> </span>bfloat16<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--usage-stats<span class="w"> </span>off<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--sharded<span class="w"> </span><span class="nb">false</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--cuda-memory-fraction<span class="w"> </span><span class="m">0</span>.7<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--model-id<span class="w"> </span><span class="nv">$INFERENCE_MODEL</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--port<span class="w"> </span><span class="nv">$INFERENCE_PORT</span><span class="w"> </span>--hostname<span class="w"> </span><span class="m">0</span>.0.0.0
</pre></div>
</div>
<p>If you are using Llama Stack Safety / Shield APIs, then you will need to also run another instance of a TGI with a corresponding safety model like <code class="docutils literal notranslate"><span class="pre">meta-llama/Llama-Guard-3-1B</span></code> using a script like:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">SAFETY_INFERENCE_PORT</span><span class="o">=</span><span class="m">8282</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">DEH_SAFETY_URL</span><span class="o">=</span>http://0.0.0.0:<span class="nv">$SAFETY_INFERENCE_PORT</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">SAFETY_MODEL</span><span class="o">=</span>meta-llama/Llama-Guard-3-1B
<span class="nb">export</span><span class="w"> </span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">1</span>

docker<span class="w"> </span>run<span class="w"> </span>--rm<span class="w"> </span>-it<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--pull<span class="w"> </span>always<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--network<span class="w"> </span>host<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-v<span class="w"> </span><span class="nv">$HOME</span>/.cache/huggingface:/data<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-e<span class="w"> </span><span class="nv">HF_TOKEN</span><span class="o">=</span><span class="nv">$HF_TOKEN</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-p<span class="w"> </span><span class="nv">$SAFETY_INFERENCE_PORT</span>:<span class="nv">$SAFETY_INFERENCE_PORT</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--gpus<span class="w"> </span><span class="nv">$CUDA_VISIBLE_DEVICES</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>ghcr.io/huggingface/text-generation-inference<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--dtype<span class="w"> </span>bfloat16<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--usage-stats<span class="w"> </span>off<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--sharded<span class="w"> </span><span class="nb">false</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--cuda-memory-fraction<span class="w"> </span><span class="m">0</span>.7<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--model-id<span class="w"> </span><span class="nv">$SAFETY_MODEL</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--hostname<span class="w"> </span><span class="m">0</span>.0.0.0<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--port<span class="w"> </span><span class="nv">$SAFETY_INFERENCE_PORT</span>
</pre></div>
</div>
</section>
<section id="dell-distribution-relies-on-chromadb-for-vector-database-usage">
<h2>Dell distribution relies on ChromaDB for vector database usage<a class="headerlink" href="#dell-distribution-relies-on-chromadb-for-vector-database-usage" title="Link to this heading"></a></h2>
<p>You can start a chroma-db easily using docker.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># This is where the indices are persisted</span>
mkdir<span class="w"> </span>-p<span class="w"> </span><span class="nv">$HOME</span>/chromadb

podman<span class="w"> </span>run<span class="w"> </span>--rm<span class="w"> </span>-it<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--network<span class="w"> </span>host<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--name<span class="w"> </span>chromadb<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-v<span class="w"> </span><span class="nv">$HOME</span>/chromadb:/chroma/chroma<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-e<span class="w"> </span><span class="nv">IS_PERSISTENT</span><span class="o">=</span>TRUE<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>chromadb/chroma:latest<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--port<span class="w"> </span><span class="nv">$CHROMADB_PORT</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--host<span class="w"> </span><span class="nv">$CHROMADB_HOST</span>
</pre></div>
</div>
</section>
<section id="running-llama-stack">
<h2>Running Llama Stack<a class="headerlink" href="#running-llama-stack" title="Link to this heading"></a></h2>
<p>Now you are ready to run Llama Stack with TGI as the inference provider. You can do this via venv or Docker which has a pre-built image.</p>
<section id="via-docker">
<h3>Via Docker<a class="headerlink" href="#via-docker" title="Link to this heading"></a></h3>
<p>This method allows you to get started quickly without having to build the distribution code.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>run<span class="w"> </span>-it<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--pull<span class="w"> </span>always<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--network<span class="w"> </span>host<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-p<span class="w"> </span><span class="nv">$LLAMA_STACK_PORT</span>:<span class="nv">$LLAMA_STACK_PORT</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-v<span class="w"> </span><span class="nv">$HOME</span>/.llama:/root/.llama<span class="w"> </span><span class="se">\</span>
<span class="w">  </span><span class="c1"># NOTE: mount the llama-stack / llama-model directories if testing local changes else not needed</span>
<span class="w">  </span>-v<span class="w"> </span>/home/hjshah/git/llama-stack:/app/llama-stack-source<span class="w"> </span>-v<span class="w"> </span>/home/hjshah/git/llama-models:/app/llama-models-source<span class="w"> </span><span class="se">\</span>
<span class="w">  </span><span class="c1"># localhost/distribution-dell:dev if building / testing locally</span>
<span class="w">  </span>llamastack/distribution-dell<span class="se">\</span>
<span class="w">  </span>--port<span class="w"> </span><span class="nv">$LLAMA_STACK_PORT</span><span class="w">  </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">INFERENCE_MODEL</span><span class="o">=</span><span class="nv">$INFERENCE_MODEL</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">DEH_URL</span><span class="o">=</span><span class="nv">$DEH_URL</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">CHROMA_URL</span><span class="o">=</span><span class="nv">$CHROMA_URL</span>
</pre></div>
</div>
<p>If you are using Llama Stack Safety / Shield APIs, use:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># You need a local checkout of llama-stack to run this, get it using</span>
<span class="c1"># git clone https://github.com/meta-llama/llama-stack.git</span>
<span class="nb">cd</span><span class="w"> </span>/path/to/llama-stack

<span class="nb">export</span><span class="w"> </span><span class="nv">SAFETY_INFERENCE_PORT</span><span class="o">=</span><span class="m">8282</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">DEH_SAFETY_URL</span><span class="o">=</span>http://0.0.0.0:<span class="nv">$SAFETY_INFERENCE_PORT</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">SAFETY_MODEL</span><span class="o">=</span>meta-llama/Llama-Guard-3-1B

docker<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-it<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--pull<span class="w"> </span>always<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-p<span class="w"> </span><span class="nv">$LLAMA_STACK_PORT</span>:<span class="nv">$LLAMA_STACK_PORT</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-v<span class="w"> </span><span class="nv">$HOME</span>/.llama:/root/.llama<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-v<span class="w"> </span>./llama_stack/distributions/tgi/run-with-safety.yaml:/root/my-run.yaml<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>llamastack/distribution-dell<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--config<span class="w"> </span>/root/my-run.yaml<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--port<span class="w"> </span><span class="nv">$LLAMA_STACK_PORT</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">INFERENCE_MODEL</span><span class="o">=</span><span class="nv">$INFERENCE_MODEL</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">DEH_URL</span><span class="o">=</span><span class="nv">$DEH_URL</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">SAFETY_MODEL</span><span class="o">=</span><span class="nv">$SAFETY_MODEL</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">DEH_SAFETY_URL</span><span class="o">=</span><span class="nv">$DEH_SAFETY_URL</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">CHROMA_URL</span><span class="o">=</span><span class="nv">$CHROMA_URL</span>
</pre></div>
</div>
</section>
<section id="via-venv">
<h3>Via venv<a class="headerlink" href="#via-venv" title="Link to this heading"></a></h3>
<p>Make sure you have done <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">llama-stack</span></code> and have the Llama Stack CLI available.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>llama<span class="w"> </span>stack<span class="w"> </span>build<span class="w"> </span>--distro<span class="w"> </span>dell<span class="w"> </span>--image-type<span class="w"> </span>venv
llama<span class="w"> </span>stack<span class="w"> </span>run<span class="w"> </span>dell
<span class="w">  </span>--port<span class="w"> </span><span class="nv">$LLAMA_STACK_PORT</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">INFERENCE_MODEL</span><span class="o">=</span><span class="nv">$INFERENCE_MODEL</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">DEH_URL</span><span class="o">=</span><span class="nv">$DEH_URL</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">CHROMA_URL</span><span class="o">=</span><span class="nv">$CHROMA_URL</span>
</pre></div>
</div>
<p>If you are using Llama Stack Safety / Shield APIs, use:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>llama<span class="w"> </span>stack<span class="w"> </span>run<span class="w"> </span>./run-with-safety.yaml<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--port<span class="w"> </span><span class="nv">$LLAMA_STACK_PORT</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">INFERENCE_MODEL</span><span class="o">=</span><span class="nv">$INFERENCE_MODEL</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">DEH_URL</span><span class="o">=</span><span class="nv">$DEH_URL</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">SAFETY_MODEL</span><span class="o">=</span><span class="nv">$SAFETY_MODEL</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">DEH_SAFETY_URL</span><span class="o">=</span><span class="nv">$DEH_SAFETY_URL</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">CHROMA_URL</span><span class="o">=</span><span class="nv">$CHROMA_URL</span>
</pre></div>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Meta.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  
<div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    <span class="fa fa-book"> Read the Docs</span>
    v: v0.2.20
    <span class="fa fa-caret-down"></span>
  </span>
  <div class="rst-other-versions">
    <dl>
      <dt>Versions</dt>
      <dd>
        <a href="/v0.2.19/">latest</a>
      </dd>
      <dd>
        <a href="/v0.2.19/">v0.2.19</a>
      </dd>
      <dd>
        <a href="/v0.2.18/">v0.2.18</a>
      </dd>
      <dd>
        <a href="/v0.2.17/">v0.2.17</a>
      </dd>
      <dd>
        <a href="/v0.2.16/">v0.2.16</a>
      </dd>
      <dd>
        <a href="/v0.2.15/">v0.2.15</a>
      </dd>
      <dd>
        <a href="/v0.2.14/">v0.2.14</a>
      </dd>
      <dd>
        <a href="/v0.2.13/">v0.2.13</a>
      </dd>
      <dd>
        <a href="/v0.2.12/">v0.2.12</a>
      </dd>
      <dd>
        <a href="/v0.2.11/">v0.2.11</a>
      </dd>
    </dl>
  </div>
</div><script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>