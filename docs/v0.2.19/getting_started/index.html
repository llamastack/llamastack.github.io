


<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Getting Started &mdash; llama-stack 0.2.19 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../_static/css/my_theme.css?v=f1163765" />
      <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
      <link rel="stylesheet" type="text/css" href="../_static/dark_mode_css/general.css?v=c0a7eb24" />
      <link rel="stylesheet" type="text/css" href="../_static/dark_mode_css/dark.css?v=70edf1c7" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=0a5b2d08"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../_static/copybutton.js?v=09bf800d"></script>
      <script src="../_static/js/detect_theme.js?v=76226c80"></script>
      <script src="../_static/js/keyboard_shortcuts.js?v=62563c3b"></script>
      <script src="../_static/design-tabs.js?v=f930bc37"></script>
      <script src="../_static/dark_mode_js/default_light.js?v=c2e647ce"></script>
      <script src="../_static/dark_mode_js/theme_switcher.js?v=358d3910"></script>
    <script src="../_static/js/theme.js"></script>
    <script src="../_static/js/versions.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Core Concepts" href="../concepts/index.html" />
    <link rel="prev" title="Llama Stack" href="../index.html" />
 

<script src="../_static/version-loader.js"></script>

</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #2980B9" >

          
          
          <a href="../index.html" class="icon icon-home">
            llama-stack
          </a>
              <div class="switch-menus">
                <div class="version-switch"></div>
                <div class="language-switch"></div>
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../index.html">Llama Stack</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Getting Started</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#quickstart">Quickstart</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#step-1-install-and-setup">Step 1: Install and setup</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-2-run-the-llama-stack-server">Step 2: Run the Llama Stack server</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-3-run-the-demo">Step 3: Run the demo</a></li>
<li class="toctree-l3"><a class="reference internal" href="#next-steps">Next Steps</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#libraries-sdks">Libraries (SDKs)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#detailed-tutorial">Detailed Tutorial</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#step-1-installation-and-setup">Step 1: Installation and Setup</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-2-run-llama-stack">Step 2:  Run Llama Stack</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-3-run-client-cli">Step 3: Run Client CLI</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-4-run-the-demos">Step 4: Run the Demos</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../concepts/index.html">Core Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../providers/index.html">API Providers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../distributions/index.html">Distributions Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced_apis/index.html">Advanced APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../building_applications/index.html">AI Application Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploying/index.html">Deployment Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contributing/index.html">Contributing to Llama Stack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contributing/index.html#llama-stack-benchmark-suite-on-kubernetes">Llama Stack Benchmark Suite on Kubernetes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references/index.html">References</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: #2980B9" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">llama-stack</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Getting Started</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/getting_started/index.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="getting-started">
<h1>Getting Started<a class="headerlink" href="#getting-started" title="Link to this heading">ÔÉÅ</a></h1>
<section id="quickstart">
<h2>Quickstart<a class="headerlink" href="#quickstart" title="Link to this heading">ÔÉÅ</a></h2>
<p>Get started with Llama Stack in minutes!</p>
<p>Llama Stack is a stateful service with REST APIs to support the seamless transition of AI applications across different
environments. You can build and test using a local server first and deploy to a hosted endpoint for production.</p>
<p>In this guide, we‚Äôll walk through how to build a RAG application locally using Llama Stack with <a class="reference external" href="https://ollama.com/">Ollama</a>
as the inference <a class="reference internal" href="../providers/inference/index.html"><span class="doc std std-doc">provider</span></a> for a Llama Model.</p>
<p><strong>üí° Notebook Version:</strong> You can also follow this quickstart guide in a Jupyter notebook format: <a class="reference external" href="https://github.com/meta-llama/llama-stack/blob/main/docs/quick_start.ipynb">quick_start.ipynb</a></p>
<section id="step-1-install-and-setup">
<h3>Step 1: Install and setup<a class="headerlink" href="#step-1-install-and-setup" title="Link to this heading">ÔÉÅ</a></h3>
<ol class="arabic simple">
<li><p>Install <a class="reference external" href="https://docs.astral.sh/uv/">uv</a></p></li>
<li><p>Run inference on a Llama model with <a class="reference external" href="https://ollama.com/download">Ollama</a></p></li>
</ol>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ollama<span class="w"> </span>run<span class="w"> </span>llama3.2:3b<span class="w"> </span>--keepalive<span class="w"> </span>60m
</pre></div>
</div>
</section>
<section id="step-2-run-the-llama-stack-server">
<h3>Step 2: Run the Llama Stack server<a class="headerlink" href="#step-2-run-the-llama-stack-server" title="Link to this heading">ÔÉÅ</a></h3>
<p>We will use <code class="docutils literal notranslate"><span class="pre">uv</span></code> to run the Llama Stack server.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">OLLAMA_URL</span><span class="o">=</span>http://localhost:11434<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>uv<span class="w"> </span>run<span class="w"> </span>--with<span class="w"> </span>llama-stack<span class="w"> </span>llama<span class="w"> </span>stack<span class="w"> </span>build<span class="w"> </span>--distro<span class="w"> </span>starter<span class="w"> </span>--image-type<span class="w"> </span>venv<span class="w"> </span>--run
</pre></div>
</div>
</section>
<section id="step-3-run-the-demo">
<h3>Step 3: Run the demo<a class="headerlink" href="#step-3-run-the-demo" title="Link to this heading">ÔÉÅ</a></h3>
<p>Now open up a new terminal and copy the following script into a file named <code class="docutils literal notranslate"><span class="pre">demo_script.py</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Copyright (c) Meta Platforms, Inc. and affiliates.</span>
<span class="c1"># All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># This source code is licensed under the terms described in the LICENSE file in</span>
<span class="c1"># the root directory of this source tree.</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">llama_stack_client</span><span class="w"> </span><span class="kn">import</span> <span class="n">Agent</span><span class="p">,</span> <span class="n">AgentEventLogger</span><span class="p">,</span> <span class="n">RAGDocument</span><span class="p">,</span> <span class="n">LlamaStackClient</span>

<span class="n">vector_db_id</span> <span class="o">=</span> <span class="s2">&quot;my_demo_vector_db&quot;</span>
<span class="n">client</span> <span class="o">=</span> <span class="n">LlamaStackClient</span><span class="p">(</span><span class="n">base_url</span><span class="o">=</span><span class="s2">&quot;http://localhost:8321&quot;</span><span class="p">)</span>

<span class="n">models</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">list</span><span class="p">()</span>

<span class="c1"># Select the first LLM and first embedding models</span>
<span class="n">model_id</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">m</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">models</span> <span class="k">if</span> <span class="n">m</span><span class="o">.</span><span class="n">model_type</span> <span class="o">==</span> <span class="s2">&quot;llm&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">identifier</span>
<span class="n">embedding_model_id</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">em</span> <span class="o">:=</span> <span class="nb">next</span><span class="p">(</span><span class="n">m</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">models</span> <span class="k">if</span> <span class="n">m</span><span class="o">.</span><span class="n">model_type</span> <span class="o">==</span> <span class="s2">&quot;embedding&quot;</span><span class="p">)</span>
<span class="p">)</span><span class="o">.</span><span class="n">identifier</span>
<span class="n">embedding_dimension</span> <span class="o">=</span> <span class="n">em</span><span class="o">.</span><span class="n">metadata</span><span class="p">[</span><span class="s2">&quot;embedding_dimension&quot;</span><span class="p">]</span>

<span class="n">_</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">vector_dbs</span><span class="o">.</span><span class="n">register</span><span class="p">(</span>
    <span class="n">vector_db_id</span><span class="o">=</span><span class="n">vector_db_id</span><span class="p">,</span>
    <span class="n">embedding_model</span><span class="o">=</span><span class="n">embedding_model_id</span><span class="p">,</span>
    <span class="n">embedding_dimension</span><span class="o">=</span><span class="n">embedding_dimension</span><span class="p">,</span>
    <span class="n">provider_id</span><span class="o">=</span><span class="s2">&quot;faiss&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">source</span> <span class="o">=</span> <span class="s2">&quot;https://www.paulgraham.com/greatwork.html&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;rag_tool&gt; Ingesting document:&quot;</span><span class="p">,</span> <span class="n">source</span><span class="p">)</span>
<span class="n">document</span> <span class="o">=</span> <span class="n">RAGDocument</span><span class="p">(</span>
    <span class="n">document_id</span><span class="o">=</span><span class="s2">&quot;document_1&quot;</span><span class="p">,</span>
    <span class="n">content</span><span class="o">=</span><span class="n">source</span><span class="p">,</span>
    <span class="n">mime_type</span><span class="o">=</span><span class="s2">&quot;text/html&quot;</span><span class="p">,</span>
    <span class="n">metadata</span><span class="o">=</span><span class="p">{},</span>
<span class="p">)</span>
<span class="n">client</span><span class="o">.</span><span class="n">tool_runtime</span><span class="o">.</span><span class="n">rag_tool</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span>
    <span class="n">documents</span><span class="o">=</span><span class="p">[</span><span class="n">document</span><span class="p">],</span>
    <span class="n">vector_db_id</span><span class="o">=</span><span class="n">vector_db_id</span><span class="p">,</span>
    <span class="n">chunk_size_in_tokens</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">agent</span> <span class="o">=</span> <span class="n">Agent</span><span class="p">(</span>
    <span class="n">client</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model_id</span><span class="p">,</span>
    <span class="n">instructions</span><span class="o">=</span><span class="s2">&quot;You are a helpful assistant&quot;</span><span class="p">,</span>
    <span class="n">tools</span><span class="o">=</span><span class="p">[</span>
        <span class="p">{</span>
            <span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;builtin::rag/knowledge_search&quot;</span><span class="p">,</span>
            <span class="s2">&quot;args&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;vector_db_ids&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">vector_db_id</span><span class="p">]},</span>
        <span class="p">}</span>
    <span class="p">],</span>
<span class="p">)</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;How do you do great work?&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;prompt&gt;&quot;</span><span class="p">,</span> <span class="n">prompt</span><span class="p">)</span>

<span class="n">use_stream</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">create_turn</span><span class="p">(</span>
    <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">}],</span>
    <span class="n">session_id</span><span class="o">=</span><span class="n">agent</span><span class="o">.</span><span class="n">create_session</span><span class="p">(</span><span class="s2">&quot;rag_session&quot;</span><span class="p">),</span>
    <span class="n">stream</span><span class="o">=</span><span class="n">use_stream</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Only call `AgentEventLogger().log(response)` for streaming responses.</span>
<span class="k">if</span> <span class="n">use_stream</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">log</span> <span class="ow">in</span> <span class="n">AgentEventLogger</span><span class="p">()</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">response</span><span class="p">):</span>
        <span class="n">log</span><span class="o">.</span><span class="n">print</span><span class="p">()</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</pre></div>
</div>
<p>We will use <code class="docutils literal notranslate"><span class="pre">uv</span></code> to run the script</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">uv</span> <span class="n">run</span> <span class="o">--</span><span class="k">with</span> <span class="n">llama</span><span class="o">-</span><span class="n">stack</span><span class="o">-</span><span class="n">client</span><span class="p">,</span><span class="n">fire</span><span class="p">,</span><span class="n">requests</span> <span class="n">demo_script</span><span class="o">.</span><span class="n">py</span>
</pre></div>
</div>
<p>And you should see output like below.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>rag_tool&gt; Ingesting document: https://www.paulgraham.com/greatwork.html

prompt&gt; How do you do great work?

inference&gt; [knowledge_search(query=&quot;What is the key to doing great work&quot;)]

tool_execution&gt; Tool:knowledge_search Args:{&#39;query&#39;: &#39;What is the key to doing great work&#39;}

tool_execution&gt; Tool:knowledge_search Response:[TextContentItem(text=&#39;knowledge_search tool found 5 chunks:\nBEGIN of knowledge_search tool results.\n&#39;, type=&#39;text&#39;), TextContentItem(text=&quot;Result 1:\nDocument_id:docum\nContent:  work. Doing great work means doing something important\nso well that you expand people&#39;s ideas of what&#39;s possible. But\nthere&#39;s no threshold for importance. It&#39;s a matter of degree, and\noften hard to judge at the time anyway.\n&quot;, type=&#39;text&#39;), TextContentItem(text=&quot;Result 2:\nDocument_id:docum\nContent:  work. Doing great work means doing something important\nso well that you expand people&#39;s ideas of what&#39;s possible. But\nthere&#39;s no threshold for importance. It&#39;s a matter of degree, and\noften hard to judge at the time anyway.\n&quot;, type=&#39;text&#39;), TextContentItem(text=&quot;Result 3:\nDocument_id:docum\nContent:  work. Doing great work means doing something important\nso well that you expand people&#39;s ideas of what&#39;s possible. But\nthere&#39;s no threshold for importance. It&#39;s a matter of degree, and\noften hard to judge at the time anyway.\n&quot;, type=&#39;text&#39;), TextContentItem(text=&quot;Result 4:\nDocument_id:docum\nContent:  work. Doing great work means doing something important\nso well that you expand people&#39;s ideas of what&#39;s possible. But\nthere&#39;s no threshold for importance. It&#39;s a matter of degree, and\noften hard to judge at the time anyway.\n&quot;, type=&#39;text&#39;), TextContentItem(text=&quot;Result 5:\nDocument_id:docum\nContent:  work. Doing great work means doing something important\nso well that you expand people&#39;s ideas of what&#39;s possible. But\nthere&#39;s no threshold for importance. It&#39;s a matter of degree, and\noften hard to judge at the time anyway.\n&quot;, type=&#39;text&#39;), TextContentItem(text=&#39;END of knowledge_search tool results.\n&#39;, type=&#39;text&#39;)]

inference&gt; Based on the search results, it seems that doing great work means doing something important so well that you expand people&#39;s ideas of what&#39;s possible. However, there is no clear threshold for importance, and it can be difficult to judge at the time.

To further clarify, I would suggest that doing great work involves:

* Completing tasks with high quality and attention to detail
* Expanding on existing knowledge or ideas
* Making a positive impact on others through your work
* Striving for excellence and continuous improvement

Ultimately, great work is about making a meaningful contribution and leaving a lasting impression.
</pre></div>
</div>
<p>Congratulations! You‚Äôve successfully built your first RAG application using Llama Stack! üéâü•≥</p>
<div class="tip admonition">
<p class="admonition-title">HuggingFace access</p>
<p>If you are getting a <strong>401 Client Error</strong> from HuggingFace for the <strong>all-MiniLM-L6-v2</strong> model, try setting <strong>HF_TOKEN</strong> to a valid HuggingFace token in your environment</p>
</div>
</section>
<section id="next-steps">
<h3>Next Steps<a class="headerlink" href="#next-steps" title="Link to this heading">ÔÉÅ</a></h3>
<p>Now you‚Äôre ready to dive deeper into Llama Stack!</p>
<ul class="simple">
<li><p>Explore the <a class="reference internal" href="detailed_tutorial.html"><span class="std std-doc">Detailed Tutorial</span></a>.</p></li>
<li><p>Try the <a class="reference external" href="https://github.com/meta-llama/llama-stack/blob/main/docs/getting_started.ipynb">Getting Started Notebook</a>.</p></li>
<li><p>Browse more <a class="reference external" href="https://github.com/meta-llama/llama-stack/tree/main/docs/notebooks">Notebooks on GitHub</a>.</p></li>
<li><p>Learn about Llama Stack <a class="reference internal" href="../concepts/index.html"><span class="std std-doc">Concepts</span></a>.</p></li>
<li><p>Discover how to <a class="reference internal" href="../distributions/index.html"><span class="std std-doc">Build Llama Stacks</span></a>.</p></li>
<li><p>Refer to our <a class="reference internal" href="../references/index.html"><span class="std std-doc">References</span></a> for details on the Llama CLI and Python SDK.</p></li>
<li><p>Check out the <a class="reference external" href="https://github.com/meta-llama/llama-stack-apps/tree/main/examples">llama-stack-apps</a> repository for example applications and tutorials.</p></li>
</ul>
</section>
</section>
<section id="libraries-sdks">
<h2>Libraries (SDKs)<a class="headerlink" href="#libraries-sdks" title="Link to this heading">ÔÉÅ</a></h2>
<p>We have a number of client-side SDKs available for different languages.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head text-center"><p><strong>Language</strong></p></th>
<th class="head text-center"><p><strong>Client SDK</strong></p></th>
<th class="head text-center"><p><strong>Package</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p>Python</p></td>
<td class="text-center"><p><a class="reference external" href="https://github.com/meta-llama/llama-stack-client-python">llama-stack-client-python</a></p></td>
<td class="text-center"><p><a class="reference external" href="https://pypi.org/project/llama_stack_client/"><img alt="PyPI version" src="https://img.shields.io/pypi/v/llama_stack_client.svg" /></a></p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>Swift</p></td>
<td class="text-center"><p><a class="reference external" href="https://github.com/meta-llama/llama-stack-client-swift/tree/latest-release">llama-stack-client-swift</a></p></td>
<td class="text-center"><p><a class="reference external" href="https://swiftpackageindex.com/meta-llama/llama-stack-client-swift"><img alt="Swift Package Index" src="https://img.shields.io/endpoint?url=https%3A%2F%2Fswiftpackageindex.com%2Fapi%2Fpackages%2Fmeta-llama%2Fllama-stack-client-swift%2Fbadge%3Ftype%3Dswift-versions" /></a></p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>Node</p></td>
<td class="text-center"><p><a class="reference external" href="https://github.com/meta-llama/llama-stack-client-node">llama-stack-client-node</a></p></td>
<td class="text-center"><p><a class="reference external" href="https://npmjs.org/package/llama-stack-client"><img alt="NPM version" src="https://img.shields.io/npm/v/llama-stack-client.svg" /></a></p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>Kotlin</p></td>
<td class="text-center"><p><a class="reference external" href="https://github.com/meta-llama/llama-stack-client-kotlin/tree/latest-release">llama-stack-client-kotlin</a></p></td>
<td class="text-center"><p><a class="reference external" href="https://central.sonatype.com/artifact/com.llama.llamastack/llama-stack-client-kotlin"><img alt="Maven version" src="https://img.shields.io/maven-central/v/com.llama.llamastack/llama-stack-client-kotlin" /></a></p></td>
</tr>
</tbody>
</table>
</section>
<section id="detailed-tutorial">
<h2>Detailed Tutorial<a class="headerlink" href="#detailed-tutorial" title="Link to this heading">ÔÉÅ</a></h2>
<p>In this guide, we‚Äôll walk through how you can use the Llama Stack (server and client SDK) to test a simple agent.
A Llama Stack agent is a simple integrated system that can perform tasks by combining a Llama model for reasoning with
tools (e.g., RAG, web search, code execution, etc.) for taking actions.
In Llama Stack, we provide a server exposing multiple APIs. These APIs are backed by implementations from different providers.</p>
<p>Llama Stack is a stateful service with REST APIs to support seamless transition of AI applications across different environments. The server can be run in a variety of ways, including as a standalone binary, Docker container, or hosted service. You can build and test using a local server first and deploy to a hosted endpoint for production.</p>
<p>In this guide, we‚Äôll walk through how to build a RAG agent locally using Llama Stack with <a class="reference external" href="https://ollama.com/">Ollama</a>
as the inference <a class="reference internal" href="../providers/index.html#inference"><span class="std std-ref">provider</span></a> for a Llama Model.</p>
<section id="step-1-installation-and-setup">
<h3>Step 1: Installation and Setup<a class="headerlink" href="#step-1-installation-and-setup" title="Link to this heading">ÔÉÅ</a></h3>
<p>Install Ollama by following the instructions on the <a class="reference external" href="https://ollama.com/download">Ollama website</a>, then
download Llama 3.2 3B model, and then start the Ollama service.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ollama<span class="w"> </span>pull<span class="w"> </span>llama3.2:3b
ollama<span class="w"> </span>run<span class="w"> </span>llama3.2:3b<span class="w"> </span>--keepalive<span class="w"> </span>60m
</pre></div>
</div>
<p>Install <a class="reference external" href="https://docs.astral.sh/uv/">uv</a> to setup your virtual environment</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-0" name="sd-tab-set-0" type="radio">
<label class="sd-tab-label" for="sd-tab-item-0">
macOS and Linux</label><div class="sd-tab-content docutils">
<p>Use <code class="docutils literal notranslate"><span class="pre">curl</span></code> to download the script and execute it with <code class="docutils literal notranslate"><span class="pre">sh</span></code>:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">curl -LsSf https://astral.sh/uv/install.sh | sh</span>
</pre></div>
</div>
</div>
<input id="sd-tab-item-1" name="sd-tab-set-0" type="radio">
<label class="sd-tab-label" for="sd-tab-item-1">
Windows</label><div class="sd-tab-content docutils">
<p>Use <code class="docutils literal notranslate"><span class="pre">irm</span></code> to download the script and execute it with <code class="docutils literal notranslate"><span class="pre">iex</span></code>:</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">powershell -ExecutionPolicy ByPass -c &quot;irm https://astral.sh/uv/install.ps1 | iex&quot;</span>
</pre></div>
</div>
</div>
</div>
<p>Setup your virtual environment.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>uv<span class="w"> </span>sync<span class="w"> </span>--python<span class="w"> </span><span class="m">3</span>.12
<span class="nb">source</span><span class="w"> </span>.venv/bin/activate
</pre></div>
</div>
</section>
<section id="step-2-run-llama-stack">
<h3>Step 2:  Run Llama Stack<a class="headerlink" href="#step-2-run-llama-stack" title="Link to this heading">ÔÉÅ</a></h3>
<p>Llama Stack is a server that exposes multiple APIs, you connect with it using the Llama Stack client SDK.</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-2" name="sd-tab-set-1" type="radio">
<label class="sd-tab-label" for="sd-tab-item-2">
Using <code class="docutils literal notranslate"><span class="pre">venv</span></code></label><div class="sd-tab-content docutils">
<p>You can use Python to build and run the Llama Stack server, which is useful for testing and development.</p>
<p>Llama Stack uses a <a class="reference internal" href="../distributions/configuration.html"><span class="std std-doc">YAML configuration file</span></a> to specify the stack setup,
which defines the providers and their settings. The generated configuration serves as a starting point that you can <a class="reference internal" href="../distributions/customizing_run_yaml.html"><span class="std std-doc">customize for your specific needs</span></a>.
Now let‚Äôs build and run the Llama Stack config for Ollama.
We use <code class="docutils literal notranslate"><span class="pre">starter</span></code> as template. By default all providers are disabled, this requires enable ollama by passing environment variables.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>llama<span class="w"> </span>stack<span class="w"> </span>build<span class="w"> </span>--distro<span class="w"> </span>starter<span class="w"> </span>--image-type<span class="w"> </span>venv<span class="w"> </span>--run
</pre></div>
</div>
</div>
<input id="sd-tab-item-3" name="sd-tab-set-1" type="radio">
<label class="sd-tab-label" for="sd-tab-item-3">
Using <code class="docutils literal notranslate"><span class="pre">venv</span></code></label><div class="sd-tab-content docutils">
<p>You can use Python to build and run the Llama Stack server, which is useful for testing and development.</p>
<p>Llama Stack uses a <a class="reference internal" href="../distributions/configuration.html"><span class="std std-doc">YAML configuration file</span></a> to specify the stack setup,
which defines the providers and their settings.
Now let‚Äôs build and run the Llama Stack config for Ollama.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>llama<span class="w"> </span>stack<span class="w"> </span>build<span class="w"> </span>--distro<span class="w"> </span>starter<span class="w"> </span>--image-type<span class="w"> </span>venv<span class="w"> </span>--run
</pre></div>
</div>
</div>
<input id="sd-tab-item-4" name="sd-tab-set-1" type="radio">
<label class="sd-tab-label" for="sd-tab-item-4">
Using a Container</label><div class="sd-tab-content docutils">
<p>You can use a container image to run the Llama Stack server. We provide several container images for the server
component that works with different inference providers out of the box. For this guide, we will use
<code class="docutils literal notranslate"><span class="pre">llamastack/distribution-starter</span></code> as the container image. If you‚Äôd like to build your own image or customize the
configurations, please check out <a class="reference internal" href="../distributions/building_distro.html"><span class="std std-doc">this guide</span></a>.
First lets setup some environment variables and create a local directory to mount into the container‚Äôs file system.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">LLAMA_STACK_PORT</span><span class="o">=</span><span class="m">8321</span>
mkdir<span class="w"> </span>-p<span class="w"> </span>~/.llama
</pre></div>
</div>
<p>Then start the server using the container tool of your choice.  For example, if you are running Docker you can use the
following command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>run<span class="w"> </span>-it<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--pull<span class="w"> </span>always<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-p<span class="w"> </span><span class="nv">$LLAMA_STACK_PORT</span>:<span class="nv">$LLAMA_STACK_PORT</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-v<span class="w"> </span>~/.llama:/root/.llama<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>llamastack/distribution-starter<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--port<span class="w"> </span><span class="nv">$LLAMA_STACK_PORT</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">OLLAMA_URL</span><span class="o">=</span>http://host.docker.internal:11434
</pre></div>
</div>
<p>Note to start the container with Podman, you can do the same but replace <code class="docutils literal notranslate"><span class="pre">docker</span></code> at the start of the command with
<code class="docutils literal notranslate"><span class="pre">podman</span></code>. If you are using <code class="docutils literal notranslate"><span class="pre">podman</span></code> older than <code class="docutils literal notranslate"><span class="pre">4.7.0</span></code>, please also replace <code class="docutils literal notranslate"><span class="pre">host.docker.internal</span></code> in the <code class="docutils literal notranslate"><span class="pre">OLLAMA_URL</span></code>
with <code class="docutils literal notranslate"><span class="pre">host.containers.internal</span></code>.</p>
<p>The configuration YAML for the Ollama distribution is available at <code class="docutils literal notranslate"><span class="pre">distributions/ollama/run.yaml</span></code>.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Docker containers run in their own isolated network namespaces on Linux. To allow the container to communicate with services running on the host via <code class="docutils literal notranslate"><span class="pre">localhost</span></code>, you need <code class="docutils literal notranslate"><span class="pre">--network=host</span></code>. This makes the container use the host‚Äôs network directly so it can connect to Ollama running on <code class="docutils literal notranslate"><span class="pre">localhost:11434</span></code>.</p>
<p>Linux users having issues running the above command should instead try the following:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>run<span class="w"> </span>-it<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--pull<span class="w"> </span>always<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-p<span class="w"> </span><span class="nv">$LLAMA_STACK_PORT</span>:<span class="nv">$LLAMA_STACK_PORT</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-v<span class="w"> </span>~/.llama:/root/.llama<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--network<span class="o">=</span>host<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>llamastack/distribution-starter<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--port<span class="w"> </span><span class="nv">$LLAMA_STACK_PORT</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">OLLAMA_URL</span><span class="o">=</span>http://localhost:11434
</pre></div>
</div>
</div>
</div>
</div>
<p>You will see output like below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">INFO</span><span class="p">:</span>     <span class="n">Application</span> <span class="n">startup</span> <span class="n">complete</span><span class="o">.</span>
<span class="n">INFO</span><span class="p">:</span>     <span class="n">Uvicorn</span> <span class="n">running</span> <span class="n">on</span> <span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="p">[</span><span class="s1">&#39;::&#39;</span><span class="p">,</span> <span class="s1">&#39;0.0.0.0&#39;</span><span class="p">]:</span><span class="mi">8321</span> <span class="p">(</span><span class="n">Press</span> <span class="n">CTRL</span><span class="o">+</span><span class="n">C</span> <span class="n">to</span> <span class="n">quit</span><span class="p">)</span>
</pre></div>
</div>
<p>Now you can use the Llama Stack client to run inference and build agents!</p>
<p>You can reuse the server setup or use the <a class="reference external" href="https://github.com/meta-llama/llama-stack-client-python/">Llama Stack Client</a>.
Note that the client package is already included in the <code class="docutils literal notranslate"><span class="pre">llama-stack</span></code> package.</p>
</section>
<section id="step-3-run-client-cli">
<h3>Step 3: Run Client CLI<a class="headerlink" href="#step-3-run-client-cli" title="Link to this heading">ÔÉÅ</a></h3>
<p>Open a new terminal and navigate to the same directory you started the server from. Then set up a new or activate your
existing server virtual environment.</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-5" name="sd-tab-set-2" type="radio">
<label class="sd-tab-label" for="sd-tab-item-5">
Reuse Server <code class="docutils literal notranslate"><span class="pre">venv</span></code></label><div class="sd-tab-content docutils">
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># The client is included in the llama-stack package so we just activate the server venv</span>
<span class="nb">source</span><span class="w"> </span>.venv/bin/activate
</pre></div>
</div>
</div>
<input id="sd-tab-item-6" name="sd-tab-set-2" type="radio">
<label class="sd-tab-label" for="sd-tab-item-6">
Install with <code class="docutils literal notranslate"><span class="pre">venv</span></code></label><div class="sd-tab-content docutils">
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>uv<span class="w"> </span>venv<span class="w"> </span>client<span class="w"> </span>--python<span class="w"> </span><span class="m">3</span>.12
<span class="nb">source</span><span class="w"> </span>client/bin/activate
pip<span class="w"> </span>install<span class="w"> </span>llama-stack-client
</pre></div>
</div>
</div>
</div>
<p>Now let‚Äôs use the <code class="docutils literal notranslate"><span class="pre">llama-stack-client</span></code> <a class="reference internal" href="../references/llama_stack_client_cli_reference.html"><span class="std std-doc">CLI</span></a> to check the
connectivity to the server.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>llama-stack-client<span class="w"> </span>configure<span class="w"> </span>--endpoint<span class="w"> </span>http://localhost:8321<span class="w"> </span>--api-key<span class="w"> </span>none
</pre></div>
</div>
<p>You will see the below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Done! You can now use the Llama Stack Client CLI with endpoint http://localhost:8321
</pre></div>
</div>
<p>List the models</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>llama-stack-client<span class="w"> </span>models<span class="w"> </span>list
Available<span class="w"> </span>Models

‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì
‚îÉ<span class="w"> </span>model_type<span class="w">      </span>‚îÉ<span class="w"> </span>identifier<span class="w">                          </span>‚îÉ<span class="w"> </span>provider_resource_id<span class="w">                </span>‚îÉ<span class="w"> </span>metadata<span class="w">                                  </span>‚îÉ<span class="w"> </span>provider_id<span class="w">           </span>‚îÉ
‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©
‚îÇ<span class="w"> </span>embedding<span class="w">       </span>‚îÇ<span class="w"> </span>ollama/all-minilm:l6-v2<span class="w">             </span>‚îÇ<span class="w"> </span>all-minilm:l6-v2<span class="w">                    </span>‚îÇ<span class="w"> </span><span class="o">{</span><span class="s1">&#39;embedding_dimension&#39;</span>:<span class="w"> </span><span class="m">384</span>.0<span class="o">}</span><span class="w">            </span>‚îÇ<span class="w"> </span>ollama<span class="w">                </span>‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ<span class="w"> </span>...<span class="w">             </span>‚îÇ<span class="w"> </span>...<span class="w">                                 </span>‚îÇ<span class="w"> </span>...<span class="w">                                 </span>‚îÇ<span class="w">                                           </span>‚îÇ<span class="w"> </span>...<span class="w">                   </span>‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ<span class="w"> </span>llm<span class="w">             </span>‚îÇ<span class="w"> </span>ollama/Llama-3.2:3b<span class="w">                 </span>‚îÇ<span class="w"> </span>llama3.2:3b<span class="w">                         </span>‚îÇ<span class="w">                                           </span>‚îÇ<span class="w"> </span>ollama<span class="w">                </span>‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
</pre></div>
</div>
<p>You can test basic Llama inference completion using the CLI.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>llama-stack-client<span class="w"> </span>inference<span class="w"> </span>chat-completion<span class="w"> </span>--model-id<span class="w"> </span><span class="s2">&quot;ollama/llama3.2:3b&quot;</span><span class="w"> </span>--message<span class="w"> </span><span class="s2">&quot;tell me a joke&quot;</span>
</pre></div>
</div>
<p>Sample output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">OpenAIChatCompletion</span><span class="p">(</span>
    <span class="nb">id</span><span class="o">=</span><span class="s2">&quot;chatcmpl-08d7b2be-40f3-47ed-8f16-a6f29f2436af&quot;</span><span class="p">,</span>
    <span class="n">choices</span><span class="o">=</span><span class="p">[</span>
        <span class="n">OpenAIChatCompletionChoice</span><span class="p">(</span>
            <span class="n">finish_reason</span><span class="o">=</span><span class="s2">&quot;stop&quot;</span><span class="p">,</span>
            <span class="n">index</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">message</span><span class="o">=</span><span class="n">OpenAIChatCompletionChoiceMessageOpenAIAssistantMessageParam</span><span class="p">(</span>
                <span class="n">role</span><span class="o">=</span><span class="s2">&quot;assistant&quot;</span><span class="p">,</span>
                <span class="n">content</span><span class="o">=</span><span class="s2">&quot;Why couldn&#39;t the bicycle stand up by itself?</span><span class="se">\n\n</span><span class="s2">Because it was two-tired.&quot;</span><span class="p">,</span>
                <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">tool_calls</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">refusal</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">annotations</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">audio</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                <span class="n">function_call</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="p">),</span>
            <span class="n">logprobs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="p">],</span>
    <span class="n">created</span><span class="o">=</span><span class="mi">1751725254</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;llama3.2:3b&quot;</span><span class="p">,</span>
    <span class="nb">object</span><span class="o">=</span><span class="s2">&quot;chat.completion&quot;</span><span class="p">,</span>
    <span class="n">service_tier</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">system_fingerprint</span><span class="o">=</span><span class="s2">&quot;fp_ollama&quot;</span><span class="p">,</span>
    <span class="n">usage</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">&quot;completion_tokens&quot;</span><span class="p">:</span> <span class="mi">18</span><span class="p">,</span>
        <span class="s2">&quot;prompt_tokens&quot;</span><span class="p">:</span> <span class="mi">29</span><span class="p">,</span>
        <span class="s2">&quot;total_tokens&quot;</span><span class="p">:</span> <span class="mi">47</span><span class="p">,</span>
        <span class="s2">&quot;completion_tokens_details&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
        <span class="s2">&quot;prompt_tokens_details&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">},</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="step-4-run-the-demos">
<h3>Step 4: Run the Demos<a class="headerlink" href="#step-4-run-the-demos" title="Link to this heading">ÔÉÅ</a></h3>
<p>Note that these demos show the <a class="reference internal" href="../references/python_sdk_reference/index.html"><span class="std std-doc">Python Client SDK</span></a>.
Other SDKs are also available, please refer to the <a class="reference internal" href="../index.html#client-sdks"><span class="std std-ref">Client SDK</span></a> list for the complete options.</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-7" name="sd-tab-set-3" type="radio">
<label class="sd-tab-label" for="sd-tab-item-7">
Basic Inference</label><div class="sd-tab-content docutils">
<p>Now you can run inference using the Llama Stack client SDK.</p>
<p class="rubric" id="i-create-the-script">i. Create the Script</p>
<p>Create a file <code class="docutils literal notranslate"><span class="pre">inference.py</span></code> and add the following code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">llama_stack_client</span><span class="w"> </span><span class="kn">import</span> <span class="n">LlamaStackClient</span>

<span class="n">client</span> <span class="o">=</span> <span class="n">LlamaStackClient</span><span class="p">(</span><span class="n">base_url</span><span class="o">=</span><span class="s2">&quot;http://localhost:8321&quot;</span><span class="p">)</span>

<span class="c1"># List available models</span>
<span class="n">models</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">list</span><span class="p">()</span>

<span class="c1"># Select the first LLM</span>
<span class="n">llm</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">m</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">models</span> <span class="k">if</span> <span class="n">m</span><span class="o">.</span><span class="n">model_type</span> <span class="o">==</span> <span class="s2">&quot;llm&quot;</span> <span class="ow">and</span> <span class="n">m</span><span class="o">.</span><span class="n">provider_id</span> <span class="o">==</span> <span class="s2">&quot;ollama&quot;</span><span class="p">)</span>
<span class="n">model_id</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">identifier</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Model:&quot;</span><span class="p">,</span> <span class="n">model_id</span><span class="p">)</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model_id</span><span class="p">,</span>
    <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
        <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;You are a helpful assistant.&quot;</span><span class="p">},</span>
        <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Write a haiku about coding&quot;</span><span class="p">},</span>
    <span class="p">],</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</pre></div>
</div>
<p class="rubric" id="ii-run-the-script">ii. Run the Script</p>
<p>Let‚Äôs run the script using <code class="docutils literal notranslate"><span class="pre">uv</span></code></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>uv<span class="w"> </span>run<span class="w"> </span>python<span class="w"> </span>inference.py
</pre></div>
</div>
<p>Which will output:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Model</span><span class="p">:</span> <span class="n">ollama</span><span class="o">/</span><span class="n">llama3</span><span class="mf">.2</span><span class="p">:</span><span class="mi">3</span><span class="n">b</span>
<span class="n">OpenAIChatCompletion</span><span class="p">(</span><span class="nb">id</span><span class="o">=</span><span class="s1">&#39;chatcmpl-30cd0f28-a2ad-4b6d-934b-13707fc60ebf&#39;</span><span class="p">,</span> <span class="n">choices</span><span class="o">=</span><span class="p">[</span><span class="n">OpenAIChatCompletionChoice</span><span class="p">(</span><span class="n">finish_reason</span><span class="o">=</span><span class="s1">&#39;stop&#39;</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="n">OpenAIChatCompletionChoiceMessageOpenAIAssistantMessageParam</span><span class="p">(</span><span class="n">role</span><span class="o">=</span><span class="s1">&#39;assistant&#39;</span><span class="p">,</span> <span class="n">content</span><span class="o">=</span><span class="s2">&quot;Lines of code unfold</span><span class="se">\n</span><span class="s2">Algorithms dance with ease</span><span class="se">\n</span><span class="s2">Logic&#39;s gentle kiss&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">tool_calls</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">refusal</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">annotations</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">audio</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">function_call</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span> <span class="n">logprobs</span><span class="o">=</span><span class="kc">None</span><span class="p">)],</span> <span class="n">created</span><span class="o">=</span><span class="mi">1751732480</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s1">&#39;llama3.2:3b&#39;</span><span class="p">,</span> <span class="nb">object</span><span class="o">=</span><span class="s1">&#39;chat.completion&#39;</span><span class="p">,</span> <span class="n">service_tier</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">system_fingerprint</span><span class="o">=</span><span class="s1">&#39;fp_ollama&#39;</span><span class="p">,</span> <span class="n">usage</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;completion_tokens&#39;</span><span class="p">:</span> <span class="mi">16</span><span class="p">,</span> <span class="s1">&#39;prompt_tokens&#39;</span><span class="p">:</span> <span class="mi">37</span><span class="p">,</span> <span class="s1">&#39;total_tokens&#39;</span><span class="p">:</span> <span class="mi">53</span><span class="p">,</span> <span class="s1">&#39;completion_tokens_details&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span> <span class="s1">&#39;prompt_tokens_details&#39;</span><span class="p">:</span> <span class="kc">None</span><span class="p">})</span>
</pre></div>
</div>
</div>
<input id="sd-tab-item-8" name="sd-tab-set-3" type="radio">
<label class="sd-tab-label" for="sd-tab-item-8">
Build a Simple Agent</label><div class="sd-tab-content docutils">
<p>Next we can move beyond simple inference and build an agent that can perform tasks using the Llama Stack server.</p>
<p class="rubric" id="id1">i. Create the Script</p>
<p>Create a file <code class="docutils literal notranslate"><span class="pre">agent.py</span></code> and add the following code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">llama_stack_client</span><span class="w"> </span><span class="kn">import</span> <span class="n">LlamaStackClient</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">llama_stack_client</span><span class="w"> </span><span class="kn">import</span> <span class="n">Agent</span><span class="p">,</span> <span class="n">AgentEventLogger</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">rich.pretty</span><span class="w"> </span><span class="kn">import</span> <span class="n">pprint</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">uuid</span>

<span class="n">client</span> <span class="o">=</span> <span class="n">LlamaStackClient</span><span class="p">(</span><span class="n">base_url</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;http://localhost:8321&quot;</span><span class="p">)</span>

<span class="n">models</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">list</span><span class="p">()</span>
<span class="n">llm</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">m</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">models</span> <span class="k">if</span> <span class="n">m</span><span class="o">.</span><span class="n">model_type</span> <span class="o">==</span> <span class="s2">&quot;llm&quot;</span> <span class="ow">and</span> <span class="n">m</span><span class="o">.</span><span class="n">provider_id</span> <span class="o">==</span> <span class="s2">&quot;ollama&quot;</span><span class="p">)</span>
<span class="n">model_id</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">identifier</span>

<span class="n">agent</span> <span class="o">=</span> <span class="n">Agent</span><span class="p">(</span><span class="n">client</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model_id</span><span class="p">,</span> <span class="n">instructions</span><span class="o">=</span><span class="s2">&quot;You are a helpful assistant.&quot;</span><span class="p">)</span>

<span class="n">s_id</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">create_session</span><span class="p">(</span><span class="n">session_name</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;s</span><span class="si">{</span><span class="n">uuid</span><span class="o">.</span><span class="n">uuid4</span><span class="p">()</span><span class="o">.</span><span class="n">hex</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Non-streaming ...&quot;</span><span class="p">)</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">create_turn</span><span class="p">(</span>
    <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Who are you?&quot;</span><span class="p">}],</span>
    <span class="n">session_id</span><span class="o">=</span><span class="n">s_id</span><span class="p">,</span>
    <span class="n">stream</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;agent&gt;&quot;</span><span class="p">,</span> <span class="n">response</span><span class="o">.</span><span class="n">output_message</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Streaming ...&quot;</span><span class="p">)</span>
<span class="n">stream</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">create_turn</span><span class="p">(</span>
    <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Who are you?&quot;</span><span class="p">}],</span> <span class="n">session_id</span><span class="o">=</span><span class="n">s_id</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
<span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">stream</span><span class="p">:</span>
    <span class="n">pprint</span><span class="p">(</span><span class="n">event</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Streaming with print helper...&quot;</span><span class="p">)</span>
<span class="n">stream</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">create_turn</span><span class="p">(</span>
    <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Who are you?&quot;</span><span class="p">}],</span> <span class="n">session_id</span><span class="o">=</span><span class="n">s_id</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
<span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">AgentEventLogger</span><span class="p">()</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">stream</span><span class="p">):</span>
    <span class="n">event</span><span class="o">.</span><span class="n">print</span><span class="p">()</span>
</pre></div>
</div>
<p class="rubric" id="id2">ii. Run the Script</p>
<p>Let‚Äôs run the script using <code class="docutils literal notranslate"><span class="pre">uv</span></code></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>uv<span class="w"> </span>run<span class="w"> </span>python<span class="w"> </span>agent.py
</pre></div>
</div>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">üëã Click here to see the sample output</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Non-streaming ...
agent&gt; I&#39;m an artificial intelligence designed to assist and communicate with users like you. I don&#39;t have a personal identity, but I can provide information, answer questions, and help with tasks to the best of my abilities.

I&#39;m a large language model, which means I&#39;ve been trained on a massive dataset of text from various sources, allowing me to understand and respond to a wide range of topics and questions. My purpose is to provide helpful and accurate information, and I&#39;m constantly learning and improving my responses based on the interactions I have with users like you.

I can help with:

* Answering questions on various subjects
* Providing definitions and explanations
* Offering suggestions and ideas
* Assisting with language-related tasks, such as proofreading and editing
* Generating text and content
* And more!

Feel free to ask me anything, and I&#39;ll do my best to help!
Streaming ...
AgentTurnResponseStreamChunk(
‚îÇ   event=TurnResponseEvent(
‚îÇ   ‚îÇ   payload=AgentTurnResponseStepStartPayload(
‚îÇ   ‚îÇ   ‚îÇ   event_type=&#39;step_start&#39;,
‚îÇ   ‚îÇ   ‚îÇ   step_id=&#39;69831607-fa75-424a-949b-e2049e3129d1&#39;,
‚îÇ   ‚îÇ   ‚îÇ   step_type=&#39;inference&#39;,
‚îÇ   ‚îÇ   ‚îÇ   metadata={}
‚îÇ   ‚îÇ   )
‚îÇ   )
)
AgentTurnResponseStreamChunk(
‚îÇ   event=TurnResponseEvent(
‚îÇ   ‚îÇ   payload=AgentTurnResponseStepProgressPayload(
‚îÇ   ‚îÇ   ‚îÇ   delta=TextDelta(text=&#39;As&#39;, type=&#39;text&#39;),
‚îÇ   ‚îÇ   ‚îÇ   event_type=&#39;step_progress&#39;,
‚îÇ   ‚îÇ   ‚îÇ   step_id=&#39;69831607-fa75-424a-949b-e2049e3129d1&#39;,
‚îÇ   ‚îÇ   ‚îÇ   step_type=&#39;inference&#39;
‚îÇ   ‚îÇ   )
‚îÇ   )
)
AgentTurnResponseStreamChunk(
‚îÇ   event=TurnResponseEvent(
‚îÇ   ‚îÇ   payload=AgentTurnResponseStepProgressPayload(
‚îÇ   ‚îÇ   ‚îÇ   delta=TextDelta(text=&#39; a&#39;, type=&#39;text&#39;),
‚îÇ   ‚îÇ   ‚îÇ   event_type=&#39;step_progress&#39;,
‚îÇ   ‚îÇ   ‚îÇ   step_id=&#39;69831607-fa75-424a-949b-e2049e3129d1&#39;,
‚îÇ   ‚îÇ   ‚îÇ   step_type=&#39;inference&#39;
‚îÇ   ‚îÇ   )
‚îÇ   )
)
...
AgentTurnResponseStreamChunk(
‚îÇ   event=TurnResponseEvent(
‚îÇ   ‚îÇ   payload=AgentTurnResponseStepCompletePayload(
‚îÇ   ‚îÇ   ‚îÇ   event_type=&#39;step_complete&#39;,
‚îÇ   ‚îÇ   ‚îÇ   step_details=InferenceStep(
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   api_model_response=CompletionMessage(
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   content=&#39;As a conversational AI, I don\&#39;t have a personal identity in the classical sense. I exist as a program running on computer servers, designed to process and respond to text-based inputs.\n\nI\&#39;m an instance of a type of artificial intelligence called a &quot;language model,&quot; which is trained on vast amounts of text data to generate human-like responses. My primary function is to understand and respond to natural language inputs, like our conversation right now.\n\nThink of me as a virtual assistant, a chatbot, or a conversational interface ‚Äì I\&#39;m here to provide information, answer questions, and engage in conversation to the best of my abilities. I don\&#39;t have feelings, emotions, or consciousness like humans do, but I\&#39;m designed to simulate human-like interactions to make our conversations feel more natural and helpful.\n\nSo, that\&#39;s me in a nutshell! What can I help you with today?&#39;,
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   role=&#39;assistant&#39;,
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   stop_reason=&#39;end_of_turn&#39;,
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   tool_calls=[]
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ),
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   step_id=&#39;69831607-fa75-424a-949b-e2049e3129d1&#39;,
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   step_type=&#39;inference&#39;,
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   turn_id=&#39;8b360202-f7cb-4786-baa9-166a1b46e2ca&#39;,
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   completed_at=datetime.datetime(2025, 4, 3, 1, 15, 21, 716174, tzinfo=TzInfo(UTC)),
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   started_at=datetime.datetime(2025, 4, 3, 1, 15, 14, 28823, tzinfo=TzInfo(UTC))
‚îÇ   ‚îÇ   ‚îÇ   ),
‚îÇ   ‚îÇ   ‚îÇ   step_id=&#39;69831607-fa75-424a-949b-e2049e3129d1&#39;,
‚îÇ   ‚îÇ   ‚îÇ   step_type=&#39;inference&#39;
‚îÇ   ‚îÇ   )
‚îÇ   )
)
AgentTurnResponseStreamChunk(
‚îÇ   event=TurnResponseEvent(
‚îÇ   ‚îÇ   payload=AgentTurnResponseTurnCompletePayload(
‚îÇ   ‚îÇ   ‚îÇ   event_type=&#39;turn_complete&#39;,
‚îÇ   ‚îÇ   ‚îÇ   turn=Turn(
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   input_messages=[UserMessage(content=&#39;Who are you?&#39;, role=&#39;user&#39;, context=None)],
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   output_message=CompletionMessage(
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   content=&#39;As a conversational AI, I don\&#39;t have a personal identity in the classical sense. I exist as a program running on computer servers, designed to process and respond to text-based inputs.\n\nI\&#39;m an instance of a type of artificial intelligence called a &quot;language model,&quot; which is trained on vast amounts of text data to generate human-like responses. My primary function is to understand and respond to natural language inputs, like our conversation right now.\n\nThink of me as a virtual assistant, a chatbot, or a conversational interface ‚Äì I\&#39;m here to provide information, answer questions, and engage in conversation to the best of my abilities. I don\&#39;t have feelings, emotions, or consciousness like humans do, but I\&#39;m designed to simulate human-like interactions to make our conversations feel more natural and helpful.\n\nSo, that\&#39;s me in a nutshell! What can I help you with today?&#39;,
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   role=&#39;assistant&#39;,
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   stop_reason=&#39;end_of_turn&#39;,
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   tool_calls=[]
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ),
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   session_id=&#39;abd4afea-4324-43f4-9513-cfe3970d92e8&#39;,
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   started_at=datetime.datetime(2025, 4, 3, 1, 15, 14, 28722, tzinfo=TzInfo(UTC)),
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   steps=[
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   InferenceStep(
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   api_model_response=CompletionMessage(
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   content=&#39;As a conversational AI, I don\&#39;t have a personal identity in the classical sense. I exist as a program running on computer servers, designed to process and respond to text-based inputs.\n\nI\&#39;m an instance of a type of artificial intelligence called a &quot;language model,&quot; which is trained on vast amounts of text data to generate human-like responses. My primary function is to understand and respond to natural language inputs, like our conversation right now.\n\nThink of me as a virtual assistant, a chatbot, or a conversational interface ‚Äì I\&#39;m here to provide information, answer questions, and engage in conversation to the best of my abilities. I don\&#39;t have feelings, emotions, or consciousness like humans do, but I\&#39;m designed to simulate human-like interactions to make our conversations feel more natural and helpful.\n\nSo, that\&#39;s me in a nutshell! What can I help you with today?&#39;,
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   role=&#39;assistant&#39;,
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   stop_reason=&#39;end_of_turn&#39;,
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   tool_calls=[]
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ),
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   step_id=&#39;69831607-fa75-424a-949b-e2049e3129d1&#39;,
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   step_type=&#39;inference&#39;,
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   turn_id=&#39;8b360202-f7cb-4786-baa9-166a1b46e2ca&#39;,
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   completed_at=datetime.datetime(2025, 4, 3, 1, 15, 21, 716174, tzinfo=TzInfo(UTC)),
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   started_at=datetime.datetime(2025, 4, 3, 1, 15, 14, 28823, tzinfo=TzInfo(UTC))
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   )
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ],
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   turn_id=&#39;8b360202-f7cb-4786-baa9-166a1b46e2ca&#39;,
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   completed_at=datetime.datetime(2025, 4, 3, 1, 15, 21, 727364, tzinfo=TzInfo(UTC)),
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   output_attachments=[]
‚îÇ   ‚îÇ   ‚îÇ   )
‚îÇ   ‚îÇ   )
‚îÇ   )
)


Streaming with print helper...
inference&gt; D√©j√† vu! You&#39;re asking me again!

As I mentioned earlier, I&#39;m a computer program designed to simulate conversation and answer questions. I don&#39;t have a personal identity or consciousness like a human would. I exist solely as a digital entity, running on computer servers and responding to inputs from users like you.

I&#39;m a type of artificial intelligence (AI) called a large language model, which means I&#39;ve been trained on a massive dataset of text from various sources. This training allows me to understand and respond to a wide range of questions and topics.

My purpose is to provide helpful and accurate information, answer questions, and assist users like you with tasks and conversations. I don&#39;t have personal preferences, emotions, or opinions like humans do. My goal is to be informative, neutral, and respectful in my responses.

So, that&#39;s me in a nutshell!
</pre></div>
</div>
</div>
</details></div>
<input id="sd-tab-item-9" name="sd-tab-set-3" type="radio">
<label class="sd-tab-label" for="sd-tab-item-9">
Build a RAG Agent</label><div class="sd-tab-content docutils">
<p>For our last demo, we can build a RAG agent that can answer questions about the Torchtune project using the documents
in a vector database.</p>
<p class="rubric" id="id3">i. Create the Script</p>
<p>Create a file <code class="docutils literal notranslate"><span class="pre">rag_agent.py</span></code> and add the following code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">llama_stack_client</span><span class="w"> </span><span class="kn">import</span> <span class="n">LlamaStackClient</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">llama_stack_client</span><span class="w"> </span><span class="kn">import</span> <span class="n">Agent</span><span class="p">,</span> <span class="n">AgentEventLogger</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">llama_stack_client.types</span><span class="w"> </span><span class="kn">import</span> <span class="n">Document</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">uuid</span>

<span class="n">client</span> <span class="o">=</span> <span class="n">LlamaStackClient</span><span class="p">(</span><span class="n">base_url</span><span class="o">=</span><span class="s2">&quot;http://localhost:8321&quot;</span><span class="p">)</span>

<span class="c1"># Create a vector database instance</span>
<span class="n">embed_lm</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">m</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">client</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">list</span><span class="p">()</span> <span class="k">if</span> <span class="n">m</span><span class="o">.</span><span class="n">model_type</span> <span class="o">==</span> <span class="s2">&quot;embedding&quot;</span><span class="p">)</span>
<span class="n">embedding_model</span> <span class="o">=</span> <span class="n">embed_lm</span><span class="o">.</span><span class="n">identifier</span>
<span class="n">vector_db_id</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;v</span><span class="si">{</span><span class="n">uuid</span><span class="o">.</span><span class="n">uuid4</span><span class="p">()</span><span class="o">.</span><span class="n">hex</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="n">client</span><span class="o">.</span><span class="n">vector_dbs</span><span class="o">.</span><span class="n">register</span><span class="p">(</span>
    <span class="n">vector_db_id</span><span class="o">=</span><span class="n">vector_db_id</span><span class="p">,</span>
    <span class="n">embedding_model</span><span class="o">=</span><span class="n">embedding_model</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Create Documents</span>
<span class="n">urls</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;memory_optimizations.rst&quot;</span><span class="p">,</span>
    <span class="s2">&quot;chat.rst&quot;</span><span class="p">,</span>
    <span class="s2">&quot;llama3.rst&quot;</span><span class="p">,</span>
    <span class="s2">&quot;qat_finetune.rst&quot;</span><span class="p">,</span>
    <span class="s2">&quot;lora_finetune.rst&quot;</span><span class="p">,</span>
<span class="p">]</span>
<span class="n">documents</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">Document</span><span class="p">(</span>
        <span class="n">document_id</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;num-</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="n">content</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;https://raw.githubusercontent.com/pytorch/torchtune/main/docs/source/tutorials/</span><span class="si">{</span><span class="n">url</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
        <span class="n">mime_type</span><span class="o">=</span><span class="s2">&quot;text/plain&quot;</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="p">{},</span>
    <span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">url</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">urls</span><span class="p">)</span>
<span class="p">]</span>

<span class="c1"># Insert documents</span>
<span class="n">client</span><span class="o">.</span><span class="n">tool_runtime</span><span class="o">.</span><span class="n">rag_tool</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span>
    <span class="n">documents</span><span class="o">=</span><span class="n">documents</span><span class="p">,</span>
    <span class="n">vector_db_id</span><span class="o">=</span><span class="n">vector_db_id</span><span class="p">,</span>
    <span class="n">chunk_size_in_tokens</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Get the model being served</span>
<span class="n">llm</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span>
    <span class="n">m</span>
    <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">client</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">list</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">m</span><span class="o">.</span><span class="n">model_type</span> <span class="o">==</span> <span class="s2">&quot;llm&quot;</span> <span class="ow">and</span> <span class="n">m</span><span class="o">.</span><span class="n">provider_id</span> <span class="o">==</span> <span class="s2">&quot;ollama&quot;</span>
<span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">llm</span><span class="o">.</span><span class="n">identifier</span>

<span class="c1"># Create the RAG agent</span>
<span class="n">rag_agent</span> <span class="o">=</span> <span class="n">Agent</span><span class="p">(</span>
    <span class="n">client</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">instructions</span><span class="o">=</span><span class="s2">&quot;You are a helpful assistant. Use the RAG tool to answer questions as needed.&quot;</span><span class="p">,</span>
    <span class="n">tools</span><span class="o">=</span><span class="p">[</span>
        <span class="p">{</span>
            <span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;builtin::rag/knowledge_search&quot;</span><span class="p">,</span>
            <span class="s2">&quot;args&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;vector_db_ids&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">vector_db_id</span><span class="p">]},</span>
        <span class="p">}</span>
    <span class="p">],</span>
<span class="p">)</span>

<span class="n">session_id</span> <span class="o">=</span> <span class="n">rag_agent</span><span class="o">.</span><span class="n">create_session</span><span class="p">(</span><span class="n">session_name</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;s</span><span class="si">{</span><span class="n">uuid</span><span class="o">.</span><span class="n">uuid4</span><span class="p">()</span><span class="o">.</span><span class="n">hex</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">turns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;what is torchtune&quot;</span><span class="p">,</span> <span class="s2">&quot;tell me about dora&quot;</span><span class="p">]</span>

<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">turns</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;user&gt;&quot;</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
    <span class="n">stream</span> <span class="o">=</span> <span class="n">rag_agent</span><span class="o">.</span><span class="n">create_turn</span><span class="p">(</span>
        <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">t</span><span class="p">}],</span> <span class="n">session_id</span><span class="o">=</span><span class="n">session_id</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>
    <span class="k">for</span> <span class="n">event</span> <span class="ow">in</span> <span class="n">AgentEventLogger</span><span class="p">()</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">stream</span><span class="p">):</span>
        <span class="n">event</span><span class="o">.</span><span class="n">print</span><span class="p">()</span>
</pre></div>
</div>
<p class="rubric" id="id4">ii. Run the Script</p>
<p>Let‚Äôs run the script using <code class="docutils literal notranslate"><span class="pre">uv</span></code></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>uv<span class="w"> </span>run<span class="w"> </span>python<span class="w"> </span>rag_agent.py
</pre></div>
</div>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">üëã Click here to see the sample output</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>user&gt; what is torchtune
inference&gt; [knowledge_search(query=&#39;TorchTune&#39;)]
tool_execution&gt; Tool:knowledge_search Args:{&#39;query&#39;: &#39;TorchTune&#39;}
tool_execution&gt; Tool:knowledge_search Response:[TextContentItem(text=&#39;knowledge_search tool found 5 chunks:\nBEGIN of knowledge_search tool results.\n&#39;, type=&#39;text&#39;), TextContentItem(text=&#39;Result 1:\nDocument_id:num-1\nContent:  conversational data, :func:`~torchtune.datasets.chat_dataset` seems to be a good fit. ..., type=&#39;text&#39;), TextContentItem(text=&#39;END of knowledge_search tool results.\n&#39;, type=&#39;text&#39;)]
inference&gt; Here is a high-level overview of the text:

**LoRA Finetuning with PyTorch Tune**

PyTorch Tune provides a recipe for LoRA (Low-Rank Adaptation) finetuning, which is a technique to adapt pre-trained models to new tasks. The recipe uses the `lora_finetune_distributed` command.
...
Overall, DORA is a powerful reinforcement learning algorithm that can learn complex tasks from human demonstrations. However, it requires careful consideration of the challenges and limitations to achieve optimal results.
</pre></div>
</div>
</div>
</details></div>
</div>
<p><strong>You‚Äôre Ready to Build Your Own Apps!</strong></p>
<p>Congrats! ü•≥ Now you‚Äôre ready to <a class="reference internal" href="../building_applications/index.html"><span class="doc std std-doc">build your own Llama Stack applications</span></a>! üöÄ</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../index.html" class="btn btn-neutral float-left" title="Llama Stack" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../concepts/index.html" class="btn btn-neutral float-right" title="Core Concepts" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Meta.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  
<div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    <span class="fa fa-book"> Read the Docs</span>
    v: v0.2.19
    <span class="fa fa-caret-down"></span>
  </span>
  <div class="rst-other-versions">
    <dl>
      <dt>Versions</dt>
      <dd>
        <a href="/v0.2.19/">latest</a>
      </dd>
      <dd class="rtd-current-item">
        <a href="/v0.2.19/">v0.2.19</a>
      </dd>
      <dd>
        <a href="/v0.2.18/">v0.2.18</a>
      </dd>
      <dd>
        <a href="/v0.2.17/">v0.2.17</a>
      </dd>
      <dd>
        <a href="/v0.2.16/">v0.2.16</a>
      </dd>
      <dd>
        <a href="/v0.2.15/">v0.2.15</a>
      </dd>
      <dd>
        <a href="/v0.2.14/">v0.2.14</a>
      </dd>
      <dd>
        <a href="/v0.2.13/">v0.2.13</a>
      </dd>
      <dd>
        <a href="/v0.2.12/">v0.2.12</a>
      </dd>
      <dd>
        <a href="/v0.2.11/">v0.2.11</a>
      </dd>
    </dl>
  </div>
</div><script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>