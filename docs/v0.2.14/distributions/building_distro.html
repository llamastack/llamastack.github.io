


<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Build your own Distribution &mdash; llama-stack 0.2.14 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../_static/css/my_theme.css?v=f1163765" />
      <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
      <link rel="stylesheet" type="text/css" href="../_static/dark_mode_css/general.css?v=c0a7eb24" />
      <link rel="stylesheet" type="text/css" href="../_static/dark_mode_css/dark.css?v=70edf1c7" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=71c180b9"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../_static/copybutton.js?v=09bf800d"></script>
      <script src="../_static/js/detect_theme.js?v=76226c80"></script>
      <script src="../_static/js/keyboard_shortcuts.js"></script>
      <script src="../_static/design-tabs.js?v=f930bc37"></script>
      <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      <script src="../_static/dark_mode_js/default_light.js?v=c2e647ce"></script>
      <script src="../_static/dark_mode_js/theme_switcher.js?v=358d3910"></script>
    <script src="../_static/js/theme.js"></script>
    <script src="../_static/js/versions.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Building AI Applications (Examples)" href="../building_applications/index.html" />
    <link rel="prev" title="Kubernetes Deployment Guide" href="kubernetes_deployment.html" />
 

<script src="../_static/version-loader.js"></script>

</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #2980B9" >

          
          
          <a href="../index.html" class="icon icon-home">
            llama-stack
          </a>
              <div class="switch-menus">
                <div class="version-switch"></div>
                <div class="language-switch"></div>
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../index.html">Llama Stack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/index.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/detailed_tutorial.html">Detailed Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../introduction/index.html">Why Llama Stack?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../concepts/index.html">Core Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../openai/index.html">OpenAI API Compatibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../providers/index.html">Providers Overview</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Distributions Overview</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="importing_as_library.html">Using Llama Stack as a Library</a></li>
<li class="toctree-l2"><a class="reference internal" href="configuration.html">Configuring a “Stack”</a></li>
<li class="toctree-l2"><a class="reference internal" href="list_of_distributions.html">Available List of Distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="kubernetes_deployment.html">Kubernetes Deployment Guide</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Build your own Distribution</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#setting-your-log-level">Setting your log level</a></li>
<li class="toctree-l3"><a class="reference internal" href="#llama-stack-build">Llama Stack Build</a></li>
<li class="toctree-l3"><a class="reference internal" href="#running-your-stack-server">Running your Stack server</a></li>
<li class="toctree-l3"><a class="reference internal" href="#listing-distributions">Listing Distributions</a></li>
<li class="toctree-l3"><a class="reference internal" href="#removing-a-distribution">Removing a Distribution</a></li>
<li class="toctree-l3"><a class="reference internal" href="#troubleshooting">Troubleshooting</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../building_applications/index.html">Building AI Applications (Examples)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../playground/index.html">Llama Stack Playground</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contributing/index.html">Contributing to Llama-Stack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references/index.html">References</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: #2980B9" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">llama-stack</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="index.html">Distributions Overview</a></li>
      <li class="breadcrumb-item active">Build your own Distribution</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/distributions/building_distro.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="build-your-own-distribution">
<h1>Build your own Distribution<a class="headerlink" href="#build-your-own-distribution" title="Link to this heading"></a></h1>
<p>This guide will walk you through the steps to get started with building a Llama Stack distribution from scratch with your choice of API providers.</p>
<section id="setting-your-log-level">
<h2>Setting your log level<a class="headerlink" href="#setting-your-log-level" title="Link to this heading"></a></h2>
<p>In order to specify the proper logging level users can apply the following environment variable <code class="docutils literal notranslate"><span class="pre">LLAMA_STACK_LOGGING</span></code> with the following format:</p>
<p><code class="docutils literal notranslate"><span class="pre">LLAMA_STACK_LOGGING=server=debug;core=info</span></code></p>
<p>Where each category in the following list:</p>
<ul class="simple">
<li><p>all</p></li>
<li><p>core</p></li>
<li><p>server</p></li>
<li><p>router</p></li>
<li><p>inference</p></li>
<li><p>agents</p></li>
<li><p>safety</p></li>
<li><p>eval</p></li>
<li><p>tools</p></li>
<li><p>client</p></li>
</ul>
<p>Can be set to any of the following log levels:</p>
<ul class="simple">
<li><p>debug</p></li>
<li><p>info</p></li>
<li><p>warning</p></li>
<li><p>error</p></li>
<li><p>critical</p></li>
</ul>
<p>The default global log level is <code class="docutils literal notranslate"><span class="pre">info</span></code>. <code class="docutils literal notranslate"><span class="pre">all</span></code> sets the log level for all components.</p>
<p>A user can also set <code class="docutils literal notranslate"><span class="pre">LLAMA_STACK_LOG_FILE</span></code> which will pipe the logs to the specified path as well as to the terminal. An example would be: <code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">LLAMA_STACK_LOG_FILE=server.log</span></code></p>
</section>
<section id="llama-stack-build">
<h2>Llama Stack Build<a class="headerlink" href="#llama-stack-build" title="Link to this heading"></a></h2>
<p>In order to build your own distribution, we recommend you clone the <code class="docutils literal notranslate"><span class="pre">llama-stack</span></code> repository.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">git</span> <span class="n">clone</span> <span class="n">git</span><span class="nd">@github</span><span class="o">.</span><span class="n">com</span><span class="p">:</span><span class="n">meta</span><span class="o">-</span><span class="n">llama</span><span class="o">/</span><span class="n">llama</span><span class="o">-</span><span class="n">stack</span><span class="o">.</span><span class="n">git</span>
<span class="n">cd</span> <span class="n">llama</span><span class="o">-</span><span class="n">stack</span>
<span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">e</span> <span class="o">.</span>
</pre></div>
</div>
<p>Use the CLI to build your distribution.
The main points to consider are:</p>
<ol class="arabic simple">
<li><p><strong>Image Type</strong> - Do you want a Conda / venv environment or a Container (eg. Docker)</p></li>
<li><p><strong>Template</strong> - Do you want to use a template to build your distribution? or start from scratch ?</p></li>
<li><p><strong>Config</strong> - Do you want to use a pre-existing config file to build your distribution?</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>llama stack build -h
usage: llama stack build [-h] [--config CONFIG] [--template TEMPLATE] [--list-templates] [--image-type {conda,container,venv}] [--image-name IMAGE_NAME] [--print-deps-only] [--run]

Build a Llama stack container

options:
  -h, --help            show this help message and exit
  --config CONFIG       Path to a config file to use for the build. You can find example configs in llama_stack/distributions/**/build.yaml. If this argument is not provided, you will
                        be prompted to enter information interactively (default: None)
  --template TEMPLATE   Name of the example template config to use for build. You may use `llama stack build --list-templates` to check out the available templates (default: None)
  --list-templates      Show the available templates for building a Llama Stack distribution (default: False)
  --image-type {conda,container,venv}
                        Image Type to use for the build. If not specified, will use the image type from the template config. (default: None)
  --image-name IMAGE_NAME
                        [for image-type=conda|container|venv] Name of the conda or virtual environment to use for the build. If not specified, currently active environment will be used if
                        found. (default: None)
  --print-deps-only     Print the dependencies for the stack only, without building the stack (default: False)
  --run                 Run the stack after building using the same image type, name, and other applicable arguments (default: False)

</pre></div>
</div>
<p>After this step is complete, a file named <code class="docutils literal notranslate"><span class="pre">&lt;name&gt;-build.yaml</span></code> and template file <code class="docutils literal notranslate"><span class="pre">&lt;name&gt;-run.yaml</span></code> will be generated and saved at the output file path specified at the end of the command.</p>
<div class="sd-tab-set docutils">
<input checked="checked" id="sd-tab-item-0" name="sd-tab-set-0" type="radio">
<label class="sd-tab-label" for="sd-tab-item-0">
Building from a template</label><div class="sd-tab-content docutils">
<p>To build from alternative API providers, we provide distribution templates for users to get started building a distribution backed by different providers.</p>
<p>The following command will allow you to see the available templates and their corresponding providers.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">llama</span> <span class="n">stack</span> <span class="n">build</span> <span class="o">--</span><span class="nb">list</span><span class="o">-</span><span class="n">templates</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">------------------------------+-----------------------------------------------------------------------------+</span>
<span class="o">|</span> <span class="n">Template</span> <span class="n">Name</span>                <span class="o">|</span> <span class="n">Description</span>                                                                 <span class="o">|</span>
<span class="o">+------------------------------+-----------------------------------------------------------------------------+</span>
<span class="o">|</span> <span class="n">watsonx</span>                      <span class="o">|</span> <span class="n">Use</span> <span class="n">watsonx</span> <span class="k">for</span> <span class="n">running</span> <span class="n">LLM</span> <span class="n">inference</span>                                       <span class="o">|</span>
<span class="o">+------------------------------+-----------------------------------------------------------------------------+</span>
<span class="o">|</span> <span class="n">vllm</span><span class="o">-</span><span class="n">gpu</span>                     <span class="o">|</span> <span class="n">Use</span> <span class="n">a</span> <span class="n">built</span><span class="o">-</span><span class="ow">in</span> <span class="n">vLLM</span> <span class="n">engine</span> <span class="k">for</span> <span class="n">running</span> <span class="n">LLM</span> <span class="n">inference</span>                        <span class="o">|</span>
<span class="o">+------------------------------+-----------------------------------------------------------------------------+</span>
<span class="o">|</span> <span class="n">together</span>                     <span class="o">|</span> <span class="n">Use</span> <span class="n">Together</span><span class="o">.</span><span class="n">AI</span> <span class="k">for</span> <span class="n">running</span> <span class="n">LLM</span> <span class="n">inference</span>                                   <span class="o">|</span>
<span class="o">+------------------------------+-----------------------------------------------------------------------------+</span>
<span class="o">|</span> <span class="n">tgi</span>                          <span class="o">|</span> <span class="n">Use</span> <span class="p">(</span><span class="n">an</span> <span class="n">external</span><span class="p">)</span> <span class="n">TGI</span> <span class="n">server</span> <span class="k">for</span> <span class="n">running</span> <span class="n">LLM</span> <span class="n">inference</span>                      <span class="o">|</span>
<span class="o">+------------------------------+-----------------------------------------------------------------------------+</span>
<span class="o">|</span> <span class="n">starter</span>                      <span class="o">|</span> <span class="n">Quick</span> <span class="n">start</span> <span class="n">template</span> <span class="k">for</span> <span class="n">running</span> <span class="n">Llama</span> <span class="n">Stack</span> <span class="k">with</span> <span class="n">several</span> <span class="n">popular</span> <span class="n">providers</span> <span class="o">|</span>
<span class="o">+------------------------------+-----------------------------------------------------------------------------+</span>
<span class="o">|</span> <span class="n">sambanova</span>                    <span class="o">|</span> <span class="n">Use</span> <span class="n">SambaNova</span> <span class="k">for</span> <span class="n">running</span> <span class="n">LLM</span> <span class="n">inference</span> <span class="ow">and</span> <span class="n">safety</span>                          <span class="o">|</span>
<span class="o">+------------------------------+-----------------------------------------------------------------------------+</span>
<span class="o">|</span> <span class="n">remote</span><span class="o">-</span><span class="n">vllm</span>                  <span class="o">|</span> <span class="n">Use</span> <span class="p">(</span><span class="n">an</span> <span class="n">external</span><span class="p">)</span> <span class="n">vLLM</span> <span class="n">server</span> <span class="k">for</span> <span class="n">running</span> <span class="n">LLM</span> <span class="n">inference</span>                     <span class="o">|</span>
<span class="o">+------------------------------+-----------------------------------------------------------------------------+</span>
<span class="o">|</span> <span class="n">postgres</span><span class="o">-</span><span class="n">demo</span>                <span class="o">|</span> <span class="n">Quick</span> <span class="n">start</span> <span class="n">template</span> <span class="k">for</span> <span class="n">running</span> <span class="n">Llama</span> <span class="n">Stack</span> <span class="k">with</span> <span class="n">several</span> <span class="n">popular</span> <span class="n">providers</span> <span class="o">|</span>
<span class="o">+------------------------------+-----------------------------------------------------------------------------+</span>
<span class="o">|</span> <span class="n">passthrough</span>                  <span class="o">|</span> <span class="n">Use</span> <span class="n">Passthrough</span> <span class="n">hosted</span> <span class="n">llama</span><span class="o">-</span><span class="n">stack</span> <span class="n">endpoint</span> <span class="k">for</span> <span class="n">LLM</span> <span class="n">inference</span>               <span class="o">|</span>
<span class="o">+------------------------------+-----------------------------------------------------------------------------+</span>
<span class="o">|</span> <span class="nb">open</span><span class="o">-</span><span class="n">benchmark</span>               <span class="o">|</span> <span class="n">Distribution</span> <span class="k">for</span> <span class="n">running</span> <span class="nb">open</span> <span class="n">benchmarks</span>                                    <span class="o">|</span>
<span class="o">+------------------------------+-----------------------------------------------------------------------------+</span>
<span class="o">|</span> <span class="n">ollama</span>                       <span class="o">|</span> <span class="n">Use</span> <span class="p">(</span><span class="n">an</span> <span class="n">external</span><span class="p">)</span> <span class="n">Ollama</span> <span class="n">server</span> <span class="k">for</span> <span class="n">running</span> <span class="n">LLM</span> <span class="n">inference</span>                   <span class="o">|</span>
<span class="o">+------------------------------+-----------------------------------------------------------------------------+</span>
<span class="o">|</span> <span class="n">nvidia</span>                       <span class="o">|</span> <span class="n">Use</span> <span class="n">NVIDIA</span> <span class="n">NIM</span> <span class="k">for</span> <span class="n">running</span> <span class="n">LLM</span> <span class="n">inference</span><span class="p">,</span> <span class="n">evaluation</span> <span class="ow">and</span> <span class="n">safety</span>             <span class="o">|</span>
<span class="o">+------------------------------+-----------------------------------------------------------------------------+</span>
<span class="o">|</span> <span class="n">meta</span><span class="o">-</span><span class="n">reference</span><span class="o">-</span><span class="n">gpu</span>           <span class="o">|</span> <span class="n">Use</span> <span class="n">Meta</span> <span class="n">Reference</span> <span class="k">for</span> <span class="n">running</span> <span class="n">LLM</span> <span class="n">inference</span>                                <span class="o">|</span>
<span class="o">+------------------------------+-----------------------------------------------------------------------------+</span>
<span class="o">|</span> <span class="n">llama_api</span>                    <span class="o">|</span> <span class="n">Distribution</span> <span class="k">for</span> <span class="n">running</span> <span class="n">e2e</span> <span class="n">tests</span> <span class="ow">in</span> <span class="n">CI</span>                                    <span class="o">|</span>
<span class="o">+------------------------------+-----------------------------------------------------------------------------+</span>
<span class="o">|</span> <span class="n">hf</span><span class="o">-</span><span class="n">serverless</span>                <span class="o">|</span> <span class="n">Use</span> <span class="p">(</span><span class="n">an</span> <span class="n">external</span><span class="p">)</span> <span class="n">Hugging</span> <span class="n">Face</span> <span class="n">Inference</span> <span class="n">Endpoint</span> <span class="k">for</span> <span class="n">running</span> <span class="n">LLM</span> <span class="n">inference</span> <span class="o">|</span>
<span class="o">+------------------------------+-----------------------------------------------------------------------------+</span>
<span class="o">|</span> <span class="n">hf</span><span class="o">-</span><span class="n">endpoint</span>                  <span class="o">|</span> <span class="n">Use</span> <span class="p">(</span><span class="n">an</span> <span class="n">external</span><span class="p">)</span> <span class="n">Hugging</span> <span class="n">Face</span> <span class="n">Inference</span> <span class="n">Endpoint</span> <span class="k">for</span> <span class="n">running</span> <span class="n">LLM</span> <span class="n">inference</span> <span class="o">|</span>
<span class="o">+------------------------------+-----------------------------------------------------------------------------+</span>
<span class="o">|</span> <span class="n">groq</span>                         <span class="o">|</span> <span class="n">Use</span> <span class="n">Groq</span> <span class="k">for</span> <span class="n">running</span> <span class="n">LLM</span> <span class="n">inference</span>                                          <span class="o">|</span>
<span class="o">+------------------------------+-----------------------------------------------------------------------------+</span>
<span class="o">|</span> <span class="n">fireworks</span>                    <span class="o">|</span> <span class="n">Use</span> <span class="n">Fireworks</span><span class="o">.</span><span class="n">AI</span> <span class="k">for</span> <span class="n">running</span> <span class="n">LLM</span> <span class="n">inference</span>                                  <span class="o">|</span>
<span class="o">+------------------------------+-----------------------------------------------------------------------------+</span>
<span class="o">|</span> <span class="n">experimental</span><span class="o">-</span><span class="n">post</span><span class="o">-</span><span class="n">training</span>   <span class="o">|</span> <span class="n">Experimental</span> <span class="n">template</span> <span class="k">for</span> <span class="n">post</span> <span class="n">training</span>                                     <span class="o">|</span>
<span class="o">+------------------------------+-----------------------------------------------------------------------------+</span>
<span class="o">|</span> <span class="n">dell</span>                         <span class="o">|</span> <span class="n">Dell</span><span class="s1">&#39;s distribution of Llama Stack. TGI inference via Dell&#39;</span><span class="n">s</span> <span class="n">custom</span>         <span class="o">|</span>
<span class="o">|</span>                              <span class="o">|</span> <span class="n">container</span>                                                                   <span class="o">|</span>
<span class="o">+------------------------------+-----------------------------------------------------------------------------+</span>
<span class="o">|</span> <span class="n">ci</span><span class="o">-</span><span class="n">tests</span>                     <span class="o">|</span> <span class="n">Distribution</span> <span class="k">for</span> <span class="n">running</span> <span class="n">e2e</span> <span class="n">tests</span> <span class="ow">in</span> <span class="n">CI</span>                                    <span class="o">|</span>
<span class="o">+------------------------------+-----------------------------------------------------------------------------+</span>
<span class="o">|</span> <span class="n">cerebras</span>                     <span class="o">|</span> <span class="n">Use</span> <span class="n">Cerebras</span> <span class="k">for</span> <span class="n">running</span> <span class="n">LLM</span> <span class="n">inference</span>                                      <span class="o">|</span>
<span class="o">+------------------------------+-----------------------------------------------------------------------------+</span>
<span class="o">|</span> <span class="n">bedrock</span>                      <span class="o">|</span> <span class="n">Use</span> <span class="n">AWS</span> <span class="n">Bedrock</span> <span class="k">for</span> <span class="n">running</span> <span class="n">LLM</span> <span class="n">inference</span> <span class="ow">and</span> <span class="n">safety</span>                        <span class="o">|</span>
<span class="o">+------------------------------+-----------------------------------------------------------------------------+</span>
</pre></div>
</div>
<p>You may then pick a template to build your distribution with providers fitted to your liking.</p>
<p>For example, to build a distribution with TGI as the inference provider, you can run:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ llama stack build --template tgi
...
You can now edit ~/.llama/distributions/llamastack-tgi/tgi-run.yaml and run `llama stack run ~/.llama/distributions/llamastack-tgi/tgi-run.yaml`
</pre></div>
</div>
</div>
<input id="sd-tab-item-1" name="sd-tab-set-0" type="radio">
<label class="sd-tab-label" for="sd-tab-item-1">
Building from Scratch</label><div class="sd-tab-content docutils">
<p>If the provided templates do not fit your use case, you could start off with running <code class="docutils literal notranslate"><span class="pre">llama</span> <span class="pre">stack</span> <span class="pre">build</span></code> which will allow you to a interactively enter wizard where you will be prompted to enter build configurations.</p>
<p>It would be best to start with a template and understand the structure of the config file and the various concepts ( APIS, providers, resources, etc.) before starting from scratch.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>llama stack build

&gt; Enter a name for your Llama Stack (e.g. my-local-stack): my-stack
&gt; Enter the image type you want your Llama Stack to be built as (container or conda or venv): conda

Llama Stack is composed of several APIs working together. Let&#39;s select
the provider types (implementations) you want to use for these APIs.

Tip: use &lt;TAB&gt; to see options for the providers.

&gt; Enter provider for API inference: inline::meta-reference
&gt; Enter provider for API safety: inline::llama-guard
&gt; Enter provider for API agents: inline::meta-reference
&gt; Enter provider for API memory: inline::faiss
&gt; Enter provider for API datasetio: inline::meta-reference
&gt; Enter provider for API scoring: inline::meta-reference
&gt; Enter provider for API eval: inline::meta-reference
&gt; Enter provider for API telemetry: inline::meta-reference

 &gt; (Optional) Enter a short description for your Llama Stack:

You can now edit ~/.llama/distributions/llamastack-my-local-stack/my-local-stack-run.yaml and run `llama stack run ~/.llama/distributions/llamastack-my-local-stack/my-local-stack-run.yaml`
</pre></div>
</div>
</div>
<input id="sd-tab-item-2" name="sd-tab-set-0" type="radio">
<label class="sd-tab-label" for="sd-tab-item-2">
Building from a pre-existing build config file</label><div class="sd-tab-content docutils">
<ul class="simple">
<li><p>In addition to templates, you may customize the build to your liking through editing config files and build from config files with the following command.</p></li>
<li><p>The config file will be of contents like the ones in <code class="docutils literal notranslate"><span class="pre">llama_stack/templates/*build.yaml</span></code>.</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ cat llama_stack/templates/ollama/build.yaml

name: ollama
distribution_spec:
  description: Like local, but use ollama for running LLM inference
  providers:
    inference: remote::ollama
    memory: inline::faiss
    safety: inline::llama-guard
    agents: inline::meta-reference
    telemetry: inline::meta-reference
image_name: ollama
image_type: conda

# If some providers are external, you can specify the path to the implementation
external_providers_dir: ~/.llama/providers.d
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">llama</span> <span class="n">stack</span> <span class="n">build</span> <span class="o">--</span><span class="n">config</span> <span class="n">llama_stack</span><span class="o">/</span><span class="n">templates</span><span class="o">/</span><span class="n">ollama</span><span class="o">/</span><span class="n">build</span><span class="o">.</span><span class="n">yaml</span>
</pre></div>
</div>
</div>
<input id="sd-tab-item-3" name="sd-tab-set-0" type="radio">
<label class="sd-tab-label" for="sd-tab-item-3">
Building with External Providers</label><div class="sd-tab-content docutils">
<p>Llama Stack supports external providers that live outside of the main codebase. This allows you to create and maintain your own providers independently or use community-provided providers.</p>
<p>To build a distribution with external providers, you need to:</p>
<ol class="arabic simple">
<li><p>Configure the <code class="docutils literal notranslate"><span class="pre">external_providers_dir</span></code> in your build configuration file:</p></li>
</ol>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="c1"># Example my-external-stack.yaml with external providers</span>
<span class="nt">version</span><span class="p">:</span><span class="w"> </span><span class="s">&#39;2&#39;</span>
<span class="nt">distribution_spec</span><span class="p">:</span>
<span class="w">  </span><span class="nt">description</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Custom distro for CI tests</span>
<span class="w">  </span><span class="nt">providers</span><span class="p">:</span>
<span class="w">    </span><span class="nt">inference</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">remote::custom_ollama</span>
<span class="c1"># Add more providers as needed</span>
<span class="nt">image_type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">container</span>
<span class="nt">image_name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ci-test</span>
<span class="c1"># Path to external provider implementations</span>
<span class="nt">external_providers_dir</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">~/.llama/providers.d</span>
</pre></div>
</div>
<p>Here’s an example for a custom Ollama provider:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">adapter</span><span class="p">:</span>
<span class="w">  </span><span class="nt">adapter_type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">custom_ollama</span>
<span class="w">  </span><span class="nt">pip_packages</span><span class="p">:</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ollama</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">aiohttp</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">llama-stack-provider-ollama</span><span class="w"> </span><span class="c1"># This is the provider package</span>
<span class="w">  </span><span class="nt">config_class</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">llama_stack_ollama_provider.config.OllamaImplConfig</span>
<span class="w">  </span><span class="nt">module</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">llama_stack_ollama_provider</span>
<span class="nt">api_dependencies</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[]</span>
<span class="nt">optional_api_dependencies</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[]</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">pip_packages</span></code> section lists the Python packages required by the provider, as well as the
provider package itself. The package must be available on PyPI or can be provided from a local
directory or a git repository (git must be installed on the build environment).</p>
<ol class="arabic simple" start="2">
<li><p>Build your distribution using the config file:</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">llama</span> <span class="n">stack</span> <span class="n">build</span> <span class="o">--</span><span class="n">config</span> <span class="n">my</span><span class="o">-</span><span class="n">external</span><span class="o">-</span><span class="n">stack</span><span class="o">.</span><span class="n">yaml</span>
</pre></div>
</div>
<p>For more information on external providers, including directory structure, provider types, and implementation requirements, see the <a class="reference internal" href="../providers/external.html"><span class="std std-doc">External Providers documentation</span></a>.</p>
</div>
<input id="sd-tab-item-4" name="sd-tab-set-0" type="radio">
<label class="sd-tab-label" for="sd-tab-item-4">
Building Container</label><div class="sd-tab-content docutils">
<div class="tip admonition">
<p class="admonition-title">Podman Alternative</p>
<p>Podman is supported as an alternative to Docker. Set <code class="docutils literal notranslate"><span class="pre">CONTAINER_BINARY</span></code> to <code class="docutils literal notranslate"><span class="pre">podman</span></code> in your environment to use Podman.</p>
</div>
<p>To build a container image, you may start off from a template and use the <code class="docutils literal notranslate"><span class="pre">--image-type</span> <span class="pre">container</span></code> flag to specify <code class="docutils literal notranslate"><span class="pre">container</span></code> as the build image type.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">llama</span> <span class="n">stack</span> <span class="n">build</span> <span class="o">--</span><span class="n">template</span> <span class="n">ollama</span> <span class="o">--</span><span class="n">image</span><span class="o">-</span><span class="nb">type</span> <span class="n">container</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ llama stack build --template ollama --image-type container
...
Containerfile created successfully in /tmp/tmp.viA3a3Rdsg/ContainerfileFROM python:3.10-slim
...
</pre></div>
</div>
<p>You can now edit ~/meta-llama/llama-stack/tmp/configs/ollama-run.yaml and run <code class="docutils literal notranslate"><span class="pre">llama</span> <span class="pre">stack</span> <span class="pre">run</span> <span class="pre">~/meta-llama/llama-stack/tmp/configs/ollama-run.yaml</span></code></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>
<span class="n">Now</span> <span class="nb">set</span> <span class="n">some</span> <span class="n">environment</span> <span class="n">variables</span> <span class="k">for</span> <span class="n">the</span> <span class="n">inference</span> <span class="n">model</span> <span class="n">ID</span> <span class="ow">and</span> <span class="n">Llama</span> <span class="n">Stack</span> <span class="n">Port</span> <span class="ow">and</span> <span class="n">create</span> <span class="n">a</span> <span class="n">local</span> <span class="n">directory</span> <span class="n">to</span> <span class="n">mount</span> <span class="n">into</span> <span class="n">the</span> <span class="n">container</span><span class="s1">&#39;s file system.</span>
</pre></div>
</div>
<p>export INFERENCE_MODEL=“llama3.2:3b”
export LLAMA_STACK_PORT=8321
mkdir -p ~/.llama</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>
<span class="n">After</span> <span class="n">this</span> <span class="n">step</span> <span class="ow">is</span> <span class="n">successful</span><span class="p">,</span> <span class="n">you</span> <span class="n">should</span> <span class="n">be</span> <span class="n">able</span> <span class="n">to</span> <span class="n">find</span> <span class="n">the</span> <span class="n">built</span> <span class="n">container</span> <span class="n">image</span> <span class="ow">and</span> <span class="n">test</span> <span class="n">it</span> <span class="k">with</span> <span class="n">the</span> <span class="n">below</span> <span class="n">Docker</span> <span class="n">command</span><span class="p">:</span>

</pre></div>
</div>
<p>docker run -d <br />
-p <span class="math notranslate nohighlight">\(LLAMA_STACK_PORT:\)</span>LLAMA_STACK_PORT <br />
-v ~/.llama:/root/.llama <br />
localhost/distribution-ollama:dev <br />
–port <span class="math notranslate nohighlight">\(LLAMA_STACK_PORT \
  --env INFERENCE_MODEL=\)</span>INFERENCE_MODEL <br />
–env OLLAMA_URL=http://host.docker.internal:11434</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>
Here are the docker flags and their uses:

* `-d`: Runs the container in the detached mode as a background process

* `-p $LLAMA_STACK_PORT:$LLAMA_STACK_PORT`: Maps the container port to the host port for accessing the server

* `-v ~/.llama:/root/.llama`: Mounts the local .llama directory to persist configurations and data

* `localhost/distribution-ollama:dev`: The name and tag of the container image to run

* `--port $LLAMA_STACK_PORT`: Port number for the server to listen on

* `--env INFERENCE_MODEL=$INFERENCE_MODEL`: Sets the model to use for inference

* `--env OLLAMA_URL=http://host.docker.internal:11434`: Configures the URL for the Ollama service

</pre></div>
</div>
</div>
</div>
</section>
<section id="running-your-stack-server">
<h2>Running your Stack server<a class="headerlink" href="#running-your-stack-server" title="Link to this heading"></a></h2>
<p>Now, let’s start the Llama Stack Distribution Server. You will need the YAML configuration file which was written out at the end by the <code class="docutils literal notranslate"><span class="pre">llama</span> <span class="pre">stack</span> <span class="pre">build</span></code> step.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>llama stack run -h
usage: llama stack run [-h] [--port PORT] [--image-name IMAGE_NAME] [--env KEY=VALUE]
                       [--image-type {conda,venv}] [--enable-ui]
                       [config | template]

Start the server for a Llama Stack Distribution. You should have already built (or downloaded) and configured the distribution.

positional arguments:
  config | template     Path to config file to use for the run or name of known template (`llama stack list` for a list). (default: None)

options:
  -h, --help            show this help message and exit
  --port PORT           Port to run the server on. It can also be passed via the env var LLAMA_STACK_PORT. (default: 8321)
  --image-name IMAGE_NAME
                        Name of the image to run. Defaults to the current environment (default: None)
  --env KEY=VALUE       Environment variables to pass to the server in KEY=VALUE format. Can be specified multiple times. (default: None)
  --image-type {conda,venv}
                        Image Type used during the build. This can be either conda or venv. (default: None)
  --enable-ui           Start the UI server (default: False)
</pre></div>
</div>
<p><strong>Note:</strong> Container images built with <code class="docutils literal notranslate"><span class="pre">llama</span> <span class="pre">stack</span> <span class="pre">build</span> <span class="pre">--image-type</span> <span class="pre">container</span></code> cannot be run using <code class="docutils literal notranslate"><span class="pre">llama</span> <span class="pre">stack</span> <span class="pre">run</span></code>. Instead, they must be run directly using Docker or Podman commands as shown in the container building section above.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Start using template name</span>
<span class="n">llama</span> <span class="n">stack</span> <span class="n">run</span> <span class="n">tgi</span>

<span class="c1"># Start using config file</span>
<span class="n">llama</span> <span class="n">stack</span> <span class="n">run</span> <span class="o">~/.</span><span class="n">llama</span><span class="o">/</span><span class="n">distributions</span><span class="o">/</span><span class="n">llamastack</span><span class="o">-</span><span class="n">my</span><span class="o">-</span><span class="n">local</span><span class="o">-</span><span class="n">stack</span><span class="o">/</span><span class="n">my</span><span class="o">-</span><span class="n">local</span><span class="o">-</span><span class="n">stack</span><span class="o">-</span><span class="n">run</span><span class="o">.</span><span class="n">yaml</span>

<span class="c1"># Start using a venv</span>
<span class="n">llama</span> <span class="n">stack</span> <span class="n">run</span> <span class="o">--</span><span class="n">image</span><span class="o">-</span><span class="nb">type</span> <span class="n">venv</span> <span class="o">~/.</span><span class="n">llama</span><span class="o">/</span><span class="n">distributions</span><span class="o">/</span><span class="n">llamastack</span><span class="o">-</span><span class="n">my</span><span class="o">-</span><span class="n">local</span><span class="o">-</span><span class="n">stack</span><span class="o">/</span><span class="n">my</span><span class="o">-</span><span class="n">local</span><span class="o">-</span><span class="n">stack</span><span class="o">-</span><span class="n">run</span><span class="o">.</span><span class="n">yaml</span>

<span class="c1"># Start using a conda environment</span>
<span class="n">llama</span> <span class="n">stack</span> <span class="n">run</span> <span class="o">--</span><span class="n">image</span><span class="o">-</span><span class="nb">type</span> <span class="n">conda</span> <span class="o">~/.</span><span class="n">llama</span><span class="o">/</span><span class="n">distributions</span><span class="o">/</span><span class="n">llamastack</span><span class="o">-</span><span class="n">my</span><span class="o">-</span><span class="n">local</span><span class="o">-</span><span class="n">stack</span><span class="o">/</span><span class="n">my</span><span class="o">-</span><span class="n">local</span><span class="o">-</span><span class="n">stack</span><span class="o">-</span><span class="n">run</span><span class="o">.</span><span class="n">yaml</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ llama stack run ~/.llama/distributions/llamastack-my-local-stack/my-local-stack-run.yaml

Serving API inspect
 GET /health
 GET /providers/list
 GET /routes/list
Serving API inference
 POST /inference/chat_completion
 POST /inference/completion
 POST /inference/embeddings
...
Serving API agents
 POST /agents/create
 POST /agents/session/create
 POST /agents/turn/create
 POST /agents/delete
 POST /agents/session/delete
 POST /agents/session/get
 POST /agents/step/get
 POST /agents/turn/get

Listening on [&#39;::&#39;, &#39;0.0.0.0&#39;]:8321
INFO:     Started server process [2935911]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://[&#39;::&#39;, &#39;0.0.0.0&#39;]:8321 (Press CTRL+C to quit)
INFO:     2401:db00:35c:2d2b:face:0:c9:0:54678 - &quot;GET /models/list HTTP/1.1&quot; 200 OK
</pre></div>
</div>
</section>
<section id="listing-distributions">
<h2>Listing Distributions<a class="headerlink" href="#listing-distributions" title="Link to this heading"></a></h2>
<p>Using the list command, you can view all existing Llama Stack distributions, including stacks built from templates, from scratch, or using custom configuration files.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">llama</span> <span class="n">stack</span> <span class="nb">list</span> <span class="o">-</span><span class="n">h</span>
<span class="n">usage</span><span class="p">:</span> <span class="n">llama</span> <span class="n">stack</span> <span class="nb">list</span> <span class="p">[</span><span class="o">-</span><span class="n">h</span><span class="p">]</span>

<span class="nb">list</span> <span class="n">the</span> <span class="n">build</span> <span class="n">stacks</span>

<span class="n">options</span><span class="p">:</span>
  <span class="o">-</span><span class="n">h</span><span class="p">,</span> <span class="o">--</span><span class="n">help</span>  <span class="n">show</span> <span class="n">this</span> <span class="n">help</span> <span class="n">message</span> <span class="ow">and</span> <span class="n">exit</span>
</pre></div>
</div>
<p>Example Usage</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">llama</span> <span class="n">stack</span> <span class="nb">list</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">------------------------------+-----------------------------------------------------------------------------+--------------+------------+</span>
<span class="o">|</span> <span class="n">Stack</span> <span class="n">Name</span>                  <span class="o">|</span> <span class="n">Path</span>                                                                        <span class="o">|</span> <span class="n">Build</span> <span class="n">Config</span> <span class="o">|</span> <span class="n">Run</span> <span class="n">Config</span> <span class="o">|</span>
<span class="o">+------------------------------+-----------------------------------------------------------------------------+--------------+------------+</span>
<span class="o">|</span> <span class="n">together</span>                    <span class="o">|</span> <span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">wenzhou</span><span class="o">/.</span><span class="n">llama</span><span class="o">/</span><span class="n">distributions</span><span class="o">/</span><span class="n">together</span>                                 <span class="o">|</span> <span class="n">Yes</span>          <span class="o">|</span> <span class="n">No</span>         <span class="o">|</span>
<span class="o">+------------------------------+-----------------------------------------------------------------------------+--------------+------------+</span>
<span class="o">|</span> <span class="n">bedrock</span>                     <span class="o">|</span> <span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">wenzhou</span><span class="o">/.</span><span class="n">llama</span><span class="o">/</span><span class="n">distributions</span><span class="o">/</span><span class="n">bedrock</span>                                  <span class="o">|</span> <span class="n">Yes</span>          <span class="o">|</span> <span class="n">No</span>         <span class="o">|</span>
<span class="o">+------------------------------+-----------------------------------------------------------------------------+--------------+------------+</span>
<span class="o">|</span> <span class="n">starter</span>                     <span class="o">|</span> <span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">wenzhou</span><span class="o">/.</span><span class="n">llama</span><span class="o">/</span><span class="n">distributions</span><span class="o">/</span><span class="n">starter</span>                                  <span class="o">|</span> <span class="n">No</span>           <span class="o">|</span> <span class="n">No</span>         <span class="o">|</span>
<span class="o">+------------------------------+-----------------------------------------------------------------------------+--------------+------------+</span>
<span class="o">|</span> <span class="n">remote</span><span class="o">-</span><span class="n">vllm</span>                 <span class="o">|</span> <span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">wenzhou</span><span class="o">/.</span><span class="n">llama</span><span class="o">/</span><span class="n">distributions</span><span class="o">/</span><span class="n">remote</span><span class="o">-</span><span class="n">vllm</span>                              <span class="o">|</span> <span class="n">Yes</span>          <span class="o">|</span> <span class="n">Yes</span>        <span class="o">|</span>
<span class="o">+------------------------------+-----------------------------------------------------------------------------+--------------+------------+</span>
</pre></div>
</div>
</section>
<section id="removing-a-distribution">
<h2>Removing a Distribution<a class="headerlink" href="#removing-a-distribution" title="Link to this heading"></a></h2>
<p>Use the remove command to delete a distribution you’ve previously built.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">llama</span> <span class="n">stack</span> <span class="n">rm</span> <span class="o">-</span><span class="n">h</span>
<span class="n">usage</span><span class="p">:</span> <span class="n">llama</span> <span class="n">stack</span> <span class="n">rm</span> <span class="p">[</span><span class="o">-</span><span class="n">h</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="nb">all</span><span class="p">]</span> <span class="p">[</span><span class="n">name</span><span class="p">]</span>

<span class="n">Remove</span> <span class="n">the</span> <span class="n">build</span> <span class="n">stack</span>

<span class="n">positional</span> <span class="n">arguments</span><span class="p">:</span>
  <span class="n">name</span>        <span class="n">Name</span> <span class="n">of</span> <span class="n">the</span> <span class="n">stack</span> <span class="n">to</span> <span class="n">delete</span> <span class="p">(</span><span class="n">default</span><span class="p">:</span> <span class="kc">None</span><span class="p">)</span>

<span class="n">options</span><span class="p">:</span>
  <span class="o">-</span><span class="n">h</span><span class="p">,</span> <span class="o">--</span><span class="n">help</span>  <span class="n">show</span> <span class="n">this</span> <span class="n">help</span> <span class="n">message</span> <span class="ow">and</span> <span class="n">exit</span>
  <span class="o">--</span><span class="nb">all</span><span class="p">,</span> <span class="o">-</span><span class="n">a</span>   <span class="n">Delete</span> <span class="nb">all</span> <span class="n">stacks</span> <span class="p">(</span><span class="n">use</span> <span class="k">with</span> <span class="n">caution</span><span class="p">)</span> <span class="p">(</span><span class="n">default</span><span class="p">:</span> <span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">llama</span> <span class="n">stack</span> <span class="n">rm</span> <span class="n">llamastack</span><span class="o">-</span><span class="n">test</span>
</pre></div>
</div>
<p>To keep your environment organized and avoid clutter, consider using <code class="docutils literal notranslate"><span class="pre">llama</span> <span class="pre">stack</span> <span class="pre">list</span></code> to review old or unused distributions and <code class="docutils literal notranslate"><span class="pre">llama</span> <span class="pre">stack</span> <span class="pre">rm</span> <span class="pre">&lt;name&gt;</span></code> to delete them when they’re no longer needed.</p>
</section>
<section id="troubleshooting">
<h2>Troubleshooting<a class="headerlink" href="#troubleshooting" title="Link to this heading"></a></h2>
<p>If you encounter any issues, ask questions in our discord or search through our <a class="reference external" href="https://github.com/meta-llama/llama-stack/issues">GitHub Issues</a>, or file an new issue.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="kubernetes_deployment.html" class="btn btn-neutral float-left" title="Kubernetes Deployment Guide" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../building_applications/index.html" class="btn btn-neutral float-right" title="Building AI Applications (Examples)" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Meta.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  
<div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    <span class="fa fa-book"> Read the Docs</span>
    v: v0.2.14
    <span class="fa fa-caret-down"></span>
  </span>
  <div class="rst-other-versions">
    <dl>
      <dt>Versions</dt>
      <dd>
        <a href="/v0.2.19/">latest</a>
      </dd>
      <dd>
        <a href="/v0.2.19/">v0.2.19</a>
      </dd>
      <dd>
        <a href="/v0.2.18/">v0.2.18</a>
      </dd>
      <dd>
        <a href="/v0.2.17/">v0.2.17</a>
      </dd>
      <dd>
        <a href="/v0.2.16/">v0.2.16</a>
      </dd>
      <dd>
        <a href="/v0.2.15/">v0.2.15</a>
      </dd>
      <dd class="rtd-current-item">
        <a href="/v0.2.14/">v0.2.14</a>
      </dd>
      <dd>
        <a href="/v0.2.13/">v0.2.13</a>
      </dd>
      <dd>
        <a href="/v0.2.12/">v0.2.12</a>
      </dd>
      <dd>
        <a href="/v0.2.11/">v0.2.11</a>
      </dd>
    </dl>
  </div>
</div><script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>