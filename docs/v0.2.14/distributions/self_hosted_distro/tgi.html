


<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>TGI Distribution &mdash; llama-stack 0.2.14 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/my_theme.css?v=f1163765" />
      <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
      <link rel="stylesheet" type="text/css" href="../../_static/dark_mode_css/general.css?v=c0a7eb24" />
      <link rel="stylesheet" type="text/css" href="../../_static/dark_mode_css/dark.css?v=70edf1c7" />

  
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=71c180b9"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../_static/copybutton.js?v=09bf800d"></script>
      <script src="../../_static/js/detect_theme.js?v=76226c80"></script>
      <script src="../../_static/js/keyboard_shortcuts.js"></script>
      <script src="../../_static/design-tabs.js?v=f930bc37"></script>
      <script src="../../_static/dark_mode_js/default_light.js?v=c2e647ce"></script>
      <script src="../../_static/dark_mode_js/theme_switcher.js?v=358d3910"></script>
    <script src="../../_static/js/theme.js"></script>
    <script src="../../_static/js/versions.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="NVIDIA Distribution" href="nvidia.html" />
    <link rel="prev" title="Meta Reference Distribution" href="meta-reference-gpu.html" />
 

<script src="../../_static/version-loader.js"></script>

</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #2980B9" >

          
          
          <a href="../../index.html" class="icon icon-home">
            llama-stack
          </a>
              <div class="switch-menus">
                <div class="version-switch"></div>
                <div class="language-switch"></div>
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../index.html">Llama Stack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/index.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/detailed_tutorial.html">Detailed Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../introduction/index.html">Why Llama Stack?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../concepts/index.html">Core Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../openai/index.html">OpenAI API Compatibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../providers/index.html">Providers Overview</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Distributions Overview</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../importing_as_library.html">Using Llama Stack as a Library</a></li>
<li class="toctree-l2"><a class="reference internal" href="../configuration.html">Configuring a “Stack”</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../list_of_distributions.html">Available List of Distributions</a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="../list_of_distributions.html#selection-of-a-distribution-template">Selection of a Distribution / Template</a><ul class="current">
<li class="toctree-l4 current"><a class="reference internal" href="../list_of_distributions.html#distribution-details">Distribution Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../list_of_distributions.html#on-device-distributions">On-Device Distributions</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../kubernetes_deployment.html">Kubernetes Deployment Guide</a></li>
<li class="toctree-l2"><a class="reference internal" href="../building_distro.html">Build your own Distribution</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../building_applications/index.html">Building AI Applications (Examples)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../playground/index.html">Llama Stack Playground</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../contributing/index.html">Contributing to Llama-Stack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../references/index.html">References</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: #2980B9" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">llama-stack</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html">Distributions Overview</a></li>
          <li class="breadcrumb-item"><a href="../list_of_distributions.html">Available List of Distributions</a></li>
      <li class="breadcrumb-item active">TGI Distribution</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/distributions/self_hosted_distro/tgi.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <!-- This file was auto-generated by distro_codegen.py, please edit source -->
<section class="tex2jax_ignore mathjax_ignore" id="tgi-distribution">
<h1>TGI Distribution<a class="headerlink" href="#tgi-distribution" title="Link to this heading"></a></h1>
<div class="toctree-wrapper compound">
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">llamastack/distribution-tgi</span></code> distribution consists of the following provider configurations.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>API</p></th>
<th class="head"><p>Provider(s)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>agents</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">inline::meta-reference</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>datasetio</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">remote::huggingface</span></code>, <code class="docutils literal notranslate"><span class="pre">inline::localfs</span></code></p></td>
</tr>
<tr class="row-even"><td><p>eval</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">inline::meta-reference</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>inference</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">remote::tgi</span></code>, <code class="docutils literal notranslate"><span class="pre">inline::sentence-transformers</span></code></p></td>
</tr>
<tr class="row-even"><td><p>safety</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">inline::llama-guard</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>scoring</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">inline::basic</span></code>, <code class="docutils literal notranslate"><span class="pre">inline::llm-as-judge</span></code>, <code class="docutils literal notranslate"><span class="pre">inline::braintrust</span></code></p></td>
</tr>
<tr class="row-even"><td><p>telemetry</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">inline::meta-reference</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>tool_runtime</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">remote::brave-search</span></code>, <code class="docutils literal notranslate"><span class="pre">remote::tavily-search</span></code>, <code class="docutils literal notranslate"><span class="pre">inline::rag-runtime</span></code>, <code class="docutils literal notranslate"><span class="pre">remote::model-context-protocol</span></code></p></td>
</tr>
<tr class="row-even"><td><p>vector_io</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">inline::faiss</span></code>, <code class="docutils literal notranslate"><span class="pre">remote::chromadb</span></code>, <code class="docutils literal notranslate"><span class="pre">remote::pgvector</span></code></p></td>
</tr>
</tbody>
</table>
<p>You can use this distribution if you have GPUs and want to run an independent TGI server container for running inference.</p>
<section id="environment-variables">
<h2>Environment Variables<a class="headerlink" href="#environment-variables" title="Link to this heading"></a></h2>
<p>The following environment variables can be configured:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">LLAMA_STACK_PORT</span></code>: Port for the Llama Stack distribution server (default: <code class="docutils literal notranslate"><span class="pre">8321</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">INFERENCE_MODEL</span></code>: Inference model loaded into the TGI server (default: <code class="docutils literal notranslate"><span class="pre">meta-llama/Llama-3.2-3B-Instruct</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">TGI_URL</span></code>: URL of the TGI server with the main inference model (default: <code class="docutils literal notranslate"><span class="pre">http://127.0.0.1:8080/v1</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">TGI_SAFETY_URL</span></code>: URL of the TGI server with the safety model (default: <code class="docutils literal notranslate"><span class="pre">http://127.0.0.1:8081/v1</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">SAFETY_MODEL</span></code>: Name of the safety (Llama-Guard) model to use (default: <code class="docutils literal notranslate"><span class="pre">meta-llama/Llama-Guard-3-1B</span></code>)</p></li>
</ul>
</section>
<section id="setting-up-tgi-server">
<h2>Setting up TGI server<a class="headerlink" href="#setting-up-tgi-server" title="Link to this heading"></a></h2>
<p>Please check the <a class="reference external" href="https://github.com/huggingface/text-generation-inference?tab=readme-ov-file#get-started">TGI Getting Started Guide</a> to get a TGI endpoint. Here is a sample script to start a TGI server locally via Docker:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">INFERENCE_PORT</span><span class="o">=</span><span class="m">8080</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">INFERENCE_MODEL</span><span class="o">=</span>meta-llama/Llama-3.2-3B-Instruct
<span class="nb">export</span><span class="w"> </span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span>

docker<span class="w"> </span>run<span class="w"> </span>--rm<span class="w"> </span>-it<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--pull<span class="w"> </span>always<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-v<span class="w"> </span><span class="nv">$HOME</span>/.cache/huggingface:/data<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-p<span class="w"> </span><span class="nv">$INFERENCE_PORT</span>:<span class="nv">$INFERENCE_PORT</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--gpus<span class="w"> </span><span class="nv">$CUDA_VISIBLE_DEVICES</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>ghcr.io/huggingface/text-generation-inference:2.3.1<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--dtype<span class="w"> </span>bfloat16<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--usage-stats<span class="w"> </span>off<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--sharded<span class="w"> </span><span class="nb">false</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--cuda-memory-fraction<span class="w"> </span><span class="m">0</span>.7<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--model-id<span class="w"> </span><span class="nv">$INFERENCE_MODEL</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--port<span class="w"> </span><span class="nv">$INFERENCE_PORT</span>
</pre></div>
</div>
<p>If you are using Llama Stack Safety / Shield APIs, then you will need to also run another instance of a TGI with a corresponding safety model like <code class="docutils literal notranslate"><span class="pre">meta-llama/Llama-Guard-3-1B</span></code> using a script like:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">SAFETY_PORT</span><span class="o">=</span><span class="m">8081</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">SAFETY_MODEL</span><span class="o">=</span>meta-llama/Llama-Guard-3-1B
<span class="nb">export</span><span class="w"> </span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">1</span>

docker<span class="w"> </span>run<span class="w"> </span>--rm<span class="w"> </span>-it<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--pull<span class="w"> </span>always<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-v<span class="w"> </span><span class="nv">$HOME</span>/.cache/huggingface:/data<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-p<span class="w"> </span><span class="nv">$SAFETY_PORT</span>:<span class="nv">$SAFETY_PORT</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--gpus<span class="w"> </span><span class="nv">$CUDA_VISIBLE_DEVICES</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>ghcr.io/huggingface/text-generation-inference:2.3.1<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--dtype<span class="w"> </span>bfloat16<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--usage-stats<span class="w"> </span>off<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--sharded<span class="w"> </span><span class="nb">false</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--model-id<span class="w"> </span><span class="nv">$SAFETY_MODEL</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--port<span class="w"> </span><span class="nv">$SAFETY_PORT</span>
</pre></div>
</div>
</section>
<section id="running-llama-stack">
<h2>Running Llama Stack<a class="headerlink" href="#running-llama-stack" title="Link to this heading"></a></h2>
<p>Now you are ready to run Llama Stack with TGI as the inference provider. You can do this via Conda (build code) or Docker which has a pre-built image.</p>
<section id="via-docker">
<h3>Via Docker<a class="headerlink" href="#via-docker" title="Link to this heading"></a></h3>
<p>This method allows you to get started quickly without having to build the distribution code.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">LLAMA_STACK_PORT</span><span class="o">=</span><span class="m">8321</span>
docker<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-it<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--pull<span class="w"> </span>always<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-p<span class="w"> </span><span class="nv">$LLAMA_STACK_PORT</span>:<span class="nv">$LLAMA_STACK_PORT</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>llamastack/distribution-tgi<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--port<span class="w"> </span><span class="nv">$LLAMA_STACK_PORT</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">INFERENCE_MODEL</span><span class="o">=</span><span class="nv">$INFERENCE_MODEL</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">TGI_URL</span><span class="o">=</span>http://host.docker.internal:<span class="nv">$INFERENCE_PORT</span>
</pre></div>
</div>
<p>If you are using Llama Stack Safety / Shield APIs, use:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># You need a local checkout of llama-stack to run this, get it using</span>
<span class="c1"># git clone https://github.com/meta-llama/llama-stack.git</span>
<span class="nb">cd</span><span class="w"> </span>/path/to/llama-stack

docker<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-it<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--pull<span class="w"> </span>always<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-p<span class="w"> </span><span class="nv">$LLAMA_STACK_PORT</span>:<span class="nv">$LLAMA_STACK_PORT</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-v<span class="w"> </span>~/.llama:/root/.llama<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-v<span class="w"> </span>./llama_stack/templates/tgi/run-with-safety.yaml:/root/my-run.yaml<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>llamastack/distribution-tgi<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--config<span class="w"> </span>/root/my-run.yaml<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--port<span class="w"> </span><span class="nv">$LLAMA_STACK_PORT</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">INFERENCE_MODEL</span><span class="o">=</span><span class="nv">$INFERENCE_MODEL</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">TGI_URL</span><span class="o">=</span>http://host.docker.internal:<span class="nv">$INFERENCE_PORT</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">SAFETY_MODEL</span><span class="o">=</span><span class="nv">$SAFETY_MODEL</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">TGI_SAFETY_URL</span><span class="o">=</span>http://host.docker.internal:<span class="nv">$SAFETY_PORT</span>
</pre></div>
</div>
</section>
<section id="via-conda">
<h3>Via Conda<a class="headerlink" href="#via-conda" title="Link to this heading"></a></h3>
<p>Make sure you have done <code class="docutils literal notranslate"><span class="pre">uv</span> <span class="pre">pip</span> <span class="pre">install</span> <span class="pre">llama-stack</span></code> and have the Llama Stack CLI available.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>llama<span class="w"> </span>stack<span class="w"> </span>build<span class="w"> </span>--template<span class="w"> </span>tgi<span class="w"> </span>--image-type<span class="w"> </span>conda
llama<span class="w"> </span>stack<span class="w"> </span>run<span class="w"> </span>./run.yaml
<span class="w">  </span>--port<span class="w"> </span><span class="nv">$LLAMA_STACK_PORT</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">INFERENCE_MODEL</span><span class="o">=</span><span class="nv">$INFERENCE_MODEL</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">TGI_URL</span><span class="o">=</span>http://127.0.0.1:<span class="nv">$INFERENCE_PORT</span>
</pre></div>
</div>
<p>If you are using Llama Stack Safety / Shield APIs, use:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>llama<span class="w"> </span>stack<span class="w"> </span>run<span class="w"> </span>./run-with-safety.yaml<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--port<span class="w"> </span><span class="nv">$LLAMA_STACK_PORT</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">INFERENCE_MODEL</span><span class="o">=</span><span class="nv">$INFERENCE_MODEL</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">TGI_URL</span><span class="o">=</span>http://127.0.0.1:<span class="nv">$INFERENCE_PORT</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">SAFETY_MODEL</span><span class="o">=</span><span class="nv">$SAFETY_MODEL</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">TGI_SAFETY_URL</span><span class="o">=</span>http://127.0.0.1:<span class="nv">$SAFETY_PORT</span>
</pre></div>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="meta-reference-gpu.html" class="btn btn-neutral float-left" title="Meta Reference Distribution" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="nvidia.html" class="btn btn-neutral float-right" title="NVIDIA Distribution" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Meta.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  
<div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    <span class="fa fa-book"> Read the Docs</span>
    v: v0.2.14
    <span class="fa fa-caret-down"></span>
  </span>
  <div class="rst-other-versions">
    <dl>
      <dt>Versions</dt>
      <dd>
        <a href="/v0.2.19/">latest</a>
      </dd>
      <dd>
        <a href="/v0.2.19/">v0.2.19</a>
      </dd>
      <dd>
        <a href="/v0.2.18/">v0.2.18</a>
      </dd>
      <dd>
        <a href="/v0.2.17/">v0.2.17</a>
      </dd>
      <dd>
        <a href="/v0.2.16/">v0.2.16</a>
      </dd>
      <dd>
        <a href="/v0.2.15/">v0.2.15</a>
      </dd>
      <dd class="rtd-current-item">
        <a href="/v0.2.14/">v0.2.14</a>
      </dd>
      <dd>
        <a href="/v0.2.13/">v0.2.13</a>
      </dd>
      <dd>
        <a href="/v0.2.12/">v0.2.12</a>
      </dd>
      <dd>
        <a href="/v0.2.11/">v0.2.11</a>
      </dd>
    </dl>
  </div>
</div><script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>