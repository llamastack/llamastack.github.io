


<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>NVIDIA NEMO &mdash; llama-stack 0.2.12 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/my_theme.css?v=f1163765" />
      <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
      <link rel="stylesheet" type="text/css" href="../../_static/dark_mode_css/general.css?v=c0a7eb24" />
      <link rel="stylesheet" type="text/css" href="../../_static/dark_mode_css/dark.css?v=70edf1c7" />

  
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=1e30946d"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../_static/copybutton.js?v=09bf800d"></script>
      <script src="../../_static/js/detect_theme.js?v=76226c80"></script>
      <script src="../../_static/js/keyboard_shortcuts.js"></script>
      <script src="../../_static/design-tabs.js?v=f930bc37"></script>
      <script src="../../_static/dark_mode_js/default_light.js?v=c2e647ce"></script>
      <script src="../../_static/dark_mode_js/theme_switcher.js?v=358d3910"></script>
    <script src="../../_static/js/theme.js"></script>
    <script src="../../_static/js/versions.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Faiss" href="../vector_io/faiss.html" />
    <link rel="prev" title="TorchTune" href="torchtune.html" />
 

<script src="../../_static/version-loader.js"></script>

</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #2980B9" >

          
          
          <a href="../../index.html" class="icon icon-home">
            llama-stack
          </a>
              <div class="switch-menus">
                <div class="version-switch"></div>
                <div class="language-switch"></div>
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../index.html">Llama Stack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/index.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/detailed_tutorial.html">Detailed Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../introduction/index.html">Why Llama Stack?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../concepts/index.html">Core Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../openai/index.html">OpenAI API Compatibility</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Providers Overview</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../index.html#external-providers">External Providers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#agents">Agents</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#datasetio">DatasetIO</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#eval">Eval</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#inference">Inference</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../index.html#post-training">Post Training</a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="../index.html#post-training-providers">Post Training Providers</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="../external.html">External Providers</a></li>
<li class="toctree-l4"><a class="reference internal" href="huggingface.html">HuggingFace SFTTrainer</a></li>
<li class="toctree-l4"><a class="reference internal" href="torchtune.html">TorchTune</a></li>
<li class="toctree-l4 current"><a class="current reference internal" href="#">NVIDIA NEMO</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#safety">Safety</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#scoring">Scoring</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#telemetry">Telemetry</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#tool-runtime">Tool Runtime</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#vector-io">Vector IO</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../distributions/index.html">Distributions Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../building_applications/index.html">Building AI Applications (Examples)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../playground/index.html">Llama Stack Playground</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../contributing/index.html">Contributing to Llama-Stack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../references/index.html">References</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: #2980B9" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">llama-stack</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html">Providers Overview</a></li>
      <li class="breadcrumb-item active">NVIDIA NEMO</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/providers/post_training/nvidia_nemo.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="nvidia-nemo">
<h1>NVIDIA NEMO<a class="headerlink" href="#nvidia-nemo" title="Link to this heading"></a></h1>
<p><a class="reference external" href="https://developer.nvidia.com/nemo-framework">NVIDIA NEMO</a> is a remote post training provider for Llama Stack. It provides enterprise-grade fine-tuning capabilities through NVIDIA’s NeMo Customizer service.</p>
<section id="features">
<h2>Features<a class="headerlink" href="#features" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p>Enterprise-grade fine-tuning capabilities</p></li>
<li><p>Support for LoRA and SFT fine-tuning</p></li>
<li><p>Integration with NVIDIA’s NeMo Customizer service</p></li>
<li><p>Support for various NVIDIA-optimized models</p></li>
<li><p>Efficient training with NVIDIA hardware acceleration</p></li>
</ul>
</section>
<section id="usage">
<h2>Usage<a class="headerlink" href="#usage" title="Link to this heading"></a></h2>
<p>To use NVIDIA NEMO in your Llama Stack project, follow these steps:</p>
<ol class="arabic simple">
<li><p>Configure your Llama Stack project to use this provider.</p></li>
<li><p>Set up your NVIDIA API credentials.</p></li>
<li><p>Kick off a fine-tuning job using the Llama Stack post_training API.</p></li>
</ol>
</section>
<section id="setup">
<h2>Setup<a class="headerlink" href="#setup" title="Link to this heading"></a></h2>
<p>You’ll need to set the following environment variables:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">NVIDIA_API_KEY</span><span class="o">=</span><span class="s2">&quot;your-api-key&quot;</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NVIDIA_DATASET_NAMESPACE</span><span class="o">=</span><span class="s2">&quot;default&quot;</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NVIDIA_CUSTOMIZER_URL</span><span class="o">=</span><span class="s2">&quot;your-customizer-url&quot;</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NVIDIA_PROJECT_ID</span><span class="o">=</span><span class="s2">&quot;your-project-id&quot;</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">NVIDIA_OUTPUT_MODEL_DIR</span><span class="o">=</span><span class="s2">&quot;your-output-model-dir&quot;</span>
</pre></div>
</div>
</section>
<section id="run-training">
<h2>Run Training<a class="headerlink" href="#run-training" title="Link to this heading"></a></h2>
<p>You can access the provider and the <code class="docutils literal notranslate"><span class="pre">supervised_fine_tune</span></code> method via the post_training API:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">uuid</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">llama_stack_client.types</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">post_training_supervised_fine_tune_params</span><span class="p">,</span>
    <span class="n">algorithm_config_param</span><span class="p">,</span>
<span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">create_http_client</span><span class="p">():</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">llama_stack_client</span><span class="w"> </span><span class="kn">import</span> <span class="n">LlamaStackClient</span>

    <span class="k">return</span> <span class="n">LlamaStackClient</span><span class="p">(</span><span class="n">base_url</span><span class="o">=</span><span class="s2">&quot;http://localhost:8321&quot;</span><span class="p">)</span>


<span class="n">client</span> <span class="o">=</span> <span class="n">create_http_client</span><span class="p">()</span>

<span class="c1"># Example Dataset</span>
<span class="n">client</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">register</span><span class="p">(</span>
    <span class="n">purpose</span><span class="o">=</span><span class="s2">&quot;post-training/messages&quot;</span><span class="p">,</span>
    <span class="n">source</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;uri&quot;</span><span class="p">,</span>
        <span class="s2">&quot;uri&quot;</span><span class="p">:</span> <span class="s2">&quot;huggingface://datasets/llamastack/simpleqa?split=train&quot;</span><span class="p">,</span>
    <span class="p">},</span>
    <span class="n">dataset_id</span><span class="o">=</span><span class="s2">&quot;simpleqa&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">training_config</span> <span class="o">=</span> <span class="n">post_training_supervised_fine_tune_params</span><span class="o">.</span><span class="n">TrainingConfig</span><span class="p">(</span>
    <span class="n">data_config</span><span class="o">=</span><span class="n">post_training_supervised_fine_tune_params</span><span class="o">.</span><span class="n">TrainingConfigDataConfig</span><span class="p">(</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>  <span class="c1"># Default batch size for NEMO</span>
        <span class="n">data_format</span><span class="o">=</span><span class="s2">&quot;instruct&quot;</span><span class="p">,</span>
        <span class="n">dataset_id</span><span class="o">=</span><span class="s2">&quot;simpleqa&quot;</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">),</span>
    <span class="n">n_epochs</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>  <span class="c1"># Default epochs for NEMO</span>
    <span class="n">optimizer_config</span><span class="o">=</span><span class="n">post_training_supervised_fine_tune_params</span><span class="o">.</span><span class="n">TrainingConfigOptimizerConfig</span><span class="p">(</span>
        <span class="n">lr</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">,</span>  <span class="c1"># Default learning rate</span>
        <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>  <span class="c1"># NEMO-specific parameter</span>
    <span class="p">),</span>
    <span class="c1"># NEMO-specific parameters</span>
    <span class="n">log_every_n_steps</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">val_check_interval</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span>
    <span class="n">sequence_packing_enabled</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">hidden_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">attention_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">ffn_dropout</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">algorithm_config</span> <span class="o">=</span> <span class="n">algorithm_config_param</span><span class="o">.</span><span class="n">LoraFinetuningConfig</span><span class="p">(</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>  <span class="c1"># Default alpha for NEMO</span>
    <span class="nb">type</span><span class="o">=</span><span class="s2">&quot;LoRA&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">job_uuid</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;test-job</span><span class="si">{</span><span class="n">uuid</span><span class="o">.</span><span class="n">uuid4</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span>

<span class="c1"># Example Model - must be a supported NEMO model</span>
<span class="n">training_model</span> <span class="o">=</span> <span class="s2">&quot;meta/llama-3.1-8b-instruct&quot;</span>

<span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">post_training</span><span class="o">.</span><span class="n">supervised_fine_tune</span><span class="p">(</span>
    <span class="n">job_uuid</span><span class="o">=</span><span class="n">job_uuid</span><span class="p">,</span>
    <span class="n">logger_config</span><span class="o">=</span><span class="p">{},</span>
    <span class="n">model</span><span class="o">=</span><span class="n">training_model</span><span class="p">,</span>
    <span class="n">hyperparam_search_config</span><span class="o">=</span><span class="p">{},</span>
    <span class="n">training_config</span><span class="o">=</span><span class="n">training_config</span><span class="p">,</span>
    <span class="n">algorithm_config</span><span class="o">=</span><span class="n">algorithm_config</span><span class="p">,</span>
    <span class="n">checkpoint_dir</span><span class="o">=</span><span class="s2">&quot;output&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Job: &quot;</span><span class="p">,</span> <span class="n">job_uuid</span><span class="p">)</span>

<span class="c1"># Wait for the job to complete!</span>
<span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
    <span class="n">status</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">post_training</span><span class="o">.</span><span class="n">job</span><span class="o">.</span><span class="n">status</span><span class="p">(</span><span class="n">job_uuid</span><span class="o">=</span><span class="n">job_uuid</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">status</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Job not found&quot;</span><span class="p">)</span>
        <span class="k">break</span>

    <span class="nb">print</span><span class="p">(</span><span class="n">status</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">status</span><span class="o">.</span><span class="n">status</span> <span class="o">==</span> <span class="s2">&quot;completed&quot;</span><span class="p">:</span>
        <span class="k">break</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Waiting for job to complete...&quot;</span><span class="p">)</span>
    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>

<span class="n">end_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Job completed in&quot;</span><span class="p">,</span> <span class="n">end_time</span> <span class="o">-</span> <span class="n">start_time</span><span class="p">,</span> <span class="s2">&quot;seconds!&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Artifacts:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">client</span><span class="o">.</span><span class="n">post_training</span><span class="o">.</span><span class="n">job</span><span class="o">.</span><span class="n">artifacts</span><span class="p">(</span><span class="n">job_uuid</span><span class="o">=</span><span class="n">job_uuid</span><span class="p">))</span>
</pre></div>
</div>
</section>
<section id="supported-models">
<h2>Supported Models<a class="headerlink" href="#supported-models" title="Link to this heading"></a></h2>
<p>Currently supports the following models:</p>
<ul class="simple">
<li><p>meta/llama-3.1-8b-instruct</p></li>
<li><p>meta/llama-3.2-1b-instruct</p></li>
</ul>
</section>
<section id="supported-parameters">
<h2>Supported Parameters<a class="headerlink" href="#supported-parameters" title="Link to this heading"></a></h2>
<section id="trainingconfig">
<h3>TrainingConfig<a class="headerlink" href="#trainingconfig" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p>n_epochs (default: 50)</p></li>
<li><p>data_config</p></li>
<li><p>optimizer_config</p></li>
<li><p>log_every_n_steps</p></li>
<li><p>val_check_interval (default: 0.25)</p></li>
<li><p>sequence_packing_enabled (default: False)</p></li>
<li><p>hidden_dropout (0.0-1.0)</p></li>
<li><p>attention_dropout (0.0-1.0)</p></li>
<li><p>ffn_dropout (0.0-1.0)</p></li>
</ul>
</section>
<section id="dataconfig">
<h3>DataConfig<a class="headerlink" href="#dataconfig" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p>dataset_id</p></li>
<li><p>batch_size (default: 8)</p></li>
</ul>
</section>
<section id="optimizerconfig">
<h3>OptimizerConfig<a class="headerlink" href="#optimizerconfig" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p>lr (default: 0.0001)</p></li>
<li><p>weight_decay (default: 0.01)</p></li>
</ul>
</section>
<section id="lora-config">
<h3>LoRA Config<a class="headerlink" href="#lora-config" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p>alpha (default: 16)</p></li>
<li><p>type (must be “LoRA”)</p></li>
</ul>
<p>Note: Some parameters from the standard Llama Stack API are not supported and will be ignored with a warning.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="torchtune.html" class="btn btn-neutral float-left" title="TorchTune" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../vector_io/faiss.html" class="btn btn-neutral float-right" title="Faiss" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Meta.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  
<div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    <span class="fa fa-book"> Read the Docs</span>
    v: v0.2.12
    <span class="fa fa-caret-down"></span>
  </span>
  <div class="rst-other-versions">
    <dl>
      <dt>Versions</dt>
      <dd>
        <a href="/v0.2.19/">latest</a>
      </dd>
      <dd>
        <a href="/v0.2.19/">v0.2.19</a>
      </dd>
      <dd>
        <a href="/v0.2.18/">v0.2.18</a>
      </dd>
      <dd>
        <a href="/v0.2.17/">v0.2.17</a>
      </dd>
      <dd>
        <a href="/v0.2.16/">v0.2.16</a>
      </dd>
      <dd>
        <a href="/v0.2.15/">v0.2.15</a>
      </dd>
      <dd>
        <a href="/v0.2.14/">v0.2.14</a>
      </dd>
      <dd>
        <a href="/v0.2.13/">v0.2.13</a>
      </dd>
      <dd class="rtd-current-item">
        <a href="/v0.2.12/">v0.2.12</a>
      </dd>
      <dd>
        <a href="/v0.2.11/">v0.2.11</a>
      </dd>
    </dl>
  </div>
</div><script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>