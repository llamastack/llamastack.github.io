


<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Starter Distribution &mdash; llama-stack 0.2.16 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/my_theme.css?v=f1163765" />
      <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
      <link rel="stylesheet" type="text/css" href="../../_static/dark_mode_css/general.css?v=c0a7eb24" />
      <link rel="stylesheet" type="text/css" href="../../_static/dark_mode_css/dark.css?v=70edf1c7" />

  
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=e2418eca"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../_static/copybutton.js?v=09bf800d"></script>
      <script src="../../_static/js/detect_theme.js?v=76226c80"></script>
      <script src="../../_static/js/keyboard_shortcuts.js"></script>
      <script src="../../_static/design-tabs.js?v=f930bc37"></script>
      <script src="../../_static/dark_mode_js/default_light.js?v=c2e647ce"></script>
      <script src="../../_static/dark_mode_js/theme_switcher.js?v=358d3910"></script>
    <script src="../../_static/js/theme.js"></script>
    <script src="../../_static/js/versions.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Meta Reference Distribution" href="meta-reference-gpu.html" />
    <link rel="prev" title="Available Distributions" href="../list_of_distributions.html" />
 

<script src="../../_static/version-loader.js"></script>

</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #2980B9" >

          
          
          <a href="../../index.html" class="icon icon-home">
            llama-stack
          </a>
              <div class="switch-menus">
                <div class="version-switch"></div>
                <div class="language-switch"></div>
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../index.html">Llama Stack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/index.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../concepts/index.html">Core Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../providers/index.html">API Providers Overview</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Distributions Overview</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../list_of_distributions.html">Available Distributions</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../list_of_distributions.html#quick-reference">Quick Reference</a></li>
<li class="toctree-l3"><a class="reference internal" href="../list_of_distributions.html#choose-your-distribution">Choose Your Distribution</a></li>
<li class="toctree-l3 current"><a class="reference internal" href="../list_of_distributions.html#detailed-documentation">Detailed Documentation</a><ul class="current">
<li class="toctree-l4 current"><a class="reference internal" href="../list_of_distributions.html#self-hosted-distributions">Self-Hosted Distributions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../list_of_distributions.html#remote-hosted-solutions">Remote-Hosted Solutions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../list_of_distributions.html#mobile-sdks">Mobile SDKs</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../list_of_distributions.html#decision-flow">Decision Flow</a></li>
<li class="toctree-l3"><a class="reference internal" href="../list_of_distributions.html#next-steps">Next Steps</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../building_distro.html">Build your own Distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../customizing_run_yaml.html">Customizing run.yaml Files</a></li>
<li class="toctree-l2"><a class="reference internal" href="../importing_as_library.html">Using Llama Stack as a Library</a></li>
<li class="toctree-l2"><a class="reference internal" href="../configuration.html">Configuring a “Stack”</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced_apis/index.html">Advanced APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../building_applications/index.html">AI Application Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../deploying/index.html">Deployment Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../contributing/index.html">Contributing to Llama-Stack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../references/index.html">References</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: #2980B9" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">llama-stack</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html">Distributions Overview</a></li>
          <li class="breadcrumb-item"><a href="../list_of_distributions.html">Available Distributions</a></li>
      <li class="breadcrumb-item active">Starter Distribution</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/distributions/self_hosted_distro/starter.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <!-- This file was auto-generated by distro_codegen.py, please edit source -->
<section class="tex2jax_ignore mathjax_ignore" id="starter-distribution">
<h1>Starter Distribution<a class="headerlink" href="#starter-distribution" title="Link to this heading"></a></h1>
<div class="toctree-wrapper compound">
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">llamastack/distribution-starter</span></code> distribution is a comprehensive, multi-provider distribution that includes most of the available inference providers in Llama Stack. It’s designed to be a one-stop solution for developers who want to experiment with different AI providers without having to configure each one individually.</p>
<section id="provider-composition">
<h2>Provider Composition<a class="headerlink" href="#provider-composition" title="Link to this heading"></a></h2>
<p>The starter distribution consists of the following provider configurations:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>API</p></th>
<th class="head"><p>Provider(s)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>agents</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">inline::meta-reference</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>datasetio</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">remote::huggingface</span></code>, <code class="docutils literal notranslate"><span class="pre">inline::localfs</span></code></p></td>
</tr>
<tr class="row-even"><td><p>eval</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">inline::meta-reference</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>files</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">inline::localfs</span></code></p></td>
</tr>
<tr class="row-even"><td><p>inference</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">remote::openai</span></code>, <code class="docutils literal notranslate"><span class="pre">remote::fireworks</span></code>, <code class="docutils literal notranslate"><span class="pre">remote::together</span></code>, <code class="docutils literal notranslate"><span class="pre">remote::ollama</span></code>, <code class="docutils literal notranslate"><span class="pre">remote::anthropic</span></code>, <code class="docutils literal notranslate"><span class="pre">remote::gemini</span></code>, <code class="docutils literal notranslate"><span class="pre">remote::groq</span></code>, <code class="docutils literal notranslate"><span class="pre">remote::sambanova</span></code>, <code class="docutils literal notranslate"><span class="pre">remote::vllm</span></code>, <code class="docutils literal notranslate"><span class="pre">remote::tgi</span></code>, <code class="docutils literal notranslate"><span class="pre">remote::cerebras</span></code>, <code class="docutils literal notranslate"><span class="pre">remote::llama-openai-compat</span></code>, <code class="docutils literal notranslate"><span class="pre">remote::nvidia</span></code>, <code class="docutils literal notranslate"><span class="pre">remote::hf::serverless</span></code>, <code class="docutils literal notranslate"><span class="pre">remote::hf::endpoint</span></code>, <code class="docutils literal notranslate"><span class="pre">inline::sentence-transformers</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>safety</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">inline::llama-guard</span></code></p></td>
</tr>
<tr class="row-even"><td><p>scoring</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">inline::basic</span></code>, <code class="docutils literal notranslate"><span class="pre">inline::llm-as-judge</span></code>, <code class="docutils literal notranslate"><span class="pre">inline::braintrust</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>telemetry</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">inline::meta-reference</span></code></p></td>
</tr>
<tr class="row-even"><td><p>tool_runtime</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">remote::brave-search</span></code>, <code class="docutils literal notranslate"><span class="pre">remote::tavily-search</span></code>, <code class="docutils literal notranslate"><span class="pre">inline::rag-runtime</span></code>, <code class="docutils literal notranslate"><span class="pre">remote::model-context-protocol</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>vector_io</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">inline::faiss</span></code>, <code class="docutils literal notranslate"><span class="pre">inline::sqlite-vec</span></code>, <code class="docutils literal notranslate"><span class="pre">inline::milvus</span></code>, <code class="docutils literal notranslate"><span class="pre">remote::chromadb</span></code>, <code class="docutils literal notranslate"><span class="pre">remote::pgvector</span></code></p></td>
</tr>
</tbody>
</table>
</section>
<section id="inference-providers">
<h2>Inference Providers<a class="headerlink" href="#inference-providers" title="Link to this heading"></a></h2>
<p>The starter distribution includes a comprehensive set of inference providers:</p>
<section id="hosted-providers">
<h3>Hosted Providers<a class="headerlink" href="#hosted-providers" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p><strong><a class="reference external" href="https://openai.com/api/">OpenAI</a></strong>: GPT-4, GPT-3.5, O1, O3, O4 models and text embeddings -
provider ID: <code class="docutils literal notranslate"><span class="pre">openai</span></code> - reference documentation: <a class="reference internal" href="../../providers/inference/remote_openai.html"><span class="std std-doc">openai</span></a></p></li>
<li><p><strong><a class="reference external" href="https://fireworks.ai/">Fireworks</a></strong>: Llama 3.1, 3.2, 3.3, 4 Scout, 4 Maverick models and
embeddings - provider ID: <code class="docutils literal notranslate"><span class="pre">fireworks</span></code> - reference documentation: <a class="reference internal" href="../../providers/inference/remote_fireworks.html"><span class="std std-doc">fireworks</span></a></p></li>
<li><p><strong><a class="reference external" href="https://together.ai/">Together</a></strong>: Llama 3.1, 3.2, 3.3, 4 Scout, 4 Maverick models and
embeddings - provider ID: <code class="docutils literal notranslate"><span class="pre">together</span></code> - reference documentation: <a class="reference internal" href="../../providers/inference/remote_together.html"><span class="std std-doc">together</span></a></p></li>
<li><p><strong><a class="reference external" href="https://www.anthropic.com/">Anthropic</a></strong>: Claude 3.5 Sonnet, Claude 3.7 Sonnet, Claude 3.5 Haiku, and Voyage embeddings - provider ID: <code class="docutils literal notranslate"><span class="pre">anthropic</span></code> - reference documentation: <a class="reference internal" href="../../providers/inference/remote_anthropic.html"><span class="std std-doc">anthropic</span></a></p></li>
<li><p><strong><a class="reference external" href="https://gemini.google.com/">Gemini</a></strong>: Gemini 1.5, 2.0, 2.5 models and text embeddings - provider ID: <code class="docutils literal notranslate"><span class="pre">gemini</span></code> - reference documentation: <a class="reference internal" href="../../providers/inference/remote_gemini.html"><span class="std std-doc">gemini</span></a></p></li>
<li><p><strong><a class="reference external" href="https://groq.com/">Groq</a></strong>: Fast Llama models (3.1, 3.2, 3.3, 4 Scout, 4 Maverick) - provider ID: <code class="docutils literal notranslate"><span class="pre">groq</span></code> - reference documentation: <a class="reference internal" href="../../providers/inference/remote_groq.html"><span class="std std-doc">groq</span></a></p></li>
<li><p><strong><a class="reference external" href="https://www.sambanova.ai/">SambaNova</a></strong>: Llama 3.1, 3.2, 3.3, 4 Scout, 4 Maverick models - provider ID: <code class="docutils literal notranslate"><span class="pre">sambanova</span></code> - reference documentation: <a class="reference internal" href="../../providers/inference/remote_sambanova.html"><span class="std std-doc">sambanova</span></a></p></li>
<li><p><strong><a class="reference external" href="https://www.cerebras.ai/">Cerebras</a></strong>: Cerebras AI models - provider ID: <code class="docutils literal notranslate"><span class="pre">cerebras</span></code> - reference documentation: <a class="reference internal" href="../../providers/inference/remote_cerebras.html"><span class="std std-doc">cerebras</span></a></p></li>
<li><p><strong><a class="reference external" href="https://www.nvidia.com/">NVIDIA</a></strong>: NVIDIA NIM - provider ID: <code class="docutils literal notranslate"><span class="pre">nvidia</span></code> - reference documentation: <a class="reference internal" href="../../providers/inference/remote_nvidia.html"><span class="std std-doc">nvidia</span></a></p></li>
<li><p><strong><a class="reference external" href="https://huggingface.co/">HuggingFace</a></strong>: Serverless and endpoint models - provider ID: <code class="docutils literal notranslate"><span class="pre">hf::serverless</span></code> and <code class="docutils literal notranslate"><span class="pre">hf::endpoint</span></code> - reference documentation: <a class="reference internal" href="../../providers/inference/remote_hf_serverless.html"><span class="std std-doc">huggingface-serverless</span></a> and <a class="reference internal" href="../../providers/inference/remote_hf_endpoint.html"><span class="std std-doc">huggingface-endpoint</span></a></p></li>
<li><p><strong><a class="reference external" href="https://aws.amazon.com/bedrock/">Bedrock</a></strong>: AWS Bedrock models - provider ID: <code class="docutils literal notranslate"><span class="pre">bedrock</span></code> - reference documentation: <a class="reference internal" href="../../providers/inference/remote_bedrock.html"><span class="std std-doc">bedrock</span></a></p></li>
</ul>
</section>
<section id="local-remote-providers">
<h3>Local/Remote Providers<a class="headerlink" href="#local-remote-providers" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p><strong><a class="reference external" href="https://ollama.ai/">Ollama</a></strong>: Local Ollama models - provider ID: <code class="docutils literal notranslate"><span class="pre">ollama</span></code> - reference documentation: <a class="reference internal" href="../../providers/inference/remote_ollama.html"><span class="std std-doc">ollama</span></a></p></li>
<li><p><strong><a class="reference external" href="https://docs.vllm.ai/en/latest/">vLLM</a></strong>: Local or remote vLLM server - provider ID: <code class="docutils literal notranslate"><span class="pre">vllm</span></code> - reference documentation: <a class="reference internal" href="../../providers/inference/remote_vllm.html"><span class="std std-doc">vllm</span></a></p></li>
<li><p><strong><a class="reference external" href="https://github.com/huggingface/text-generation-inference">TGI</a></strong>: Text Generation Inference server - Dell Enterprise Hub’s custom TGI container too (use <code class="docutils literal notranslate"><span class="pre">DEH_URL</span></code>) - provider ID: <code class="docutils literal notranslate"><span class="pre">tgi</span></code> - reference documentation: <a class="reference internal" href="../../providers/inference/remote_tgi.html"><span class="std std-doc">tgi</span></a></p></li>
<li><p><strong><a class="reference external" href="https://www.sbert.net/">Sentence Transformers</a></strong>: Local embedding models - provider ID: <code class="docutils literal notranslate"><span class="pre">sentence-transformers</span></code> - reference documentation: <a class="reference internal" href="../../providers/inference/inline_sentence-transformers.html"><span class="std std-doc">sentence-transformers</span></a></p></li>
</ul>
<p>All providers are disabled by default. So you need to enable them by setting the environment variables.</p>
</section>
</section>
<section id="vector-io">
<h2>Vector IO<a class="headerlink" href="#vector-io" title="Link to this heading"></a></h2>
<p>The starter distribution includes a comprehensive set of vector IO providers:</p>
<ul class="simple">
<li><p><strong><a class="reference external" href="https://github.com/facebookresearch/faiss">FAISS</a></strong>: Local FAISS vector store - enabled by
default - provider ID: <code class="docutils literal notranslate"><span class="pre">faiss</span></code></p></li>
<li><p><strong><a class="reference external" href="https://www.sqlite.org/index.html">SQLite</a></strong>: Local SQLite vector store - disabled by default - provider ID: <code class="docutils literal notranslate"><span class="pre">sqlite-vec</span></code></p></li>
<li><p><strong><a class="reference external" href="https://www.trychroma.com/">ChromaDB</a></strong>: Remote ChromaDB vector store - disabled by default - provider ID: <code class="docutils literal notranslate"><span class="pre">chromadb</span></code></p></li>
<li><p><strong><a class="reference external" href="https://github.com/pgvector/pgvector">PGVector</a></strong>: PostgreSQL vector store - disabled by default - provider ID: <code class="docutils literal notranslate"><span class="pre">pgvector</span></code></p></li>
<li><p><strong><a class="reference external" href="https://milvus.io/">Milvus</a></strong>: Milvus vector store - disabled by default - provider ID: <code class="docutils literal notranslate"><span class="pre">milvus</span></code></p></li>
</ul>
</section>
<section id="environment-variables">
<h2>Environment Variables<a class="headerlink" href="#environment-variables" title="Link to this heading"></a></h2>
<p>The following environment variables can be configured:</p>
<section id="server-configuration">
<h3>Server Configuration<a class="headerlink" href="#server-configuration" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">LLAMA_STACK_PORT</span></code>: Port for the Llama Stack distribution server (default: <code class="docutils literal notranslate"><span class="pre">8321</span></code>)</p></li>
</ul>
</section>
<section id="api-keys-for-hosted-providers">
<h3>API Keys for Hosted Providers<a class="headerlink" href="#api-keys-for-hosted-providers" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">OPENAI_API_KEY</span></code>: OpenAI API key</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">FIREWORKS_API_KEY</span></code>: Fireworks API key</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">TOGETHER_API_KEY</span></code>: Together API key</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ANTHROPIC_API_KEY</span></code>: Anthropic API key</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">GEMINI_API_KEY</span></code>: Google Gemini API key</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">GROQ_API_KEY</span></code>: Groq API key</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">SAMBANOVA_API_KEY</span></code>: SambaNova API key</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">CEREBRAS_API_KEY</span></code>: Cerebras API key</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">LLAMA_API_KEY</span></code>: Llama API key</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">NVIDIA_API_KEY</span></code>: NVIDIA API key</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">HF_API_TOKEN</span></code>: HuggingFace API token</p></li>
</ul>
</section>
<section id="local-provider-configuration">
<h3>Local Provider Configuration<a class="headerlink" href="#local-provider-configuration" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">OLLAMA_URL</span></code>: Ollama server URL (default: <code class="docutils literal notranslate"><span class="pre">http://localhost:11434</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">VLLM_URL</span></code>: vLLM server URL (default: <code class="docutils literal notranslate"><span class="pre">http://localhost:8000/v1</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">VLLM_MAX_TOKENS</span></code>: vLLM max tokens (default: <code class="docutils literal notranslate"><span class="pre">4096</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">VLLM_API_TOKEN</span></code>: vLLM API token (default: <code class="docutils literal notranslate"><span class="pre">fake</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">VLLM_TLS_VERIFY</span></code>: vLLM TLS verification (default: <code class="docutils literal notranslate"><span class="pre">true</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">TGI_URL</span></code>: TGI server URL</p></li>
</ul>
</section>
<section id="model-configuration">
<h3>Model Configuration<a class="headerlink" href="#model-configuration" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">INFERENCE_MODEL</span></code>: HuggingFace model for serverless inference</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">INFERENCE_ENDPOINT_NAME</span></code>: HuggingFace endpoint name</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">OLLAMA_INFERENCE_MODEL</span></code>: Ollama model name</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">OLLAMA_EMBEDDING_MODEL</span></code>: Ollama embedding model name</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">OLLAMA_EMBEDDING_DIMENSION</span></code>: Ollama embedding dimension (default: <code class="docutils literal notranslate"><span class="pre">384</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">VLLM_INFERENCE_MODEL</span></code>: vLLM model name</p></li>
</ul>
</section>
<section id="vector-database-configuration">
<h3>Vector Database Configuration<a class="headerlink" href="#vector-database-configuration" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">SQLITE_STORE_DIR</span></code>: SQLite store directory (default: <code class="docutils literal notranslate"><span class="pre">~/.llama/distributions/starter</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ENABLE_SQLITE_VEC</span></code>: Enable SQLite vector provider</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ENABLE_CHROMADB</span></code>: Enable ChromaDB provider</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ENABLE_PGVECTOR</span></code>: Enable PGVector provider</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">CHROMADB_URL</span></code>: ChromaDB server URL</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">PGVECTOR_HOST</span></code>: PGVector host (default: <code class="docutils literal notranslate"><span class="pre">localhost</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">PGVECTOR_PORT</span></code>: PGVector port (default: <code class="docutils literal notranslate"><span class="pre">5432</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">PGVECTOR_DB</span></code>: PGVector database name</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">PGVECTOR_USER</span></code>: PGVector username</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">PGVECTOR_PASSWORD</span></code>: PGVector password</p></li>
</ul>
</section>
<section id="tool-configuration">
<h3>Tool Configuration<a class="headerlink" href="#tool-configuration" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">BRAVE_SEARCH_API_KEY</span></code>: Brave Search API key</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">TAVILY_SEARCH_API_KEY</span></code>: Tavily Search API key</p></li>
</ul>
</section>
<section id="telemetry-configuration">
<h3>Telemetry Configuration<a class="headerlink" href="#telemetry-configuration" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">OTEL_SERVICE_NAME</span></code>: OpenTelemetry service name</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">TELEMETRY_SINKS</span></code>: Telemetry sinks (default: <code class="docutils literal notranslate"><span class="pre">console,sqlite</span></code>)</p></li>
</ul>
</section>
</section>
<section id="enabling-providers">
<h2>Enabling Providers<a class="headerlink" href="#enabling-providers" title="Link to this heading"></a></h2>
<p>You can enable specific providers by setting their provider ID to a valid value using environment variables. This is useful when you want to use certain providers or don’t have the required API keys.</p>
<section id="examples-of-enabling-providers">
<h3>Examples of Enabling Providers<a class="headerlink" href="#examples-of-enabling-providers" title="Link to this heading"></a></h3>
<section id="enable-faiss-vector-provider">
<h4>Enable FAISS Vector Provider<a class="headerlink" href="#enable-faiss-vector-provider" title="Link to this heading"></a></h4>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">ENABLE_FAISS</span><span class="o">=</span>faiss
</pre></div>
</div>
</section>
<section id="enable-ollama-models">
<h4>Enable Ollama Models<a class="headerlink" href="#enable-ollama-models" title="Link to this heading"></a></h4>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">ENABLE_OLLAMA</span><span class="o">=</span>ollama
</pre></div>
</div>
</section>
<section id="disable-vllm-models">
<h4>Disable vLLM Models<a class="headerlink" href="#disable-vllm-models" title="Link to this heading"></a></h4>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">VLLM_INFERENCE_MODEL</span><span class="o">=</span>__disabled__
</pre></div>
</div>
</section>
<section id="disable-optional-vector-providers">
<h4>Disable Optional Vector Providers<a class="headerlink" href="#disable-optional-vector-providers" title="Link to this heading"></a></h4>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">ENABLE_SQLITE_VEC</span><span class="o">=</span>__disabled__
<span class="nb">export</span><span class="w"> </span><span class="nv">ENABLE_CHROMADB</span><span class="o">=</span>__disabled__
<span class="nb">export</span><span class="w"> </span><span class="nv">ENABLE_PGVECTOR</span><span class="o">=</span>__disabled__
</pre></div>
</div>
</section>
</section>
<section id="provider-id-patterns">
<h3>Provider ID Patterns<a class="headerlink" href="#provider-id-patterns" title="Link to this heading"></a></h3>
<p>The starter distribution uses several patterns for provider IDs:</p>
<ol class="arabic simple">
<li><p><strong>Direct provider IDs</strong>: <code class="docutils literal notranslate"><span class="pre">faiss</span></code>, <code class="docutils literal notranslate"><span class="pre">ollama</span></code>, <code class="docutils literal notranslate"><span class="pre">vllm</span></code></p></li>
<li><p><strong>Environment-based provider IDs</strong>: <code class="docutils literal notranslate"><span class="pre">${env.ENABLE_SQLITE_VEC:+sqlite-vec}</span></code></p></li>
<li><p><strong>Model-based provider IDs</strong>: <code class="docutils literal notranslate"><span class="pre">${env.OLLAMA_INFERENCE_MODEL:__disabled__}</span></code></p></li>
</ol>
<p>When using the <code class="docutils literal notranslate"><span class="pre">+</span></code> pattern (like <code class="docutils literal notranslate"><span class="pre">${env.ENABLE_SQLITE_VEC+sqlite-vec}</span></code>), the provider is enabled by default and can be disabled by setting the environment variable to <code class="docutils literal notranslate"><span class="pre">__disabled__</span></code>.</p>
<p>When using the <code class="docutils literal notranslate"><span class="pre">:</span></code> pattern (like <code class="docutils literal notranslate"><span class="pre">${env.OLLAMA_INFERENCE_MODEL:__disabled__}</span></code>), the provider is disabled by default and can be enabled by setting the environment variable to a valid value.</p>
</section>
</section>
<section id="running-the-distribution">
<h2>Running the Distribution<a class="headerlink" href="#running-the-distribution" title="Link to this heading"></a></h2>
<p>You can run the starter distribution via Docker, Conda, or venv.</p>
<section id="via-docker">
<h3>Via Docker<a class="headerlink" href="#via-docker" title="Link to this heading"></a></h3>
<p>This method allows you to get started quickly without having to build the distribution code.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">LLAMA_STACK_PORT</span><span class="o">=</span><span class="m">8321</span>
docker<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-it<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--pull<span class="w"> </span>always<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-p<span class="w"> </span><span class="nv">$LLAMA_STACK_PORT</span>:<span class="nv">$LLAMA_STACK_PORT</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-e<span class="w"> </span><span class="nv">OPENAI_API_KEY</span><span class="o">=</span>your_openai_key<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-e<span class="w"> </span><span class="nv">FIREWORKS_API_KEY</span><span class="o">=</span>your_fireworks_key<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-e<span class="w"> </span><span class="nv">TOGETHER_API_KEY</span><span class="o">=</span>your_together_key<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>llamastack/distribution-starter<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--port<span class="w"> </span><span class="nv">$LLAMA_STACK_PORT</span>
</pre></div>
</div>
</section>
<section id="via-conda-or-venv">
<h3>Via Conda or venv<a class="headerlink" href="#via-conda-or-venv" title="Link to this heading"></a></h3>
<p>Ensure you have configured the starter distribution using the environment variables explained above.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>uv<span class="w"> </span>run<span class="w"> </span>--with<span class="w"> </span>llama-stack<span class="w"> </span>llama<span class="w"> </span>stack<span class="w"> </span>build<span class="w"> </span>--template<span class="w"> </span>starter<span class="w"> </span>--image-type<span class="w"> </span>&lt;conda<span class="p">|</span>venv&gt;<span class="w"> </span>--run
</pre></div>
</div>
</section>
</section>
<section id="example-usage">
<h2>Example Usage<a class="headerlink" href="#example-usage" title="Link to this heading"></a></h2>
<p>Once the distribution is running, you can use any of the available models. Here are some examples:</p>
<section id="using-openai-models">
<h3>Using OpenAI Models<a class="headerlink" href="#using-openai-models" title="Link to this heading"></a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>llama-stack-client<span class="w"> </span>--endpoint<span class="w"> </span>http://localhost:8321<span class="w"> </span><span class="se">\</span>
inference<span class="w"> </span>chat-completion<span class="w"> </span><span class="se">\</span>
--model-id<span class="w"> </span>openai/gpt-4o<span class="w"> </span><span class="se">\</span>
--message<span class="w"> </span><span class="s2">&quot;Hello, how are you?&quot;</span>
</pre></div>
</div>
</section>
<section id="using-fireworks-models">
<h3>Using Fireworks Models<a class="headerlink" href="#using-fireworks-models" title="Link to this heading"></a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>llama-stack-client<span class="w"> </span>--endpoint<span class="w"> </span>http://localhost:8321<span class="w"> </span><span class="se">\</span>
inference<span class="w"> </span>chat-completion<span class="w"> </span><span class="se">\</span>
--model-id<span class="w"> </span>fireworks/meta-llama/Llama-3.2-3B-Instruct<span class="w"> </span><span class="se">\</span>
--message<span class="w"> </span><span class="s2">&quot;Write a short story about a robot.&quot;</span>
</pre></div>
</div>
</section>
<section id="using-local-ollama-models">
<h3>Using Local Ollama Models<a class="headerlink" href="#using-local-ollama-models" title="Link to this heading"></a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># First, make sure Ollama is running and you have a model</span>
ollama<span class="w"> </span>run<span class="w"> </span>llama3.2:3b

<span class="c1"># Then use it through Llama Stack</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">OLLAMA_INFERENCE_MODEL</span><span class="o">=</span>llama3.2:3b
llama-stack-client<span class="w"> </span>--endpoint<span class="w"> </span>http://localhost:8321<span class="w"> </span><span class="se">\</span>
inference<span class="w"> </span>chat-completion<span class="w"> </span><span class="se">\</span>
--model-id<span class="w"> </span>ollama/llama3.2:3b<span class="w"> </span><span class="se">\</span>
--message<span class="w"> </span><span class="s2">&quot;Explain quantum computing in simple terms.&quot;</span>
</pre></div>
</div>
</section>
</section>
<section id="storage">
<h2>Storage<a class="headerlink" href="#storage" title="Link to this heading"></a></h2>
<p>The starter distribution uses SQLite for local storage of various components:</p>
<ul class="simple">
<li><p><strong>Metadata store</strong>: <code class="docutils literal notranslate"><span class="pre">~/.llama/distributions/starter/registry.db</span></code></p></li>
<li><p><strong>Inference store</strong>: <code class="docutils literal notranslate"><span class="pre">~/.llama/distributions/starter/inference_store.db</span></code></p></li>
<li><p><strong>FAISS store</strong>: <code class="docutils literal notranslate"><span class="pre">~/.llama/distributions/starter/faiss_store.db</span></code></p></li>
<li><p><strong>SQLite vector store</strong>: <code class="docutils literal notranslate"><span class="pre">~/.llama/distributions/starter/sqlite_vec.db</span></code></p></li>
<li><p><strong>Files metadata</strong>: <code class="docutils literal notranslate"><span class="pre">~/.llama/distributions/starter/files_metadata.db</span></code></p></li>
<li><p><strong>Agents store</strong>: <code class="docutils literal notranslate"><span class="pre">~/.llama/distributions/starter/agents_store.db</span></code></p></li>
<li><p><strong>Responses store</strong>: <code class="docutils literal notranslate"><span class="pre">~/.llama/distributions/starter/responses_store.db</span></code></p></li>
<li><p><strong>Trace store</strong>: <code class="docutils literal notranslate"><span class="pre">~/.llama/distributions/starter/trace_store.db</span></code></p></li>
<li><p><strong>Evaluation store</strong>: <code class="docutils literal notranslate"><span class="pre">~/.llama/distributions/starter/meta_reference_eval.db</span></code></p></li>
<li><p><strong>Dataset I/O stores</strong>: Various HuggingFace and local filesystem stores</p></li>
</ul>
</section>
<section id="benefits-of-the-starter-distribution">
<h2>Benefits of the Starter Distribution<a class="headerlink" href="#benefits-of-the-starter-distribution" title="Link to this heading"></a></h2>
<ol class="arabic simple">
<li><p><strong>Comprehensive Coverage</strong>: Includes most popular AI providers in one distribution</p></li>
<li><p><strong>Flexible Configuration</strong>: Easy to enable/disable providers based on your needs</p></li>
<li><p><strong>No Local GPU Required</strong>: Most providers are cloud-based, making it accessible to developers without high-end hardware</p></li>
<li><p><strong>Easy Migration</strong>: Start with hosted providers and gradually move to local ones as needed</p></li>
<li><p><strong>Production Ready</strong>: Includes safety, evaluation, and telemetry components</p></li>
<li><p><strong>Tool Integration</strong>: Comes with web search, RAG, and model context protocol tools</p></li>
</ol>
<p>The starter distribution is ideal for developers who want to experiment with different AI providers, build prototypes quickly, or create applications that can work with multiple AI backends.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../list_of_distributions.html" class="btn btn-neutral float-left" title="Available Distributions" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="meta-reference-gpu.html" class="btn btn-neutral float-right" title="Meta Reference Distribution" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Meta.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  
<div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    <span class="fa fa-book"> Read the Docs</span>
    v: v0.2.16
    <span class="fa fa-caret-down"></span>
  </span>
  <div class="rst-other-versions">
    <dl>
      <dt>Versions</dt>
      <dd>
        <a href="/v0.2.19/">latest</a>
      </dd>
      <dd>
        <a href="/v0.2.19/">v0.2.19</a>
      </dd>
      <dd>
        <a href="/v0.2.18/">v0.2.18</a>
      </dd>
      <dd>
        <a href="/v0.2.17/">v0.2.17</a>
      </dd>
      <dd class="rtd-current-item">
        <a href="/v0.2.16/">v0.2.16</a>
      </dd>
      <dd>
        <a href="/v0.2.15/">v0.2.15</a>
      </dd>
      <dd>
        <a href="/v0.2.14/">v0.2.14</a>
      </dd>
      <dd>
        <a href="/v0.2.13/">v0.2.13</a>
      </dd>
      <dd>
        <a href="/v0.2.12/">v0.2.12</a>
      </dd>
      <dd>
        <a href="/v0.2.11/">v0.2.11</a>
      </dd>
    </dl>
  </div>
</div><script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>