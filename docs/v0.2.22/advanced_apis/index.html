


<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Advanced APIs &mdash; llama-stack 0.2.22 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../_static/css/my_theme.css?v=e0175246" />
      <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
      <link rel="stylesheet" type="text/css" href="../_static/dark_mode_css/general.css?v=c0a7eb24" />
      <link rel="stylesheet" type="text/css" href="../_static/dark_mode_css/dark.css?v=70edf1c7" />

  
      <script src="../_static/jquery.js?v=5d32c60e"></script>
      <script src="../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../_static/documentation_options.js?v=b360e4a1"></script>
      <script src="../_static/doctools.js?v=9bcbadda"></script>
      <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../_static/copybutton.js?v=09bf800d"></script>
      <script src="../_static/js/detect_theme.js?v=76226c80"></script>
      <script src="../_static/js/keyboard_shortcuts.js?v=62563c3b"></script>
      <script src="../_static/design-tabs.js?v=f930bc37"></script>
      <script src="../_static/dark_mode_js/default_light.js?v=c2e647ce"></script>
      <script src="../_static/dark_mode_js/theme_switcher.js?v=358d3910"></script>
    <script src="../_static/js/theme.js"></script>
    <script src="../_static/js/versions.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Post_Training Providers" href="post_training/index.html" />
    <link rel="prev" title="Configuring a “Stack”" href="../distributions/configuration.html" />
 

<script src="../_static/version-loader.js"></script>

</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #2980B9" >

          
          
          <a href="../index.html" class="icon icon-home">
            llama-stack
          </a>
              <div class="switch-menus">
                <div class="version-switch"></div>
                <div class="language-switch"></div>
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../index.html">Llama Stack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/index.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../concepts/index.html">Core Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../providers/index.html">API Providers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../distributions/index.html">Distributions Overview</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Advanced APIs</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#post-training">Post-training</a><ul>
<li class="toctree-l3"><a class="reference internal" href="post_training/index.html">Post_Training Providers</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#eval">Eval</a><ul>
<li class="toctree-l3"><a class="reference internal" href="eval/index.html">Eval Providers</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#evaluation-concepts">Evaluation Concepts</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#open-benchmark-eval">Open-benchmark Eval</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#list-of-open-benchmarks-llama-stack-support">List of open-benchmarks Llama Stack support</a></li>
<li class="toctree-l4"><a class="reference internal" href="#run-evaluation-on-open-benchmarks-via-cli">Run evaluation on open-benchmarks via CLI</a></li>
<li class="toctree-l4"><a class="reference internal" href="#spin-up-llama-stack-server">Spin up Llama Stack server</a></li>
<li class="toctree-l4"><a class="reference internal" href="#run-eval-cli">Run eval CLI</a></li>
<li class="toctree-l4"><a class="reference internal" href="#whats-next">What’s Next?</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#scoring">Scoring</a><ul>
<li class="toctree-l3"><a class="reference internal" href="scoring/index.html">Scoring Providers</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../building_applications/index.html">AI Application Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploying/index.html">Deployment Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contributing/index.html">Contributing to Llama Stack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contributing/index.html#llama-stack-benchmark-suite-on-kubernetes">Llama Stack Benchmark Suite on Kubernetes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references/index.html">References</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: #2980B9" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">llama-stack</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Advanced APIs</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/advanced_apis/index.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section class="tex2jax_ignore mathjax_ignore" id="advanced-apis">
<h1>Advanced APIs<a class="headerlink" href="#advanced-apis" title="Link to this heading"></a></h1>
<section id="post-training">
<h2>Post-training<a class="headerlink" href="#post-training" title="Link to this heading"></a></h2>
<p>Fine-tunes a model.</p>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="post_training/index.html">Post_Training Providers</a></li>
</ul>
</div>
</section>
<section id="eval">
<h2>Eval<a class="headerlink" href="#eval" title="Link to this heading"></a></h2>
<p>Generates outputs (via Inference or Agents) and perform scoring.</p>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="eval/index.html">Eval Providers</a></li>
</ul>
</div>
</section>
<section id="evaluation-concepts">
<h2>Evaluation Concepts<a class="headerlink" href="#evaluation-concepts" title="Link to this heading"></a></h2>
<p>The Llama Stack Evaluation flow allows you to run evaluations on your GenAI application datasets or pre-registered benchmarks.</p>
<p>We introduce a set of APIs in Llama Stack for supporting running evaluations of LLM applications.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">/datasetio</span></code> + <code class="docutils literal notranslate"><span class="pre">/datasets</span></code> API</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">/scoring</span></code> + <code class="docutils literal notranslate"><span class="pre">/scoring_functions</span></code> API</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">/eval</span></code> + <code class="docutils literal notranslate"><span class="pre">/benchmarks</span></code> API</p></li>
</ul>
<p>This guide goes over the sets of APIs and developer experience flow of using Llama Stack to run evaluations for different use cases. Checkout our Colab notebook on working examples with evaluations <a class="reference external" href="https://colab.research.google.com/drive/10CHyykee9j2OigaIcRv47BKG9mrNm0tJ?usp=sharing">here</a>.</p>
<p>The Evaluation APIs are associated with a set of Resources. Please visit the Resources section in our <a class="reference internal" href="../concepts/index.html"><span class="std std-doc">Core Concepts</span></a> guide for better high-level understanding.</p>
<ul class="simple">
<li><p><strong>DatasetIO</strong>: defines interface with datasets and data loaders.</p>
<ul>
<li><p>Associated with <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> resource.</p></li>
</ul>
</li>
<li><p><strong>Scoring</strong>: evaluate outputs of the system.</p>
<ul>
<li><p>Associated with <code class="docutils literal notranslate"><span class="pre">ScoringFunction</span></code> resource. We provide a suite of out-of-the box scoring functions and also the ability for you to add custom evaluators. These scoring functions are the core part of defining an evaluation task to output evaluation metrics.</p></li>
</ul>
</li>
<li><p><strong>Eval</strong>: generate outputs (via Inference or Agents) and perform scoring.</p>
<ul>
<li><p>Associated with <code class="docutils literal notranslate"><span class="pre">Benchmark</span></code> resource.</p></li>
</ul>
</li>
</ul>
<section id="open-benchmark-eval">
<h3>Open-benchmark Eval<a class="headerlink" href="#open-benchmark-eval" title="Link to this heading"></a></h3>
<section id="list-of-open-benchmarks-llama-stack-support">
<h4>List of open-benchmarks Llama Stack support<a class="headerlink" href="#list-of-open-benchmarks-llama-stack-support" title="Link to this heading"></a></h4>
<p>Llama stack pre-registers several popular open-benchmarks to easily evaluate model perfomance via CLI.</p>
<p>The list of open-benchmarks we currently support:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/abs/2009.03300">MMLU-COT</a> (Measuring Massive Multitask Language Understanding): Benchmark designed to comprehensively evaluate the breadth and depth of a model’s academic and professional understanding</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2311.12022">GPQA-COT</a> (A Graduate-Level Google-Proof Q&amp;A Benchmark): A challenging benchmark of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry.</p></li>
<li><p><a class="reference external" href="https://openai.com/index/introducing-simpleqa/">SimpleQA</a>: Benchmark designed to access models to answer short, fact-seeking questions.</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2311.16502">MMMU</a> (A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI)]: Benchmark designed to evaluate multimodal models.</p></li>
</ul>
<p>You can follow this <a class="reference internal" href="../references/evals_reference/index.html#open-benchmark-contributing-guide"><span class="std std-ref">contributing guide</span></a> to add more open-benchmarks to Llama Stack</p>
</section>
<section id="run-evaluation-on-open-benchmarks-via-cli">
<h4>Run evaluation on open-benchmarks via CLI<a class="headerlink" href="#run-evaluation-on-open-benchmarks-via-cli" title="Link to this heading"></a></h4>
<p>We have built-in functionality to run the supported open-benckmarks using llama-stack-client CLI</p>
</section>
<section id="spin-up-llama-stack-server">
<h4>Spin up Llama Stack server<a class="headerlink" href="#spin-up-llama-stack-server" title="Link to this heading"></a></h4>
<p>Spin up llama stack server with ‘open-benchmark’ template</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">llama</span> <span class="n">stack</span> <span class="n">run</span> <span class="n">llama_stack</span><span class="o">/</span><span class="n">distributions</span><span class="o">/</span><span class="nb">open</span><span class="o">-</span><span class="n">benchmark</span><span class="o">/</span><span class="n">run</span><span class="o">.</span><span class="n">yaml</span>

</pre></div>
</div>
</section>
<section id="run-eval-cli">
<h4>Run eval CLI<a class="headerlink" href="#run-eval-cli" title="Link to this heading"></a></h4>
<p>There are 3 necessary inputs to run a benchmark eval</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">list</span> <span class="pre">of</span> <span class="pre">benchmark_ids</span></code>: The list of benchmark ids to run evaluation on</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">model-id</span></code>: The model id to evaluate on</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">output_dir</span></code>: Path to store the evaluate results</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">llama</span><span class="o">-</span><span class="n">stack</span><span class="o">-</span><span class="n">client</span> <span class="nb">eval</span> <span class="n">run</span><span class="o">-</span><span class="n">benchmark</span> <span class="o">&lt;</span><span class="n">benchmark_id_1</span><span class="o">&gt;</span> <span class="o">&lt;</span><span class="n">benchmark_id_2</span><span class="o">&gt;</span> <span class="o">...</span> \
<span class="o">--</span><span class="n">model_id</span> <span class="o">&lt;</span><span class="n">model</span> <span class="nb">id</span> <span class="n">to</span> <span class="n">evaluate</span> <span class="n">on</span><span class="o">&gt;</span> \
<span class="o">--</span><span class="n">output_dir</span> <span class="o">&lt;</span><span class="n">directory</span> <span class="n">to</span> <span class="n">store</span> <span class="n">the</span> <span class="n">evaluate</span> <span class="n">results</span><span class="o">&gt;</span> \
</pre></div>
</div>
<p>You can run</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">llama</span><span class="o">-</span><span class="n">stack</span><span class="o">-</span><span class="n">client</span> <span class="nb">eval</span> <span class="n">run</span><span class="o">-</span><span class="n">benchmark</span> <span class="n">help</span>
</pre></div>
</div>
<p>to see the description of all the flags that eval run-benchmark has</p>
<p>In the output log, you can find the file path that has your evaluation results. Open that file and you can see you aggregate
evaluation results over there.</p>
</section>
<section id="whats-next">
<h4>What’s Next?<a class="headerlink" href="#whats-next" title="Link to this heading"></a></h4>
<ul class="simple">
<li><p>Check out our Colab notebook on working examples with running benchmark evaluations <a class="reference external" href="https://colab.research.google.com/github/meta-llama/llama-stack/blob/main/docs/notebooks/Llama_Stack_Benchmark_Evals.ipynb#scrollTo=mxLCsP4MvFqP">here</a>.</p></li>
<li><p>Check out our <a class="reference internal" href="../building_applications/evals.html"><span class="std std-doc">Building Applications - Evaluation</span></a> guide for more details on how to use the Evaluation APIs to evaluate your applications.</p></li>
<li><p>Check out our <a class="reference internal" href="../references/evals_reference/index.html"><span class="std std-doc">Evaluation Reference</span></a> for more details on the APIs.</p></li>
</ul>
</section>
</section>
</section>
<section id="scoring">
<h2>Scoring<a class="headerlink" href="#scoring" title="Link to this heading"></a></h2>
<p>Evaluates the outputs of the system.</p>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="scoring/index.html">Scoring Providers</a></li>
</ul>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../distributions/configuration.html" class="btn btn-neutral float-left" title="Configuring a “Stack”" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="post_training/index.html" class="btn btn-neutral float-right" title="Post_Training Providers" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Meta.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  
<div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    <span class="fa fa-book"> Read the Docs</span>
    v: v0.2.22
    <span class="fa fa-caret-down"></span>
  </span>
  <div class="rst-other-versions">
    <dl>
      <dt>Versions</dt>
      <dd>
        <a href="/v0.2.21/">latest</a>
      </dd>
      <dd>
        <a href="/v0.2.21/">v0.2.21</a>
      </dd>
      <dd>
        <a href="/v0.2.20/">v0.2.20</a>
      </dd>
      <dd>
        <a href="/v0.2.19/">v0.2.19</a>
      </dd>
      <dd>
        <a href="/v0.2.18/">v0.2.18</a>
      </dd>
      <dd>
        <a href="/v0.2.17/">v0.2.17</a>
      </dd>
      <dd>
        <a href="/v0.2.16/">v0.2.16</a>
      </dd>
      <dd>
        <a href="/v0.2.15/">v0.2.15</a>
      </dd>
      <dd>
        <a href="/v0.2.14/">v0.2.14</a>
      </dd>
      <dd>
        <a href="/v0.2.13/">v0.2.13</a>
      </dd>
      <dd>
        <a href="/v0.2.12/">v0.2.12</a>
      </dd>
      <dd>
        <a href="/v0.2.11/">v0.2.11</a>
      </dd>
    </dl>
  </div>
</div><script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>