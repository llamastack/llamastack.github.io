


<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Remote vLLM Distribution &mdash; llama-stack 0.2.11 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/my_theme.css?v=f1163765" />
      <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
      <link rel="stylesheet" type="text/css" href="../../_static/dark_mode_css/general.css?v=c0a7eb24" />
      <link rel="stylesheet" type="text/css" href="../../_static/dark_mode_css/dark.css?v=70edf1c7" />

  
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=29c81e07"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
      <script src="../../_static/copybutton.js?v=09bf800d"></script>
      <script src="../../_static/js/detect_theme.js?v=76226c80"></script>
      <script src="../../_static/js/keyboard_shortcuts.js"></script>
      <script src="../../_static/design-tabs.js?v=f930bc37"></script>
      <script src="../../_static/dark_mode_js/default_light.js?v=c2e647ce"></script>
      <script src="../../_static/dark_mode_js/theme_switcher.js?v=358d3910"></script>
    <script src="../../_static/js/theme.js"></script>
    <script src="../../_static/js/versions.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Meta Reference Distribution" href="meta-reference-gpu.html" />
    <link rel="prev" title="Remote-Hosted Distributions" href="../remote_hosted_distro/index.html" />
 

<script src="../../_static/version-loader.js"></script>

</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search"  style="background: #2980B9" >

          
          
          <a href="../../index.html" class="icon icon-home">
            llama-stack
          </a>
              <div class="switch-menus">
                <div class="version-switch"></div>
                <div class="language-switch"></div>
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../index.html">Llama Stack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/index.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/detailed_tutorial.html">Detailed Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../introduction/index.html">Why Llama Stack?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../concepts/index.html">Core Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../openai/index.html">OpenAI API Compatibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../providers/index.html">Providers Overview</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Distributions Overview</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../importing_as_library.html">Using Llama Stack as a Library</a></li>
<li class="toctree-l2"><a class="reference internal" href="../configuration.html">Configuring a “Stack”</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../list_of_distributions.html">Available List of Distributions</a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="../list_of_distributions.html#selection-of-a-distribution-template">Selection of a Distribution / Template</a><ul class="current">
<li class="toctree-l4 current"><a class="reference internal" href="../list_of_distributions.html#distribution-details">Distribution Details</a></li>
<li class="toctree-l4"><a class="reference internal" href="../list_of_distributions.html#on-device-distributions">On-Device Distributions</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../kubernetes_deployment.html">Kubernetes Deployment Guide</a></li>
<li class="toctree-l2"><a class="reference internal" href="../building_distro.html">Build your own Distribution</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../building_applications/index.html">Building AI Applications (Examples)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../playground/index.html">Llama Stack Playground</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../contributing/index.html">Contributing to Llama-Stack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../references/index.html">References</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu"  style="background: #2980B9" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">llama-stack</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html">Distributions Overview</a></li>
          <li class="breadcrumb-item"><a href="../list_of_distributions.html">Available List of Distributions</a></li>
      <li class="breadcrumb-item active">Remote vLLM Distribution</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/distributions/self_hosted_distro/remote-vllm.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <!-- This file was auto-generated by distro_codegen.py, please edit source -->
<section class="tex2jax_ignore mathjax_ignore" id="remote-vllm-distribution">
<h1>Remote vLLM Distribution<a class="headerlink" href="#remote-vllm-distribution" title="Link to this heading"></a></h1>
<div class="toctree-wrapper compound">
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">llamastack/distribution-remote-vllm</span></code> distribution consists of the following provider configurations:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>API</p></th>
<th class="head"><p>Provider(s)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>agents</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">inline::meta-reference</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>datasetio</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">remote::huggingface</span></code>, <code class="docutils literal notranslate"><span class="pre">inline::localfs</span></code></p></td>
</tr>
<tr class="row-even"><td><p>eval</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">inline::meta-reference</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>inference</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">remote::vllm</span></code>, <code class="docutils literal notranslate"><span class="pre">inline::sentence-transformers</span></code></p></td>
</tr>
<tr class="row-even"><td><p>safety</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">inline::llama-guard</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>scoring</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">inline::basic</span></code>, <code class="docutils literal notranslate"><span class="pre">inline::llm-as-judge</span></code>, <code class="docutils literal notranslate"><span class="pre">inline::braintrust</span></code></p></td>
</tr>
<tr class="row-even"><td><p>telemetry</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">inline::meta-reference</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>tool_runtime</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">remote::brave-search</span></code>, <code class="docutils literal notranslate"><span class="pre">remote::tavily-search</span></code>, <code class="docutils literal notranslate"><span class="pre">inline::rag-runtime</span></code>, <code class="docutils literal notranslate"><span class="pre">remote::model-context-protocol</span></code>, <code class="docutils literal notranslate"><span class="pre">remote::wolfram-alpha</span></code></p></td>
</tr>
<tr class="row-even"><td><p>vector_io</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">inline::faiss</span></code>, <code class="docutils literal notranslate"><span class="pre">remote::chromadb</span></code>, <code class="docutils literal notranslate"><span class="pre">remote::pgvector</span></code></p></td>
</tr>
</tbody>
</table>
<p>You can use this distribution if you want to run an independent vLLM server for inference.</p>
<section id="environment-variables">
<h2>Environment Variables<a class="headerlink" href="#environment-variables" title="Link to this heading"></a></h2>
<p>The following environment variables can be configured:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">LLAMA_STACK_PORT</span></code>: Port for the Llama Stack distribution server (default: <code class="docutils literal notranslate"><span class="pre">8321</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">INFERENCE_MODEL</span></code>: Inference model loaded into the vLLM server (default: <code class="docutils literal notranslate"><span class="pre">meta-llama/Llama-3.2-3B-Instruct</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">VLLM_URL</span></code>: URL of the vLLM server with the main inference model (default: <code class="docutils literal notranslate"><span class="pre">http://host.docker.internal:5100/v1</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">MAX_TOKENS</span></code>: Maximum number of tokens for generation (default: <code class="docutils literal notranslate"><span class="pre">4096</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">SAFETY_VLLM_URL</span></code>: URL of the vLLM server with the safety model (default: <code class="docutils literal notranslate"><span class="pre">http://host.docker.internal:5101/v1</span></code>)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">SAFETY_MODEL</span></code>: Name of the safety (Llama-Guard) model to use (default: <code class="docutils literal notranslate"><span class="pre">meta-llama/Llama-Guard-3-1B</span></code>)</p></li>
</ul>
</section>
<section id="setting-up-vllm-server">
<h2>Setting up vLLM server<a class="headerlink" href="#setting-up-vllm-server" title="Link to this heading"></a></h2>
<p>In the following sections, we’ll use AMD, NVIDIA or Intel GPUs to serve as hardware accelerators for the vLLM
server, which acts as both the LLM inference provider and the safety provider. Note that vLLM also
<a class="reference external" href="https://docs.vllm.ai/en/latest/getting_started/installation.html">supports many other hardware accelerators</a> and
that we only use GPUs here for demonstration purposes. Note that if you run into issues, you can include the environment variable <code class="docutils literal notranslate"><span class="pre">--env</span> <span class="pre">VLLM_DEBUG_LOG_API_SERVER_RESPONSE=true</span></code> (available in vLLM v0.8.3 and above) in the <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">run</span></code> command to enable log response from API server for debugging.</p>
<section id="setting-up-vllm-server-on-amd-gpu">
<h3>Setting up vLLM server on AMD GPU<a class="headerlink" href="#setting-up-vllm-server-on-amd-gpu" title="Link to this heading"></a></h3>
<p>AMD provides two main vLLM container options:</p>
<ul class="simple">
<li><p>rocm/vllm: Production-ready container</p></li>
<li><p>rocm/vllm-dev: Development container with the latest vLLM features</p></li>
</ul>
<p>Please check the <a class="reference external" href="https://rocm.blogs.amd.com/software-tools-optimization/vllm-container/README.html">Blog about ROCm vLLM Usage</a> to get more details.</p>
<p>Here is a sample script to start a ROCm vLLM server locally via Docker:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">INFERENCE_PORT</span><span class="o">=</span><span class="m">8000</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">INFERENCE_MODEL</span><span class="o">=</span>meta-llama/Llama-3.2-3B-Instruct
<span class="nb">export</span><span class="w"> </span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">VLLM_DIMG</span><span class="o">=</span><span class="s2">&quot;rocm/vllm-dev:main&quot;</span>

docker<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--pull<span class="w"> </span>always<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--ipc<span class="o">=</span>host<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--privileged<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--shm-size<span class="w"> </span>16g<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/kfd<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/dri<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--group-add<span class="w"> </span>video<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--cap-add<span class="o">=</span>SYS_PTRACE<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--cap-add<span class="o">=</span>CAP_SYS_ADMIN<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">seccomp</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">apparmor</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--env<span class="w"> </span><span class="s2">&quot;HUGGING_FACE_HUB_TOKEN=</span><span class="nv">$HF_TOKEN</span><span class="s2">&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--env<span class="w"> </span><span class="s2">&quot;HIP_VISIBLE_DEVICES=</span><span class="nv">$CUDA_VISIBLE_DEVICES</span><span class="s2">&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-p<span class="w"> </span><span class="nv">$INFERENCE_PORT</span>:<span class="nv">$INFERENCE_PORT</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span>~/.cache/huggingface:/root/.cache/huggingface<span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="nv">$VLLM_DIMG</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>python<span class="w"> </span>-m<span class="w"> </span>vllm.entrypoints.openai.api_server<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model<span class="w"> </span><span class="nv">$INFERENCE_MODEL</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--port<span class="w"> </span><span class="nv">$INFERENCE_PORT</span>
</pre></div>
</div>
<p>Note that you’ll also need to set <code class="docutils literal notranslate"><span class="pre">--enable-auto-tool-choice</span></code> and <code class="docutils literal notranslate"><span class="pre">--tool-call-parser</span></code> to <a class="reference external" href="https://docs.vllm.ai/en/latest/features/tool_calling.html">enable tool calling in vLLM</a>.</p>
<p>If you are using Llama Stack Safety / Shield APIs, then you will need to also run another instance of a vLLM with a corresponding safety model like <code class="docutils literal notranslate"><span class="pre">meta-llama/Llama-Guard-3-1B</span></code> using a script like:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">SAFETY_PORT</span><span class="o">=</span><span class="m">8081</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">SAFETY_MODEL</span><span class="o">=</span>meta-llama/Llama-Guard-3-1B
<span class="nb">export</span><span class="w"> </span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">1</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">VLLM_DIMG</span><span class="o">=</span><span class="s2">&quot;rocm/vllm-dev:main&quot;</span>

docker<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--pull<span class="w"> </span>always<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--ipc<span class="o">=</span>host<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--privileged<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--shm-size<span class="w"> </span>16g<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/kfd<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="o">=</span>/dev/dri<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--group-add<span class="w"> </span>video<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--cap-add<span class="o">=</span>SYS_PTRACE<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--cap-add<span class="o">=</span>CAP_SYS_ADMIN<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">seccomp</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--security-opt<span class="w"> </span><span class="nv">apparmor</span><span class="o">=</span>unconfined<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--env<span class="w"> </span><span class="s2">&quot;HUGGING_FACE_HUB_TOKEN=</span><span class="nv">$HF_TOKEN</span><span class="s2">&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--env<span class="w"> </span><span class="s2">&quot;HIP_VISIBLE_DEVICES=</span><span class="nv">$CUDA_VISIBLE_DEVICES</span><span class="s2">&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-p<span class="w"> </span><span class="nv">$SAFETY_PORT</span>:<span class="nv">$SAFETY_PORT</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span>~/.cache/huggingface:/root/.cache/huggingface<span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="nv">$VLLM_DIMG</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>python<span class="w"> </span>-m<span class="w"> </span>vllm.entrypoints.openai.api_server<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model<span class="w"> </span><span class="nv">$SAFETY_MODEL</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--port<span class="w"> </span><span class="nv">$SAFETY_PORT</span>
</pre></div>
</div>
</section>
<section id="setting-up-vllm-server-on-nvidia-gpu">
<h3>Setting up vLLM server on NVIDIA GPU<a class="headerlink" href="#setting-up-vllm-server-on-nvidia-gpu" title="Link to this heading"></a></h3>
<p>Please check the <a class="reference external" href="https://docs.vllm.ai/en/v0.5.5/serving/deploying_with_docker.html">vLLM Documentation</a> to get a vLLM endpoint. Here is a sample script to start a vLLM server locally via Docker:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">INFERENCE_PORT</span><span class="o">=</span><span class="m">8000</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">INFERENCE_MODEL</span><span class="o">=</span>meta-llama/Llama-3.2-3B-Instruct
<span class="nb">export</span><span class="w"> </span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">0</span>

docker<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--pull<span class="w"> </span>always<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--runtime<span class="w"> </span>nvidia<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--gpus<span class="w"> </span><span class="nv">$CUDA_VISIBLE_DEVICES</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span>~/.cache/huggingface:/root/.cache/huggingface<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--env<span class="w"> </span><span class="s2">&quot;HUGGING_FACE_HUB_TOKEN=</span><span class="nv">$HF_TOKEN</span><span class="s2">&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-p<span class="w"> </span><span class="nv">$INFERENCE_PORT</span>:<span class="nv">$INFERENCE_PORT</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--ipc<span class="o">=</span>host<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>vllm/vllm-openai:latest<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--gpu-memory-utilization<span class="w"> </span><span class="m">0</span>.7<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model<span class="w"> </span><span class="nv">$INFERENCE_MODEL</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--port<span class="w"> </span><span class="nv">$INFERENCE_PORT</span>
</pre></div>
</div>
<p>Note that you’ll also need to set <code class="docutils literal notranslate"><span class="pre">--enable-auto-tool-choice</span></code> and <code class="docutils literal notranslate"><span class="pre">--tool-call-parser</span></code> to <a class="reference external" href="https://docs.vllm.ai/en/latest/features/tool_calling.html">enable tool calling in vLLM</a>.</p>
<p>If you are using Llama Stack Safety / Shield APIs, then you will need to also run another instance of a vLLM with a corresponding safety model like <code class="docutils literal notranslate"><span class="pre">meta-llama/Llama-Guard-3-1B</span></code> using a script like:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">SAFETY_PORT</span><span class="o">=</span><span class="m">8081</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">SAFETY_MODEL</span><span class="o">=</span>meta-llama/Llama-Guard-3-1B
<span class="nb">export</span><span class="w"> </span><span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">1</span>

docker<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--pull<span class="w"> </span>always<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--runtime<span class="w"> </span>nvidia<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--gpus<span class="w"> </span><span class="nv">$CUDA_VISIBLE_DEVICES</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span>~/.cache/huggingface:/root/.cache/huggingface<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--env<span class="w"> </span><span class="s2">&quot;HUGGING_FACE_HUB_TOKEN=</span><span class="nv">$HF_TOKEN</span><span class="s2">&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-p<span class="w"> </span><span class="nv">$SAFETY_PORT</span>:<span class="nv">$SAFETY_PORT</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--ipc<span class="o">=</span>host<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>vllm/vllm-openai:latest<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--gpu-memory-utilization<span class="w"> </span><span class="m">0</span>.7<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model<span class="w"> </span><span class="nv">$SAFETY_MODEL</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--port<span class="w"> </span><span class="nv">$SAFETY_PORT</span>
</pre></div>
</div>
</section>
<section id="setting-up-vllm-server-on-intel-gpu">
<h3>Setting up vLLM server on Intel GPU<a class="headerlink" href="#setting-up-vllm-server-on-intel-gpu" title="Link to this heading"></a></h3>
<p>Refer to <a class="reference external" href="https://docs.vllm.ai/en/v0.8.2/getting_started/installation/gpu.html?device=xpu">vLLM Documentation for XPU</a> to get a vLLM endpoint. In addition to vLLM side setup which guides towards installing vLLM from sources orself-building vLLM Docker container, Intel provides prebuilt vLLM container to use on systems with Intel GPUs supported by PyTorch XPU backend:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://hub.docker.com/r/intel/vllm">intel/vllm</a></p></li>
</ul>
<p>Here is a sample script to start a vLLM server locally via Docker using Intel provided container:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">INFERENCE_PORT</span><span class="o">=</span><span class="m">8000</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">INFERENCE_MODEL</span><span class="o">=</span>meta-llama/Llama-3.2-1B-Instruct
<span class="nb">export</span><span class="w"> </span><span class="nv">ZE_AFFINITY_MASK</span><span class="o">=</span><span class="m">0</span>

docker<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--pull<span class="w"> </span>always<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="w"> </span>/dev/dri<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span>/dev/dri/by-path:/dev/dri/by-path<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span>~/.cache/huggingface:/root/.cache/huggingface<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--env<span class="w"> </span><span class="s2">&quot;HUGGING_FACE_HUB_TOKEN=</span><span class="nv">$HF_TOKEN</span><span class="s2">&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--env<span class="w"> </span><span class="nv">ZE_AFFINITY_MASK</span><span class="o">=</span><span class="nv">$ZE_AFFINITY_MASK</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-p<span class="w"> </span><span class="nv">$INFERENCE_PORT</span>:<span class="nv">$INFERENCE_PORT</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--ipc<span class="o">=</span>host<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>intel/vllm:xpu<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--gpu-memory-utilization<span class="w"> </span><span class="m">0</span>.7<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model<span class="w"> </span><span class="nv">$INFERENCE_MODEL</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--port<span class="w"> </span><span class="nv">$INFERENCE_PORT</span>
</pre></div>
</div>
<p>If you are using Llama Stack Safety / Shield APIs, then you will need to also run another instance of a vLLM with a corresponding safety model like <code class="docutils literal notranslate"><span class="pre">meta-llama/Llama-Guard-3-1B</span></code> using a script like:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">SAFETY_PORT</span><span class="o">=</span><span class="m">8081</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">SAFETY_MODEL</span><span class="o">=</span>meta-llama/Llama-Guard-3-1B
<span class="nb">export</span><span class="w"> </span><span class="nv">ZE_AFFINITY_MASK</span><span class="o">=</span><span class="m">1</span>

docker<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--pull<span class="w"> </span>always<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--device<span class="w"> </span>/dev/dri<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span>/dev/dri/by-path:/dev/dri/by-path<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-v<span class="w"> </span>~/.cache/huggingface:/root/.cache/huggingface<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--env<span class="w"> </span><span class="s2">&quot;HUGGING_FACE_HUB_TOKEN=</span><span class="nv">$HF_TOKEN</span><span class="s2">&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--env<span class="w"> </span><span class="nv">ZE_AFFINITY_MASK</span><span class="o">=</span><span class="nv">$ZE_AFFINITY_MASK</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-p<span class="w"> </span><span class="nv">$SAFETY_PORT</span>:<span class="nv">$SAFETY_PORT</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--ipc<span class="o">=</span>host<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>intel/vllm:xpu<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--gpu-memory-utilization<span class="w"> </span><span class="m">0</span>.7<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model<span class="w"> </span><span class="nv">$SAFETY_MODEL</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--port<span class="w"> </span><span class="nv">$SAFETY_PORT</span>
</pre></div>
</div>
</section>
</section>
<section id="running-llama-stack">
<h2>Running Llama Stack<a class="headerlink" href="#running-llama-stack" title="Link to this heading"></a></h2>
<p>Now you are ready to run Llama Stack with vLLM as the inference provider. You can do this via Conda (build code) or Docker which has a pre-built image.</p>
<section id="via-docker">
<h3>Via Docker<a class="headerlink" href="#via-docker" title="Link to this heading"></a></h3>
<p>This method allows you to get started quickly without having to build the distribution code.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">INFERENCE_PORT</span><span class="o">=</span><span class="m">8000</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">INFERENCE_MODEL</span><span class="o">=</span>meta-llama/Llama-3.2-3B-Instruct
<span class="nb">export</span><span class="w"> </span><span class="nv">LLAMA_STACK_PORT</span><span class="o">=</span><span class="m">8321</span>

<span class="c1"># You need a local checkout of llama-stack to run this, get it using</span>
<span class="c1"># git clone https://github.com/meta-llama/llama-stack.git</span>
<span class="nb">cd</span><span class="w"> </span>/path/to/llama-stack

docker<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--pull<span class="w"> </span>always<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-p<span class="w"> </span><span class="nv">$LLAMA_STACK_PORT</span>:<span class="nv">$LLAMA_STACK_PORT</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-v<span class="w"> </span>./llama_stack/templates/remote-vllm/run.yaml:/root/my-run.yaml<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>llamastack/distribution-remote-vllm<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--config<span class="w"> </span>/root/my-run.yaml<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--port<span class="w"> </span><span class="nv">$LLAMA_STACK_PORT</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">INFERENCE_MODEL</span><span class="o">=</span><span class="nv">$INFERENCE_MODEL</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">VLLM_URL</span><span class="o">=</span>http://host.docker.internal:<span class="nv">$INFERENCE_PORT</span>/v1
</pre></div>
</div>
<p>If you are using Llama Stack Safety / Shield APIs, use:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">SAFETY_PORT</span><span class="o">=</span><span class="m">8081</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">SAFETY_MODEL</span><span class="o">=</span>meta-llama/Llama-Guard-3-1B

<span class="c1"># You need a local checkout of llama-stack to run this, get it using</span>
<span class="c1"># git clone https://github.com/meta-llama/llama-stack.git</span>
<span class="nb">cd</span><span class="w"> </span>/path/to/llama-stack

docker<span class="w"> </span>run<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--pull<span class="w"> </span>always<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-p<span class="w"> </span><span class="nv">$LLAMA_STACK_PORT</span>:<span class="nv">$LLAMA_STACK_PORT</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-v<span class="w"> </span>~/.llama:/root/.llama<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-v<span class="w"> </span>./llama_stack/templates/remote-vllm/run-with-safety.yaml:/root/my-run.yaml<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>llamastack/distribution-remote-vllm<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--config<span class="w"> </span>/root/my-run.yaml<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--port<span class="w"> </span><span class="nv">$LLAMA_STACK_PORT</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">INFERENCE_MODEL</span><span class="o">=</span><span class="nv">$INFERENCE_MODEL</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">VLLM_URL</span><span class="o">=</span>http://host.docker.internal:<span class="nv">$INFERENCE_PORT</span>/v1<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">SAFETY_MODEL</span><span class="o">=</span><span class="nv">$SAFETY_MODEL</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">SAFETY_VLLM_URL</span><span class="o">=</span>http://host.docker.internal:<span class="nv">$SAFETY_PORT</span>/v1
</pre></div>
</div>
</section>
<section id="via-conda">
<h3>Via Conda<a class="headerlink" href="#via-conda" title="Link to this heading"></a></h3>
<p>Make sure you have done <code class="docutils literal notranslate"><span class="pre">uv</span> <span class="pre">pip</span> <span class="pre">install</span> <span class="pre">llama-stack</span></code> and have the Llama Stack CLI available.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">INFERENCE_PORT</span><span class="o">=</span><span class="m">8000</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">INFERENCE_MODEL</span><span class="o">=</span>meta-llama/Llama-3.2-3B-Instruct
<span class="nb">export</span><span class="w"> </span><span class="nv">LLAMA_STACK_PORT</span><span class="o">=</span><span class="m">8321</span>

<span class="nb">cd</span><span class="w"> </span>distributions/remote-vllm
llama<span class="w"> </span>stack<span class="w"> </span>build<span class="w"> </span>--template<span class="w"> </span>remote-vllm<span class="w"> </span>--image-type<span class="w"> </span>conda

llama<span class="w"> </span>stack<span class="w"> </span>run<span class="w"> </span>./run.yaml<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--port<span class="w"> </span><span class="nv">$LLAMA_STACK_PORT</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">INFERENCE_MODEL</span><span class="o">=</span><span class="nv">$INFERENCE_MODEL</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">VLLM_URL</span><span class="o">=</span>http://localhost:<span class="nv">$INFERENCE_PORT</span>/v1
</pre></div>
</div>
<p>If you are using Llama Stack Safety / Shield APIs, use:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">SAFETY_PORT</span><span class="o">=</span><span class="m">8081</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">SAFETY_MODEL</span><span class="o">=</span>meta-llama/Llama-Guard-3-1B

llama<span class="w"> </span>stack<span class="w"> </span>run<span class="w"> </span>./run-with-safety.yaml<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--port<span class="w"> </span><span class="nv">$LLAMA_STACK_PORT</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">INFERENCE_MODEL</span><span class="o">=</span><span class="nv">$INFERENCE_MODEL</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">VLLM_URL</span><span class="o">=</span>http://localhost:<span class="nv">$INFERENCE_PORT</span>/v1<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">SAFETY_MODEL</span><span class="o">=</span><span class="nv">$SAFETY_MODEL</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--env<span class="w"> </span><span class="nv">SAFETY_VLLM_URL</span><span class="o">=</span>http://localhost:<span class="nv">$SAFETY_PORT</span>/v1
</pre></div>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../remote_hosted_distro/index.html" class="btn btn-neutral float-left" title="Remote-Hosted Distributions" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="meta-reference-gpu.html" class="btn btn-neutral float-right" title="Meta Reference Distribution" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, Meta.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  
<div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    <span class="fa fa-book"> Read the Docs</span>
    v: v0.2.11
    <span class="fa fa-caret-down"></span>
  </span>
  <div class="rst-other-versions">
    <dl>
      <dt>Versions</dt>
      <dd>
        <a href="/v0.2.19/">latest</a>
      </dd>
      <dd>
        <a href="/v0.2.19/">v0.2.19</a>
      </dd>
      <dd>
        <a href="/v0.2.18/">v0.2.18</a>
      </dd>
      <dd>
        <a href="/v0.2.17/">v0.2.17</a>
      </dd>
      <dd>
        <a href="/v0.2.16/">v0.2.16</a>
      </dd>
      <dd>
        <a href="/v0.2.15/">v0.2.15</a>
      </dd>
      <dd>
        <a href="/v0.2.14/">v0.2.14</a>
      </dd>
      <dd>
        <a href="/v0.2.13/">v0.2.13</a>
      </dd>
      <dd>
        <a href="/v0.2.12/">v0.2.12</a>
      </dd>
      <dd class="rtd-current-item">
        <a href="/v0.2.11/">v0.2.11</a>
      </dd>
    </dl>
  </div>
</div><script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>